<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>278 high scalability-2008-03-16-Product: GlusterFS</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-278" href="#">high_scalability-2008-278</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>278 high scalability-2008-03-16-Product: GlusterFS</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-278-html" href="http://highscalability.com//blog/2008/3/16/product-glusterfs.html">html</a></p><p>Introduction: Adapted from their website:GlusterFSis a clustered file-system capable of
scaling to several peta-bytes. It aggregates various storage bricks over
Infiniband RDMA or TCP/IP interconnect into one large parallel network file
system. Storage bricks can be made of any commodity hardware such as x86-64
server with SATA-II RAID and Infiniband HBA).Cluster file systems are still
not mature for enterprise market. They are too complex to deploy and maintain
though they are extremely scalable and cheap. Can be entirely built out of
commodity OS and hardware. GlusterFS hopes to solves this problem.GlusterFS
achieved35 GBps read throughput. The GlusterFS Aggregated I/O Benchmark was
performed on 64 bricks clustered storage system over 10 Gbps Infiniband
interconnect. A cluster of 220 clients pounded the storage system with
multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB
block size. GlusterFS was configured with unify translator and round-robin
scheduler.The advantage</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Adapted from their website:GlusterFSis a clustered file-system capable of scaling to several peta-bytes. [sent-1, score-0.197]
</p><p>2 It aggregates various storage bricks over Infiniband RDMA or TCP/IP interconnect into one large parallel network file system. [sent-2, score-0.853]
</p><p>3 Storage bricks can be made of any commodity hardware such as x86-64 server with SATA-II RAID and Infiniband HBA). [sent-3, score-0.475]
</p><p>4 Cluster file systems are still not mature for enterprise market. [sent-4, score-0.172]
</p><p>5 They are too complex to deploy and maintain though they are extremely scalable and cheap. [sent-5, score-0.096]
</p><p>6 Can be entirely built out of commodity OS and hardware. [sent-6, score-0.231]
</p><p>7 The GlusterFS Aggregated I/O Benchmark was performed on 64 bricks clustered storage system over 10 Gbps Infiniband interconnect. [sent-9, score-0.671]
</p><p>8 A cluster of 220 clients pounded the storage system with multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB block size. [sent-10, score-0.293]
</p><p>9 GlusterFS was configured with unify translator and round-robin scheduler. [sent-11, score-0.408]
</p><p>10 The advantages of GlusterFS are:break* Designed for O(1) scalability and feature rich. [sent-12, score-0.053]
</p><p>11 User can recover the files and folders even without GlusterFS. [sent-14, score-0.173]
</p><p>12 * Extensible scheduling interface with modules loaded based on user's storage I/O access pattern. [sent-18, score-0.287]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('glusterfs', 0.518), ('bricks', 0.371), ('infiniband', 0.322), ('translator', 0.247), ('rdma', 0.201), ('aggregates', 0.188), ('gbps', 0.173), ('extensible', 0.158), ('clustered', 0.14), ('entirely', 0.127), ('hba', 0.124), ('unify', 0.111), ('folders', 0.107), ('commodity', 0.104), ('fest', 0.104), ('storage', 0.102), ('interconnect', 0.101), ('faq', 0.098), ('file', 0.091), ('adapted', 0.088), ('hopes', 0.081), ('mature', 0.081), ('aggregated', 0.077), ('solves', 0.076), ('modular', 0.075), ('annual', 0.075), ('port', 0.072), ('debug', 0.07), ('recover', 0.066), ('loaded', 0.063), ('modules', 0.062), ('scheduling', 0.06), ('centralized', 0.059), ('raid', 0.058), ('performed', 0.058), ('capable', 0.057), ('benchmark', 0.057), ('break', 0.056), ('gb', 0.054), ('block', 0.054), ('advantages', 0.053), ('presentation', 0.051), ('configured', 0.05), ('maintain', 0.048), ('implemented', 0.048), ('extremely', 0.048), ('clients', 0.046), ('user', 0.046), ('articles', 0.045), ('supports', 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="278-tfidf-1" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:GlusterFSis a clustered file-system capable of
scaling to several peta-bytes. It aggregates various storage bricks over
Infiniband RDMA or TCP/IP interconnect into one large parallel network file
system. Storage bricks can be made of any commodity hardware such as x86-64
server with SATA-II RAID and Infiniband HBA).Cluster file systems are still
not mature for enterprise market. They are too complex to deploy and maintain
though they are extremely scalable and cheap. Can be entirely built out of
commodity OS and hardware. GlusterFS hopes to solves this problem.GlusterFS
achieved35 GBps read throughput. The GlusterFS Aggregated I/O Benchmark was
performed on 64 bricks clustered storage system over 10 Gbps Infiniband
interconnect. A cluster of 220 clients pounded the storage system with
multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB
block size. GlusterFS was configured with unify translator and round-robin
scheduler.The advantage</p><p>2 0.20149402 <a title="278-tfidf-2" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>Introduction: Hi. I'm looking for a way to share files between EC2 nodes. Currently we are
using glusterfs to do this. It has been reliable recently, but in the past it
has crashed under high load and we've had trouble starting it up again. We've
only been able to restart it by removing the files, restarting the cluster,
and filing it up again with our files from backup. This takes ages, and will
take even longer the more files we get.What worries me is that it seems to
make each node a point of failure for the entire system. One node crashes and
soon the entire cluster has crashed. The other problem is adding another node.
It seems like you have to take down the whole thing, reconfigure to include
the new node, and restart. This kind of defeats the horizontal scaling
strategy.We are using 2 EC2 instances as web servers, 1 as a DB master, and 1
as a slave. GlusterFS is installed on the web server machines as well as the
DB slave machine (we backup files to s3 from this machine). The files are
mostly</p><p>3 0.14362824 <a title="278-tfidf-3" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>Introduction: The Clustered Storage RevolutionIf the clustered file system, clustered
storage system, storage virtualization movement is new to you then this is a
good intro paper. It's a both vendor puff piece and informative, so it might
be worth your time.A Quick Hit of What's InsideClustered storage architectures
have the ability to pull together two or more storage devices to behave as a
single entity. Clustered storage can be broken down into three types:2-way
simple failover clusteringNamespace aggregationClustered storage with a
distributed file systems (DFS)</p><p>4 0.11130486 <a title="278-tfidf-4" href="../high_scalability-2008/high_scalability-2008-10-14-Implementing_the_Lustre_File_System_with_Sun_Storage%3A_High_Performance_Storage_for_High_Performance_Computing.html">411 high scalability-2008-10-14-Implementing the Lustre File System with Sun Storage: High Performance Storage for High Performance Computing</a></p>
<p>Introduction: Much of the focus of high performance computing (HPC) has centered on CPU
performance. However, as computing requirements grow, HPC clusters are
demanding higher rates of aggregate data throughput. Today's clusters feature
larger numbers of nodes with increased compute speeds. The higher clock rates
and operations per clock cycle create increased demand for local data on each
node. In addition, InfiniBand and other high-speed, low-latency interconnects
increase the data throughput available to each node.Traditional shared file
systems such as NFS have not been able to scale to meet this growing demand
for data throughput on HPC clusters. Scalable cluster file systems that can
provide parallel data access to hundreds of nodes and petabytes of storage are
needed to provide the high data throughput required by large HPC applications,
including manufacturing, electronic design, and research.This paper describes
an implementation of the Sun Lustre file system as a scalable storage cluster
u</p><p>5 0.088776253 <a title="278-tfidf-5" href="../high_scalability-2007/high_scalability-2007-07-15-Isilon_Clustred_Storage_System.html">12 high scalability-2007-07-15-Isilon Clustred Storage System</a></p>
<p>Introduction: TheIsilon IQfamily of clustered storage systems was designed from the ground
up to meet the needs of data-intensive enterprises and high-performance
computing environments. By combining Isilon's OneFSÂ® operating system software
with the latest advances in industry-standard hardware, Isilon delivers
modular, pay-as-you-grow, enterprise-class clustered storage systems. OneFS,
with TrueScaleâ¢ technology, powers the industry's first and only storage
system that enables linear or independent scaling of performance and capacity.
This new flexible and tunable system, featuring a robust suite of clustered
storage software applications, provides customers with an "out of the box"
solution that is fully optimized for the widest range of applications and
workflow needs.* Scales from 4 TB ti 1 PB* Throughput of up to 10 GB per
seond* Linear scaling* Easy to manageRelated Articles Inside Skinny On
Isilonby StorageMojo</p><p>6 0.082726657 <a title="278-tfidf-6" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>7 0.078183755 <a title="278-tfidf-7" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>8 0.07629995 <a title="278-tfidf-8" href="../high_scalability-2012/high_scalability-2012-11-30-Stuff_The_Internet_Says_On_Scalability_For_November_30%2C_2012.html">1365 high scalability-2012-11-30-Stuff The Internet Says On Scalability For November 30, 2012</a></p>
<p>9 0.073236339 <a title="278-tfidf-9" href="../high_scalability-2007/high_scalability-2007-10-21-Paper%3A_Standardizing_Storage_Clusters_%28with_pNFS%29.html">128 high scalability-2007-10-21-Paper: Standardizing Storage Clusters (with pNFS)</a></p>
<p>10 0.072730914 <a title="278-tfidf-10" href="../high_scalability-2012/high_scalability-2012-07-25-Sponsored_Post%3A_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_ground%28ctrl%29%2C_New_Relic%2C_NetDNA%2C_Torbit%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1290 high scalability-2012-07-25-Sponsored Post: ElasticHosts, Atlantic.Net, ScaleOut, ground(ctrl), New Relic, NetDNA, Torbit, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>11 0.071338631 <a title="278-tfidf-11" href="../high_scalability-2012/high_scalability-2012-08-07-Sponsored_Post%3A_Palantir%2C_Percona%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_ground%28ctrl%29%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1300 high scalability-2012-08-07-Sponsored Post: Palantir, Percona, ElasticHosts, Atlantic.Net, ScaleOut, ground(ctrl), New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>12 0.070694692 <a title="278-tfidf-12" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>13 0.069990836 <a title="278-tfidf-13" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>14 0.067727871 <a title="278-tfidf-14" href="../high_scalability-2012/high_scalability-2012-08-21-Sponsored_Post%3A_ROBLOX%2C_Percona%2C_Palantir%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_ground%28ctrl%29%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1308 high scalability-2012-08-21-Sponsored Post: ROBLOX, Percona, Palantir, ElasticHosts, Atlantic.Net, ScaleOut, ground(ctrl), New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>15 0.066430785 <a title="278-tfidf-15" href="../high_scalability-2012/high_scalability-2012-09-05-Sponsored_Post%3A_Surge%2C_FiftyThree%2C_ROBLOX%2C_Percona%2C_Palantir%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1317 high scalability-2012-09-05-Sponsored Post: Surge, FiftyThree, ROBLOX, Percona, Palantir, ElasticHosts, Atlantic.Net, ScaleOut, New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>16 0.066075176 <a title="278-tfidf-16" href="../high_scalability-2010/high_scalability-2010-06-14-How_scalable_could_be_a_cPanel_Hosting_service%3F.html">841 high scalability-2010-06-14-How scalable could be a cPanel Hosting service?</a></p>
<p>17 0.064121403 <a title="278-tfidf-17" href="../high_scalability-2012/high_scalability-2012-09-18-Sponsored_Post%3A_NY_Times%2C_CouchConf%2C_Surge%2C_FiftyThree%2C_ROBLOX%2C_Percona%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1324 high scalability-2012-09-18-Sponsored Post: NY Times, CouchConf, Surge, FiftyThree, ROBLOX, Percona, ElasticHosts, Atlantic.Net, ScaleOut, New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>18 0.061185591 <a title="278-tfidf-18" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>19 0.060002852 <a title="278-tfidf-19" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>20 0.058476575 <a title="278-tfidf-20" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.082), (1, 0.025), (2, -0.014), (3, -0.011), (4, -0.031), (5, 0.016), (6, 0.038), (7, -0.037), (8, -0.006), (9, 0.035), (10, 0.02), (11, -0.049), (12, 0.021), (13, -0.015), (14, 0.029), (15, 0.04), (16, -0.016), (17, 0.005), (18, -0.038), (19, 0.022), (20, 0.027), (21, -0.012), (22, -0.034), (23, 0.044), (24, -0.003), (25, -0.041), (26, 0.058), (27, -0.043), (28, -0.053), (29, -0.018), (30, 0.004), (31, -0.011), (32, -0.001), (33, -0.023), (34, -0.039), (35, 0.021), (36, -0.007), (37, -0.001), (38, 0.029), (39, 0.002), (40, -0.048), (41, -0.059), (42, -0.033), (43, 0.007), (44, -0.08), (45, 0.025), (46, -0.03), (47, -0.016), (48, -0.023), (49, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96616977 <a title="278-lsi-1" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:GlusterFSis a clustered file-system capable of
scaling to several peta-bytes. It aggregates various storage bricks over
Infiniband RDMA or TCP/IP interconnect into one large parallel network file
system. Storage bricks can be made of any commodity hardware such as x86-64
server with SATA-II RAID and Infiniband HBA).Cluster file systems are still
not mature for enterprise market. They are too complex to deploy and maintain
though they are extremely scalable and cheap. Can be entirely built out of
commodity OS and hardware. GlusterFS hopes to solves this problem.GlusterFS
achieved35 GBps read throughput. The GlusterFS Aggregated I/O Benchmark was
performed on 64 bricks clustered storage system over 10 Gbps Infiniband
interconnect. A cluster of 220 clients pounded the storage system with
multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB
block size. GlusterFS was configured with unify translator and round-robin
scheduler.The advantage</p><p>2 0.83049381 <a title="278-lsi-2" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>Introduction: The Clustered Storage RevolutionIf the clustered file system, clustered
storage system, storage virtualization movement is new to you then this is a
good intro paper. It's a both vendor puff piece and informative, so it might
be worth your time.A Quick Hit of What's InsideClustered storage architectures
have the ability to pull together two or more storage devices to behave as a
single entity. Clustered storage can be broken down into three types:2-way
simple failover clusteringNamespace aggregationClustered storage with a
distributed file systems (DFS)</p><p>3 0.76252246 <a title="278-lsi-3" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>Introduction: DataDirect Networks (www.ddn.com) is searching for beta testers for our
exciting new object-based clustered storage system. Does this sound like you?*
Need to store millions to hundreds of billions of files* Want to use one big
file system but can't because no single file system scales big enough* Running
out of inodes* Have to constantly tweak file systems to perform better* Need
to replicate content to more than one data center across geographies* Have
thumbnail images or other small files that wreak havoc on your file and
storage systems* Constantly tweaking and engineering around performance and
scalability limits* No storage system delivers enough IOPS to serve your
content* Spend time load balancing the storage environment* Want a single,
simple way to manage all this dataIf this sounds like you, please contact me
at jgoldstein@ddn.com. DataDirect Networks is a 10-year old, well-established
storage systems company specializing in Extreme Storage environments. We've
deployed both</p><p>4 0.75798184 <a title="278-lsi-4" href="../high_scalability-2007/high_scalability-2007-10-21-Paper%3A_Standardizing_Storage_Clusters_%28with_pNFS%29.html">128 high scalability-2007-10-21-Paper: Standardizing Storage Clusters (with pNFS)</a></p>
<p>Introduction: pNFS (parallel NFS) is the next generation of NFS and its main claim to fame
is that it's clustered, which "enables clients to directly access file data
spread over multiple storage servers in parallel. As a result, each client can
leverage the full aggregate bandwidth of a clustered storage service at the
granularity of an individual file." About pNFSStorageMojosays:pNFS is going to
commoditize parallel data access. In 5 years we won't know how we got along
without it. Something to watch.</p><p>5 0.75573599 <a title="278-lsi-5" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>6 0.75008607 <a title="278-lsi-6" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>7 0.7356025 <a title="278-lsi-7" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>8 0.73100686 <a title="278-lsi-8" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>9 0.70565635 <a title="278-lsi-9" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>10 0.69705677 <a title="278-lsi-10" href="../high_scalability-2007/high_scalability-2007-07-15-Isilon_Clustred_Storage_System.html">12 high scalability-2007-07-15-Isilon Clustred Storage System</a></p>
<p>11 0.69424486 <a title="278-lsi-11" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>12 0.67819053 <a title="278-lsi-12" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>13 0.66574514 <a title="278-lsi-13" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>14 0.6375888 <a title="278-lsi-14" href="../high_scalability-2007/high_scalability-2007-08-01-Product%3A_MogileFS.html">53 high scalability-2007-08-01-Product: MogileFS</a></p>
<p>15 0.63680303 <a title="278-lsi-15" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>16 0.63479614 <a title="278-lsi-16" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>17 0.63197708 <a title="278-lsi-17" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>18 0.6269331 <a title="278-lsi-18" href="../high_scalability-2008/high_scalability-2008-10-14-Implementing_the_Lustre_File_System_with_Sun_Storage%3A_High_Performance_Storage_for_High_Performance_Computing.html">411 high scalability-2008-10-14-Implementing the Lustre File System with Sun Storage: High Performance Storage for High Performance Computing</a></p>
<p>19 0.61545837 <a title="278-lsi-19" href="../high_scalability-2009/high_scalability-2009-01-08-file_synchronization_solutions.html">488 high scalability-2009-01-08-file synchronization solutions</a></p>
<p>20 0.60902548 <a title="278-lsi-20" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.169), (2, 0.156), (10, 0.102), (12, 0.301), (61, 0.053), (79, 0.02), (85, 0.061), (94, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88140345 <a title="278-lda-1" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:GlusterFSis a clustered file-system capable of
scaling to several peta-bytes. It aggregates various storage bricks over
Infiniband RDMA or TCP/IP interconnect into one large parallel network file
system. Storage bricks can be made of any commodity hardware such as x86-64
server with SATA-II RAID and Infiniband HBA).Cluster file systems are still
not mature for enterprise market. They are too complex to deploy and maintain
though they are extremely scalable and cheap. Can be entirely built out of
commodity OS and hardware. GlusterFS hopes to solves this problem.GlusterFS
achieved35 GBps read throughput. The GlusterFS Aggregated I/O Benchmark was
performed on 64 bricks clustered storage system over 10 Gbps Infiniband
interconnect. A cluster of 220 clients pounded the storage system with
multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB
block size. GlusterFS was configured with unify translator and round-robin
scheduler.The advantage</p><p>2 0.87085605 <a title="278-lda-2" href="../high_scalability-2008/high_scalability-2008-01-12-Gandi.net%2C_french_registrar_launches_in_granular_server_resources..html">209 high scalability-2008-01-12-Gandi.net, french registrar launches in granular server resources.</a></p>
<p>Introduction: Gandi.net, a French domain registrar has launched a very flexible dynamic
resource allocated VPS service.</p><p>3 0.78405392 <a title="278-lda-3" href="../high_scalability-2007/high_scalability-2007-09-06-Why_doesn%27t_anyone_use_j2ee%3F.html">82 high scalability-2007-09-06-Why doesn't anyone use j2ee?</a></p>
<p>Introduction: From a reader:> Was reading through your very interesting/useful site.>Most of
the architectures are non j2ee-Does that mean that>there aren't enough
websites that are scalable(with youtube> like userbase) built with j2ee tech-
would like to know if there> are any and their architecture as>well.eBayuses
Java, but in a very pragmatic way. They use servlets, an application server,
the JDK, and they do the rest themselves. They skip JSP, entity beans, and
JMS.When you need to scale putting all your eggs in one basket is a risky
strategy. Why use JSP when you can do better? When use entity beans when you
can do better? Use servlets because they are a very effective way of handling
http requests. Use Java because it is fast, runs everywhere, and has a boat
load of libraries you can use to build your build your custom system.Probably
the major reason J2EE is absentee is simply LAMP. LAMP is just so incredibly
functional for most 2-tier shared nothing sites they don't need a better
infrastruc</p><p>4 0.74787468 <a title="278-lda-4" href="../high_scalability-2008/high_scalability-2008-03-19-Serving_JavaScript_Fast.html">285 high scalability-2008-03-19-Serving JavaScript Fast</a></p>
<p>Introduction: Cal Hendersonwrites at thinkvitamin.com: "With our so-called "Web 2.0'
applications and their rich content and interaction, we expect our
applications to increasingly make use of CSS and JavaScript. To make sure
these applications are nice and snappy to use, we need to optimize the size
and nature of content required to render the page, making sure we're
delivering the optimum experience. In practice, this means a combination of
making our content as small and fast to download as possible, while avoiding
unnecessarily refetching unmodified resources." A lot of good comments too.</p><p>5 0.74204397 <a title="278-lda-5" href="../high_scalability-2007/high_scalability-2007-07-09-LiveJournal_Architecture.html">3 high scalability-2007-07-09-LiveJournal Architecture</a></p>
<p>Introduction: A fascinating and detailed story of how LiveJournal evolved their system to
scale. LiveJournal was an early player in the free blog service race and faced
issues from quickly adding a large number of users. Blog posts come fast and
furious which causes a lot of writes and writes are particularly hard to
scale. Understanding how LiveJournal faced their scaling problems will help
any aspiring website builder.Site: http://www.livejournal.com/Information
SourcesLiveJournal- Behind The Scenes Scaling StorytimeGoogle VideoTokyo
Video2005 versionPlatformLinuxMySqlPerlMemcachedMogileFSApacheWhat's
Inside?Scaling from 1, 2, and 4 hosts to cluster of servers.Avoid single
points of failure.Using MySQL replication only takes you so far.Becoming IO
bound kills scaling.Spread out writes and reads for more parallelism.You can't
keep adding read slaves and scale.Shard storage approach, using DRBD, for
maximal throughput. Allocate shards based on roles.Caching to improve
performance with memcached. Two</p><p>6 0.71295798 <a title="278-lda-6" href="../high_scalability-2012/high_scalability-2012-10-31-Gone_Fishin%27%3A_LiveJournal_Architecture.html">1352 high scalability-2012-10-31-Gone Fishin': LiveJournal Architecture</a></p>
<p>7 0.70299888 <a title="278-lda-7" href="../high_scalability-2009/high_scalability-2009-01-12-Getting_ready_for_the_cloud.html">490 high scalability-2009-01-12-Getting ready for the cloud</a></p>
<p>8 0.6839115 <a title="278-lda-8" href="../high_scalability-2007/high_scalability-2007-11-20-Product%3A_SmartFrog_a_Distributed_Configuration_and_Deployment_Framework.html">161 high scalability-2007-11-20-Product: SmartFrog a Distributed Configuration and Deployment Framework</a></p>
<p>9 0.67852175 <a title="278-lda-9" href="../high_scalability-2010/high_scalability-2010-08-24-21_Quality_Screencasts_on_Scaling_Rails.html">886 high scalability-2010-08-24-21 Quality Screencasts on Scaling Rails</a></p>
<p>10 0.64052469 <a title="278-lda-10" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>11 0.63872749 <a title="278-lda-11" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>12 0.6371932 <a title="278-lda-12" href="../high_scalability-2013/high_scalability-2013-04-29-AWS_v_GCE_Face-off_and_Why_Innovation_Needs_Lower_Cost_Infrastructures.html">1448 high scalability-2013-04-29-AWS v GCE Face-off and Why Innovation Needs Lower Cost Infrastructures</a></p>
<p>13 0.63136834 <a title="278-lda-13" href="../high_scalability-2011/high_scalability-2011-07-01-TripAdvisor_Strategy%3A_No_Architects%2C_Engineers_Work_Across_the_Entire_Stack.html">1072 high scalability-2011-07-01-TripAdvisor Strategy: No Architects, Engineers Work Across the Entire Stack</a></p>
<p>14 0.62992203 <a title="278-lda-14" href="../high_scalability-2009/high_scalability-2009-05-19-Scaling_Memcached%3A_500%2C000%2B_Operations-Second_with_a_Single-Socket_UltraSPARC_T2.html">603 high scalability-2009-05-19-Scaling Memcached: 500,000+ Operations-Second with a Single-Socket UltraSPARC T2</a></p>
<p>15 0.62916631 <a title="278-lda-15" href="../high_scalability-2011/high_scalability-2011-09-21-5_Scalability_Poisons_and_3_Cloud_Scalability_Antidotes.html">1121 high scalability-2011-09-21-5 Scalability Poisons and 3 Cloud Scalability Antidotes</a></p>
<p>16 0.6281805 <a title="278-lda-16" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>17 0.62642777 <a title="278-lda-17" href="../high_scalability-2013/high_scalability-2013-02-04-Is_Provisioned_IOPS_Better%3F_Yes%2C_it_Delivers_More_Consistent_and_Higher_Performance_IO.html">1398 high scalability-2013-02-04-Is Provisioned IOPS Better? Yes, it Delivers More Consistent and Higher Performance IO</a></p>
<p>18 0.62376523 <a title="278-lda-18" href="../high_scalability-2008/high_scalability-2008-10-22-Scalability_Best_Practices%3A_Lessons_from_eBay.html">425 high scalability-2008-10-22-Scalability Best Practices: Lessons from eBay</a></p>
<p>19 0.62374246 <a title="278-lda-19" href="../high_scalability-2010/high_scalability-2010-10-26-Scaling_DISQUS_to_75_Million_Comments_and_17%2C000_RPS.html">928 high scalability-2010-10-26-Scaling DISQUS to 75 Million Comments and 17,000 RPS</a></p>
<p>20 0.62255764 <a title="278-lda-20" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
