<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-271" href="#">high_scalability-2008-271</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-271-html" href="http://highscalability.com//blog/2008/3/8/product-drbd-distributed-replicated-block-device.html">html</a></p><p>Introduction: From their website:DRBDis a block device which is designed to build high
availability clusters. This is done by mirroring a whole block device via (a
dedicated) network. You could see it as a network raid-1.DRBD takes over the
data, writes it to the local disk and sends it to the other host. On the other
host, it takes it to the disk there.breakThe other components needed are a
cluster membership service, which is supposed to be heartbeat, and some kind
of application that works on top of a block device.Examples:A filesystem &
fsck.A journaling FS.A database with recovery capabilities.Each device (DRBD
provides more than one of these devices) has a state, which can be 'primary'
or 'secondary'. On the node with the primary device the application is
supposed to run and to access the device (/dev/drbdX). Every write is sent to
the local 'lower level block device' and to the node with the device in
'secondary' state. The secondary device simply writes the data to its lower
level block devi</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 From their website:DRBDis a block device which is designed to build high availability clusters. [sent-1, score-1.058]
</p><p>2 This is done by mirroring a whole block device via (a dedicated) network. [sent-2, score-1.024]
</p><p>3 DRBD takes over the data, writes it to the local disk and sends it to the other host. [sent-4, score-0.355]
</p><p>4 On the other host, it takes it to the disk there. [sent-5, score-0.127]
</p><p>5 breakThe other components needed are a cluster membership service, which is supposed to be heartbeat, and some kind of application that works on top of a block device. [sent-6, score-0.552]
</p><p>6 Each device (DRBD provides more than one of these devices) has a state, which can be 'primary' or 'secondary'. [sent-10, score-0.587]
</p><p>7 On the node with the primary device the application is supposed to run and to access the device (/dev/drbdX). [sent-11, score-1.702]
</p><p>8 Every write is sent to the local 'lower level block device' and to the node with the device in 'secondary' state. [sent-12, score-1.246]
</p><p>9 The secondary device simply writes the data to its lower level block device. [sent-13, score-1.179]
</p><p>10 If the primary node fails, heartbeat is switching the secondary device into primary state and starts the application there. [sent-15, score-1.637]
</p><p>11 (If you are using it with a non-journaling FS this involves running fsck)If the failed node comes up again, it is a new secondary node and has to synchronise its content to the primary. [sent-16, score-0.702]
</p><p>12 This, of course, will happen whithout interruption of service in the background. [sent-17, score-0.114]
</p><p>13 And, of course, we only will resynchronize those parts of the device that actually have been changed. [sent-18, score-0.587]
</p><p>14 7 series, you can define an "active set" of a certain size. [sent-21, score-0.063]
</p><p>15 This makes it possible to have a total resync time of 1--3 min, regardless of device size (currently up to 4TB), even after a hard crash of an active node. [sent-22, score-0.929]
</p><p>16 Related Articles How to build a redundant, high-availability system with DRBD and Heartbeatby Pedro Pla in Linux JournalLinux-HA Press Roomwith many excellent high availability articles. [sent-23, score-0.195]
</p><p>17 MySQL clustering strategies and comparisionsWikipedia on DRBDUsing Xen for High Availability Clustersby by Kris Buytaert and Johan Huysmans in ONLamp. [sent-25, score-0.114]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('device', 0.587), ('drbd', 0.295), ('block', 0.276), ('heartbeat', 0.201), ('node', 0.193), ('secondary', 0.182), ('primary', 0.142), ('supposed', 0.141), ('availability', 0.124), ('journaling', 0.119), ('kris', 0.119), ('resync', 0.119), ('clustersby', 0.114), ('fs', 0.114), ('interruption', 0.114), ('johan', 0.114), ('mirroring', 0.096), ('active', 0.094), ('min', 0.093), ('course', 0.085), ('press', 0.084), ('xen', 0.083), ('membership', 0.083), ('carried', 0.083), ('local', 0.081), ('writes', 0.077), ('filesystem', 0.073), ('state', 0.071), ('high', 0.071), ('crash', 0.07), ('sends', 0.07), ('involves', 0.07), ('intelligent', 0.069), ('switching', 0.067), ('done', 0.065), ('disk', 0.065), ('failed', 0.064), ('clustering', 0.064), ('fails', 0.064), ('define', 0.063), ('takes', 0.062), ('regardless', 0.059), ('level', 0.057), ('recovery', 0.056), ('always', 0.054), ('redundant', 0.054), ('sent', 0.052), ('application', 0.052), ('devices', 0.052), ('strategies', 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="271-tfidf-1" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_DRBD_-_Distributed_Replicated_Block_Device.html">271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</a></p>
<p>Introduction: From their website:DRBDis a block device which is designed to build high
availability clusters. This is done by mirroring a whole block device via (a
dedicated) network. You could see it as a network raid-1.DRBD takes over the
data, writes it to the local disk and sends it to the other host. On the other
host, it takes it to the disk there.breakThe other components needed are a
cluster membership service, which is supposed to be heartbeat, and some kind
of application that works on top of a block device.Examples:A filesystem &
fsck.A journaling FS.A database with recovery capabilities.Each device (DRBD
provides more than one of these devices) has a state, which can be 'primary'
or 'secondary'. On the node with the primary device the application is
supposed to run and to access the device (/dev/drbdX). Every write is sent to
the local 'lower level block device' and to the node with the device in
'secondary' state. The secondary device simply writes the data to its lower
level block devi</p><p>2 0.24085411 <a title="271-tfidf-2" href="../high_scalability-2007/high_scalability-2007-11-06-Product%3A_ChironFS.html">143 high scalability-2007-11-06-Product: ChironFS</a></p>
<p>Introduction: If you are trying to create highly available file systems, especially across
data centers, then ChironFS is one potential solution. It's relatively new, so
there aren't lots of experience reports, but it looks worth considering. What
isChironFSand how does it work?Adapted from the ChironFS website:The Chiron
Filesystem is a Fuse based filesystem that frees you from single points of
failure. It's main purpose is to guarantee filesystem availability using
replication. But it isn't a RAID implementation. RAID replicates DEVICES not
FILESYSTEMS.Why not just use RAID over some network block device? Because it
is a block device and if one server mounts that device in RW mode, no other
server will be able to mount it in RW mode. Any real network may have many
servers and offer a variety of services. Keeping everything running can become
a real nightmare!</p><p>3 0.15039042 <a title="271-tfidf-3" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>Introduction: Youmight haveconsistency problems if you have: multiple datastores in multiple
datacenters, without distributed transactions, and with the ability to
alternately execute out of each datacenter;  syncing protocols that can fail
or sync stale data; distributed clients that cache data and then write old
back to the central store; a NoSQL database that doesn't have transactions
between updates of multiple related key-value records; application level
integrity checks; client drivenoptimistic locking.Sounds a lot like many
evolving, loosely coupled, autonomous, distributed systems these days. How do
you solve these consistency problems? Siddharth "Sid" Anand of Netflix talks
about how they solved theirs in his excellent presentation, NoSQL @ Netflix :
Part 1, given to a packed crowd at aCloud Computing Meetup. You might be
inclined to say how silly it is to have these problems in the first place, but
just hold on. See if you might share some of their problems, before getting
all judgy:Netfli</p><p>4 0.14624338 <a title="271-tfidf-4" href="../high_scalability-2007/high_scalability-2007-12-30-MySQL_clustering_strategies_and_comparisions.html">196 high scalability-2007-12-30-MySQL clustering strategies and comparisions</a></p>
<p>Introduction: Compare:1. MySQL Clustering(ndb-cluster stogare)2. MySQL / GFS-GNBD/ HA3.
MySQL / DRBD /HA4. MySQL Write Master / Multiple MySQL Read Slaves5.
Standalone MySQL Servers(Functionally seperated)</p><p>5 0.12881765 <a title="271-tfidf-5" href="../high_scalability-2011/high_scalability-2011-06-06-Apple_iCloud%3A_Syncing_and_Distributed_Storage_Over_Streaming_and_Centralized_Storage.html">1053 high scalability-2011-06-06-Apple iCloud: Syncing and Distributed Storage Over Streaming and Centralized Storage</a></p>
<p>Introduction: There has been a lot of speculation over how Apple'siCloudwould work. With the
Apple Worldwide Developers Conferencekeynotes having just completed, we
finally learned the truth. We can handle it. They made some interesting and
cost effective architecture choices that preserved the value of their devices
and the basic model of how their existing services work.A lot of pundits
foretold that with all the datacenters Apple was building we would get a
streaming music solution. Only one copy of music would be stored and then
streamed on demand to everyone. Or they could go the Google brute force method
and copy up all a user's music and play it on demand.Apple did neither. The
chose an interesting middle path that's not Google, Amazon, or even
MobileMe.They key idea is you no longer need a PC. Device content is now
synced over the air and is managed by the cloud, not your legacy computer.
Your data may not even be stored in the cloud, but the whole management,
syncing, and control of content</p><p>6 0.12628056 <a title="271-tfidf-6" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Guide_to_Cost-effective_Database_Scale-Out_using_MySQL.html">17 high scalability-2007-07-16-Paper: Guide to Cost-effective Database Scale-Out using MySQL</a></p>
<p>7 0.11436145 <a title="271-tfidf-7" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>8 0.11368017 <a title="271-tfidf-8" href="../high_scalability-2012/high_scalability-2012-01-19-Is_it_time_to_get_rid_of_the_Linux_OS_model_in_the_cloud%3F.html">1177 high scalability-2012-01-19-Is it time to get rid of the Linux OS model in the cloud?</a></p>
<p>9 0.10861393 <a title="271-tfidf-9" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>10 0.10741128 <a title="271-tfidf-10" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>11 0.10667609 <a title="271-tfidf-11" href="../high_scalability-2007/high_scalability-2007-10-08-Paper%3A_Understanding_and_Building_High_Availability-Load_Balanced_Clusters.html">117 high scalability-2007-10-08-Paper: Understanding and Building High Availability-Load Balanced Clusters</a></p>
<p>12 0.10654923 <a title="271-tfidf-12" href="../high_scalability-2013/high_scalability-2013-12-23-What_Happens_While_Your_Brain_Sleeps_is_Surprisingly_Like_How_Computers_Stay_Sane.html">1568 high scalability-2013-12-23-What Happens While Your Brain Sleeps is Surprisingly Like How Computers Stay Sane</a></p>
<p>13 0.10646307 <a title="271-tfidf-13" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>14 0.09713912 <a title="271-tfidf-14" href="../high_scalability-2007/high_scalability-2007-11-21-n-phase_commit_for_FS_writes%2C_reads_stay_local.html">163 high scalability-2007-11-21-n-phase commit for FS writes, reads stay local</a></p>
<p>15 0.094953127 <a title="271-tfidf-15" href="../high_scalability-2007/high_scalability-2007-12-12-Oracle_Can_Do_Read-Write_Splitting_Too.html">182 high scalability-2007-12-12-Oracle Can Do Read-Write Splitting Too</a></p>
<p>16 0.090715729 <a title="271-tfidf-16" href="../high_scalability-2008/high_scalability-2008-11-10-Scalability_Perspectives_%231%3A_Nicholas_Carr_%E2%80%93_The_Big_Switch.html">439 high scalability-2008-11-10-Scalability Perspectives #1: Nicholas Carr – The Big Switch</a></p>
<p>17 0.086749531 <a title="271-tfidf-17" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>18 0.086550422 <a title="271-tfidf-18" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>19 0.083882891 <a title="271-tfidf-19" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>20 0.082846433 <a title="271-tfidf-20" href="../high_scalability-2012/high_scalability-2012-03-02-Stuff_The_Internet_Says_On_Scalability_For_March_2%2C_2012.html">1203 high scalability-2012-03-02-Stuff The Internet Says On Scalability For March 2, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, 0.062), (2, -0.028), (3, -0.016), (4, -0.036), (5, 0.046), (6, 0.056), (7, -0.023), (8, -0.014), (9, -0.036), (10, 0.003), (11, 0.025), (12, 0.02), (13, -0.017), (14, 0.022), (15, 0.039), (16, 0.007), (17, 0.006), (18, -0.02), (19, -0.008), (20, 0.027), (21, 0.053), (22, -0.048), (23, 0.035), (24, -0.04), (25, 0.034), (26, 0.052), (27, -0.029), (28, -0.006), (29, -0.005), (30, -0.006), (31, -0.008), (32, 0.061), (33, -0.002), (34, 0.038), (35, 0.016), (36, -0.006), (37, -0.016), (38, -0.004), (39, -0.042), (40, -0.01), (41, -0.063), (42, -0.025), (43, 0.098), (44, 0.016), (45, -0.026), (46, 0.008), (47, 0.065), (48, -0.065), (49, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95995831 <a title="271-lsi-1" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_DRBD_-_Distributed_Replicated_Block_Device.html">271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</a></p>
<p>Introduction: From their website:DRBDis a block device which is designed to build high
availability clusters. This is done by mirroring a whole block device via (a
dedicated) network. You could see it as a network raid-1.DRBD takes over the
data, writes it to the local disk and sends it to the other host. On the other
host, it takes it to the disk there.breakThe other components needed are a
cluster membership service, which is supposed to be heartbeat, and some kind
of application that works on top of a block device.Examples:A filesystem &
fsck.A journaling FS.A database with recovery capabilities.Each device (DRBD
provides more than one of these devices) has a state, which can be 'primary'
or 'secondary'. On the node with the primary device the application is
supposed to run and to access the device (/dev/drbdX). Every write is sent to
the local 'lower level block device' and to the node with the device in
'secondary' state. The secondary device simply writes the data to its lower
level block devi</p><p>2 0.73943573 <a title="271-lsi-2" href="../high_scalability-2007/high_scalability-2007-11-06-Product%3A_ChironFS.html">143 high scalability-2007-11-06-Product: ChironFS</a></p>
<p>Introduction: If you are trying to create highly available file systems, especially across
data centers, then ChironFS is one potential solution. It's relatively new, so
there aren't lots of experience reports, but it looks worth considering. What
isChironFSand how does it work?Adapted from the ChironFS website:The Chiron
Filesystem is a Fuse based filesystem that frees you from single points of
failure. It's main purpose is to guarantee filesystem availability using
replication. But it isn't a RAID implementation. RAID replicates DEVICES not
FILESYSTEMS.Why not just use RAID over some network block device? Because it
is a block device and if one server mounts that device in RW mode, no other
server will be able to mount it in RW mode. Any real network may have many
servers and offer a variety of services. Keeping everything running can become
a real nightmare!</p><p>3 0.67562139 <a title="271-lsi-3" href="../high_scalability-2007/high_scalability-2007-08-01-Product%3A_MogileFS.html">53 high scalability-2007-08-01-Product: MogileFS</a></p>
<p>Introduction: MogileFSis an open source distributed filesystem. Its properties and features
include: Application level, No single point of failure, Automatic file
replication, Better than RAID, Flat Namespace, Shared-Nothing, No RAID
required, Local filesystem agnostic.</p><p>4 0.67381877 <a title="271-lsi-4" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>Introduction: When building a system on top of a set of wildly uncooperative and unruly
computers you have knowledge problems: knowing when other nodes are dead;
knowing when nodes become alive; getting information about other nodes so you
can make local decisions, like knowing which node should handle a request
based on a scheme for assigning nodes to a certain range of users; learning
about new configuration data; agreeing on data values; and so on.How do you
solve these problems? A common centralized approach is to use a database and
all nodes query it for information. Obvious availability and performance
issues for large distributed clusters. Another approach is to use Paxos, a
protocol for solving consensus in a network to maintain strict consistency
requirements for small groups of unreliable processes. Not practical when
larger number of nodes are involved.So what's the super cool decentralized way
to bring order to large clusters?Gossip protocols, which maintain relaxed
consistency requireme</p><p>5 0.6354515 <a title="271-lsi-5" href="../high_scalability-2007/high_scalability-2007-07-25-Paper%3A_Designing_Disaster_Tolerant_High_Availability_Clusters.html">25 high scalability-2007-07-25-Paper: Designing Disaster Tolerant High Availability Clusters</a></p>
<p>Introduction: A very detailed (339 pages) paper on how to use HP products to create a highly
available cluster. It's somewhat dated and obviously concentrates on HP
products, but it is still good information.Table of contents:1. Disaster
Tolerance and Recovery in a Serviceguard Cluster2. Building an Extended
Distance Cluster Using ServiceGuard3. Designing a Metropolitan Cluster4.
Designing a Continental Cluster5. Building Disaster-Tolerant Serviceguard
Solutions Using Metrocluster with Continuous Access XP6. Building Disaster
Tolerant Serviceguard Solutions Using Metrocluster with EMC SRDF7. Cascading
Failover in a Continental Clusterbreak-Evaluating the Need for Disaster
ToleranceWhat is a Disaster Tolerant Architecture?Types of Disaster Tolerant
ClustersExtended Distance ClustersMetropolitan ClusterContinental
ClusterContinental Cluster With Cascading FailoverDisaster Tolerant
Architecture GuidelinesProtecting Nodes through Geographic
DispersionProtecting Data through ReplicationUsing Alternative</p><p>6 0.60821968 <a title="271-lsi-6" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>7 0.60747612 <a title="271-lsi-7" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>8 0.58816057 <a title="271-lsi-8" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>9 0.58709824 <a title="271-lsi-9" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>10 0.58685684 <a title="271-lsi-10" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>11 0.58165592 <a title="271-lsi-11" href="../high_scalability-2007/high_scalability-2007-08-20-TypePad_Architecture.html">68 high scalability-2007-08-20-TypePad Architecture</a></p>
<p>12 0.5758881 <a title="271-lsi-12" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>13 0.57171673 <a title="271-lsi-13" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>14 0.56980616 <a title="271-lsi-14" href="../high_scalability-2007/high_scalability-2007-07-10-Webcast%3A_Advanced_Database_High_Availability_and_Scalability_Solutions.html">4 high scalability-2007-07-10-Webcast: Advanced Database High Availability and Scalability Solutions</a></p>
<p>15 0.56967747 <a title="271-lsi-15" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>16 0.56937975 <a title="271-lsi-16" href="../high_scalability-2012/high_scalability-2012-10-11-RAMCube%3A_Exploiting_Network_Proximity_for_RAM-Based_Key-Value_Store.html">1338 high scalability-2012-10-11-RAMCube: Exploiting Network Proximity for RAM-Based Key-Value Store</a></p>
<p>17 0.56745559 <a title="271-lsi-17" href="../high_scalability-2011/high_scalability-2011-12-30-Stuff_The_Internet_Says_On_Scalability_For_December_30%2C_2011.html">1166 high scalability-2011-12-30-Stuff The Internet Says On Scalability For December 30, 2011</a></p>
<p>18 0.56516969 <a title="271-lsi-18" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>19 0.56128472 <a title="271-lsi-19" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>20 0.55884296 <a title="271-lsi-20" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.057), (2, 0.235), (10, 0.136), (44, 0.169), (47, 0.024), (61, 0.064), (79, 0.019), (85, 0.129), (94, 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91438949 <a title="271-lda-1" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_DRBD_-_Distributed_Replicated_Block_Device.html">271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</a></p>
<p>Introduction: From their website:DRBDis a block device which is designed to build high
availability clusters. This is done by mirroring a whole block device via (a
dedicated) network. You could see it as a network raid-1.DRBD takes over the
data, writes it to the local disk and sends it to the other host. On the other
host, it takes it to the disk there.breakThe other components needed are a
cluster membership service, which is supposed to be heartbeat, and some kind
of application that works on top of a block device.Examples:A filesystem &
fsck.A journaling FS.A database with recovery capabilities.Each device (DRBD
provides more than one of these devices) has a state, which can be 'primary'
or 'secondary'. On the node with the primary device the application is
supposed to run and to access the device (/dev/drbdX). Every write is sent to
the local 'lower level block device' and to the node with the device in
'secondary' state. The secondary device simply writes the data to its lower
level block devi</p><p>2 0.83737165 <a title="271-lda-2" href="../high_scalability-2009/high_scalability-2009-04-21-Thread_Pool_Engine_in_MS_CLR_4%2C_and_Work-Stealing_scheduling_algorithm.html">575 high scalability-2009-04-21-Thread Pool Engine in MS CLR 4, and Work-Stealing scheduling algorithm</a></p>
<p>Introduction: I just saw this article inHFadeel blogthat spaek about Parallelism in .NET
Framework 4, and how Thread Pool work, and the most faoums scheduling
algorithm : Work-stealing algorithm. With preisnation to see it in action.</p><p>3 0.80862701 <a title="271-lda-3" href="../high_scalability-2014/high_scalability-2014-01-13-NYTimes_Architecture%3A_No_Head%2C_No_Master%2C_No_Single_Point_of_Failure.html">1577 high scalability-2014-01-13-NYTimes Architecture: No Head, No Master, No Single Point of Failure</a></p>
<p>Introduction: Michael Laing, a Systems Architect at NYTimes, gave thisgreat decriptionof
their use ofRabbitMQ and their overall architecture on the RabbitMQ mailing
list. The closing sentiment marks this as definitely an architecture to learn
from:Although it may seem complex,Fabrik has simple components and is mostly
principles and plumbing. The key point to grasp is that there is no head, no
master, no single point of failure. As I write this I can see components
failing (not RabbitMQ), and we are fixing them so they are more reliable. But
the system doesn't fail, users can connect, and messages are delivered,
regardless - all within design parameters.Since it's short, to the point, and
I couldn't say it better, I'll just reproduce several original sources
here:Just a quick note and thank you to the RabbitMQ team for a great
product.Our premier online offering www.nytimes.com has a new look and new
underpinnings, now including a messaging architecture implemented using
RabbitMQ.This architecture -</p><p>4 0.80738771 <a title="271-lda-4" href="../high_scalability-2008/high_scalability-2008-03-08-Audiogalaxy.com_Architecture.html">269 high scalability-2008-03-08-Audiogalaxy.com Architecture</a></p>
<p>Introduction: Update 3:Always Refer to Your V1 As a Prototype. You really do have to plan to
throw one away.Update 2:Lessons Learned Scaling the Audiogalaxy Search Engine.
Things he should have done and fun things he couldn’t justify
doing.Update:Design details of Audiogalaxy.com’s high performance MySQL search
engine. At peak times, the search engine needed to handle 1500-2000 searches
every second against a MySQL database with about 200 million rows.Search was
one of most interesting problems at Audiogalaxy. It was one of the core
functions of the site, and somewhere between 50 to 70 million searches were
performed every day. At peak times, the search engine needed to handle
1500-2000 searches every second against a MySQL database with about 200
million rows.</p><p>5 0.80276722 <a title="271-lda-5" href="../high_scalability-2013/high_scalability-2013-07-08-The_Architecture_Twitter_Uses_to_Deal_with_150M_Active_Users%2C_300K_QPS%2C_a_22_MB-S_Firehose%2C_and_Send_Tweets_in_Under_5_Seconds.html">1488 high scalability-2013-07-08-The Architecture Twitter Uses to Deal with 150M Active Users, 300K QPS, a 22 MB-S Firehose, and Send Tweets in Under 5 Seconds</a></p>
<p>Introduction: Toy solutions solving Twitter's "problems" are a favorite scalability trope.
Everybody has this idea that Twitter is easy. With a little architectural hand
waving we have a scalable Twitter, just that simple. Well, it's not that
simple asRaffi Krikorian, VP of Engineering at Twitter, describes in his
superb and very detailed presentation onTimelines at Scale. If you want to
know how Twitter works - then start here.It happened gradually so you may have
missed it, but Twitter has grown up. It started as a strugglingthree-tierish
Ruby on Railswebsite to become a beautifully service driven core that we
actually go to now to see if other services are down. Quite a change.Twitter
now has 150M world wide active users, handles 300K QPS to generate timelines,
and a firehose that churns out 22 MB/sec. 400 million tweets a day flow
through the system and it can take up to 5 minutes for a tweet to flow from
Lady Gaga's fingers to her 31 million followers.A couple of points stood
out:Twitter no lon</p><p>6 0.79751581 <a title="271-lda-6" href="../high_scalability-2012/high_scalability-2012-05-04-Stuff_The_Internet_Says_On_Scalability_For_May_4%2C_2012.html">1239 high scalability-2012-05-04-Stuff The Internet Says On Scalability For May 4, 2012</a></p>
<p>7 0.7967521 <a title="271-lda-7" href="../high_scalability-2008/high_scalability-2008-05-10-Hitting_300_SimbleDB_Requests_Per_Second_on_a_Small_EC2_Instance.html">317 high scalability-2008-05-10-Hitting 300 SimbleDB Requests Per Second on a Small EC2 Instance</a></p>
<p>8 0.79401636 <a title="271-lda-8" href="../high_scalability-2009/high_scalability-2009-08-28-Strategy%3A_Solve_Only_80_Percent_of_the_Problem.html">689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</a></p>
<p>9 0.78973669 <a title="271-lda-9" href="../high_scalability-2008/high_scalability-2008-04-10-Mysql_scalability_and_failover....html">302 high scalability-2008-04-10-Mysql scalability and failover...</a></p>
<p>10 0.78927284 <a title="271-lda-10" href="../high_scalability-2011/high_scalability-2011-07-15-Stuff_The_Internet_Says_On_Scalability_For_July_15%2C_2011.html">1080 high scalability-2011-07-15-Stuff The Internet Says On Scalability For July 15, 2011</a></p>
<p>11 0.78792363 <a title="271-lda-11" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>12 0.78119349 <a title="271-lda-12" href="../high_scalability-2014/high_scalability-2014-02-07-Stuff_The_Internet_Says_On_Scalability_For_February_7th%2C_2014.html">1592 high scalability-2014-02-07-Stuff The Internet Says On Scalability For February 7th, 2014</a></p>
<p>13 0.78084451 <a title="271-lda-13" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>14 0.77895153 <a title="271-lda-14" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>15 0.7781834 <a title="271-lda-15" href="../high_scalability-2012/high_scalability-2012-09-21-Stuff_The_Internet_Says_On_Scalability_For_September_21%2C_2012.html">1327 high scalability-2012-09-21-Stuff The Internet Says On Scalability For September 21, 2012</a></p>
<p>16 0.77739888 <a title="271-lda-16" href="../high_scalability-2012/high_scalability-2012-01-19-Is_it_time_to_get_rid_of_the_Linux_OS_model_in_the_cloud%3F.html">1177 high scalability-2012-01-19-Is it time to get rid of the Linux OS model in the cloud?</a></p>
<p>17 0.77696949 <a title="271-lda-17" href="../high_scalability-2009/high_scalability-2009-07-21-Paper%3A_Parallelizing_the_Web_Browser.html">660 high scalability-2009-07-21-Paper: Parallelizing the Web Browser</a></p>
<p>18 0.77689993 <a title="271-lda-18" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>19 0.77586484 <a title="271-lda-19" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>20 0.77525127 <a title="271-lda-20" href="../high_scalability-2009/high_scalability-2009-06-22-Improving_performance_and_scalability_with_DDD.html">635 high scalability-2009-06-22-Improving performance and scalability with DDD</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
