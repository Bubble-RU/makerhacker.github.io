<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-318" href="#">high_scalability-2008-318</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-318-html" href="http://highscalability.com//blog/2008/5/14/new-facebook-chat-feature-scales-to-70-million-users-using-e.html">html</a></p><p>Introduction: Update :  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. [sent-3, score-0.478]
</p><p>2 While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. [sent-4, score-0.398]
</p><p>3 It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them. [sent-6, score-0.228]
</p><p>4 A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. [sent-7, score-0.371]
</p><p>5 After logging in the server sends a client roster information and presence information. [sent-8, score-0.587]
</p><p>6 Your client has to be present to receive the information. [sent-9, score-0.262]
</p><p>7 If your client wants to discover the capabilities of another client then a request is sent over the wire and some time later the response comes back. [sent-10, score-0.586]
</p><p>8 Facebook has the client open a persistent connection to the IM server and uses  long polling  to send requests and continually get data from the server. [sent-15, score-0.79]
</p><p>9 Long polling is a mixture of client pull and server push. [sent-16, score-0.592]
</p><p>10 It works by having the client make a request to the server. [sent-17, score-0.324]
</p><p>11 The client connection blocks until the server has data to return. [sent-18, score-0.41]
</p><p>12 When it does data is returned, the client processes it, and then is in position to make another request of the server and get any more data that has queued up in the mean time. [sent-19, score-0.388]
</p><p>13 From a client perspective I think this approach is workable, but obviously not ideal. [sent-22, score-0.332]
</p><p>14 Your client's IMs, presence changes, subscription requests, and chat states etc are all blocked on the polling loop, which wouldn't have a predictable latency. [sent-23, score-0.754]
</p><p>15 And consider all the data those IM servers must store up in between polling intervals. [sent-29, score-0.211]
</p><p>16 Breath in- streams of data come in and must be stored waiting for the polling loop. [sent-31, score-0.267]
</p><p>17 Breath out- the polling loops hit and all the data is written to the client and released from the server. [sent-32, score-0.473]
</p><p>18 Now add network bandwidth for all the XMPP and TCP protocol overhead and CPU to process it all and you are talking some serious scalability issues. [sent-36, score-0.142]
</p><p>19 Erlang is famously excellent for its concurrency prowess and equally famous for being poor at IO, so I imagine C++ was needed for efficiency. [sent-56, score-0.203]
</p><p>20 A problem Facebook probably doesn't have to worry about scaling is the XMPP roster (contact list). [sent-62, score-0.152]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('erlang', 0.392), ('xmpp', 0.328), ('client', 0.262), ('jabber', 0.238), ('im', 0.226), ('chat', 0.216), ('polling', 0.211), ('roster', 0.152), ('facebook', 0.13), ('eugene', 0.129), ('breath', 0.112), ('presence', 0.109), ('extensions', 0.096), ('vps', 0.089), ('protocol', 0.086), ('connection', 0.084), ('concurrency', 0.078), ('states', 0.077), ('predictable', 0.073), ('obviously', 0.07), ('devoting', 0.069), ('prowess', 0.069), ('pubsub', 0.069), ('threadson', 0.069), ('workable', 0.069), ('subscription', 0.068), ('blog', 0.067), ('overcame', 0.065), ('server', 0.064), ('persistent', 0.063), ('request', 0.062), ('ims', 0.059), ('ali', 0.059), ('ejabberd', 0.059), ('reusing', 0.059), ('applicationsby', 0.058), ('overhead', 0.056), ('famously', 0.056), ('come', 0.056), ('poses', 0.055), ('mixture', 0.055), ('tower', 0.055), ('open', 0.054), ('functioning', 0.053), ('io', 0.053), ('requests', 0.052), ('shiny', 0.052), ('challenge', 0.052), ('erik', 0.051), ('downsides', 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="318-tfidf-1" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update :  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><p>2 0.25844723 <a title="318-tfidf-2" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>3 0.19715194 <a title="318-tfidf-3" href="../high_scalability-2008/high_scalability-2008-10-27-Notify.me_Architecture_-_Synchronicity_Kills.html">431 high scalability-2008-10-27-Notify.me Architecture - Synchronicity Kills</a></p>
<p>Introduction: What's cool about starting a new project is you finally have a chance to do it right. You of course eventually mess everything up in your own way, but for that one moment the world has a perfect order, a rightness that feels satisfying and good. Arne Claassen, the CTO of notify.me, a brand new real time notification delivery service, is in this honeymoon period now.  Arne has been gracious enough to share with us his philosophy of how to build a notification service. I think you'll find it fascinating because Arne goes into a lot of useful detail about how his system works.    His main design philosophy is to minimize the bottlenecks that form around synchronous access, that is when  some resource is requested and the requestor ties up more resources, waiting for a response. If the requested resource can’t be delivered in a timely manner, more and more requests pile up until the server can’t accept any new ones. Nobody gets what they want and you have an outage. Breaking synchronous op</p><p>4 0.19097701 <a title="318-tfidf-4" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>5 0.18835765 <a title="318-tfidf-5" href="../high_scalability-2008/high_scalability-2008-01-30-The_AOL_XMPP_scalability_challenge.html">234 high scalability-2008-01-30-The AOL XMPP scalability challenge</a></p>
<p>Introduction: Large scale distributed instant messaging, presence based protocol are a real challenge. With big players adopting the standard, the XMPP (eXtensible Messaging and Presence Protocol) community is facing the need to validate protocol and implementations to even larger scale.</p><p>6 0.17089576 <a title="318-tfidf-6" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>7 0.16993813 <a title="318-tfidf-7" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>8 0.16836272 <a title="318-tfidf-8" href="../high_scalability-2008/high_scalability-2008-05-10-Hitting_300_SimbleDB_Requests_Per_Second_on_a_Small_EC2_Instance.html">317 high scalability-2008-05-10-Hitting 300 SimbleDB Requests Per Second on a Small EC2 Instance</a></p>
<p>9 0.16566005 <a title="318-tfidf-9" href="../high_scalability-2013/high_scalability-2013-04-25-Paper%3A_Making_reliable_distributed_systems_in_the_presence_of_software_errors.html">1446 high scalability-2013-04-25-Paper: Making reliable distributed systems in the presence of software errors</a></p>
<p>10 0.16145043 <a title="318-tfidf-10" href="../high_scalability-2009/high_scalability-2009-11-11-Hot_Scalability_Links_for_Nov_11_2009__.html">740 high scalability-2009-11-11-Hot Scalability Links for Nov 11 2009  </a></p>
<p>11 0.157626 <a title="318-tfidf-11" href="../high_scalability-2007/high_scalability-2007-07-23-GoogleTalk_Architecture.html">21 high scalability-2007-07-23-GoogleTalk Architecture</a></p>
<p>12 0.15582219 <a title="318-tfidf-12" href="../high_scalability-2012/high_scalability-2012-08-27-Zoosk_-_The_Engineering_behind_Real_Time_Communications.html">1312 high scalability-2012-08-27-Zoosk - The Engineering behind Real Time Communications</a></p>
<p>13 0.14712849 <a title="318-tfidf-13" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>14 0.13473806 <a title="318-tfidf-14" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>15 0.1300644 <a title="318-tfidf-15" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>16 0.12233295 <a title="318-tfidf-16" href="../high_scalability-2009/high_scalability-2009-01-25-Where_do_I_start%3F.html">501 high scalability-2009-01-25-Where do I start?</a></p>
<p>17 0.12137341 <a title="318-tfidf-17" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>18 0.12021938 <a title="318-tfidf-18" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>19 0.11607943 <a title="318-tfidf-19" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>20 0.11605763 <a title="318-tfidf-20" href="../high_scalability-2008/high_scalability-2008-10-01-Joyent_-_Cloud_Computing_Built_on_Accelerators.html">399 high scalability-2008-10-01-Joyent - Cloud Computing Built on Accelerators</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.199), (1, 0.114), (2, -0.02), (3, -0.052), (4, 0.033), (5, -0.023), (6, 0.027), (7, 0.073), (8, -0.074), (9, 0.015), (10, 0.031), (11, 0.094), (12, 0.03), (13, -0.058), (14, -0.062), (15, 0.001), (16, 0.046), (17, 0.019), (18, 0.025), (19, 0.003), (20, 0.081), (21, 0.004), (22, 0.052), (23, -0.022), (24, 0.127), (25, -0.023), (26, 0.136), (27, 0.017), (28, 0.059), (29, -0.008), (30, -0.019), (31, -0.059), (32, -0.017), (33, -0.046), (34, 0.049), (35, -0.003), (36, 0.062), (37, 0.002), (38, -0.065), (39, -0.04), (40, 0.045), (41, -0.011), (42, 0.075), (43, 0.008), (44, -0.016), (45, -0.067), (46, -0.04), (47, 0.059), (48, -0.012), (49, -0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95146781 <a title="318-lsi-1" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update :  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><p>2 0.83008784 <a title="318-lsi-2" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>3 0.78468055 <a title="318-lsi-3" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>4 0.73922634 <a title="318-lsi-4" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>Introduction: Our latest strategy is taken from a  great post by Paul Saab of Facebook , detailing how with changes Facebook has made to memcached they have:
  ...been able to scale memcached to handle 200,000 UDP requests per second with an average latency of 173 microseconds. The total throughput achieved is 300,000 UDP requests/s, but the latency at that request rate is too high to be useful in our system. This is an amazing increase from 50,000 UDP requests/s using the stock version of Linux and memcached.  
To scale Facebook has hundreds of thousands of TCP connections open to their memcached processes. First, this is still amazing. It's not so long ago you could have never done this. Optimizing connection use was always a priority because the OS simply couldn't handle large numbers of connections or large numbers of threads or large numbers of CPUs. To get to this point is a big accomplishment. Still, at that scale there are problems that are often solved.  Some of the problem Facebook faced a</p><p>5 0.73868895 <a title="318-lsi-5" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>Introduction: Several readers had follow-up questions in response to  How FarmVille Scales to Harvest 75 Million Players a Month . Here are Luke's response to those questions (and a few of mine).
   How does social networking makes things easier or harder?    
 The primary interesting aspect of social networking games is how you wind up with a graph of connected users who need to be access each other's data on a frequent basis. This makes the overall dataset difficult if not impossible to partition. 
   What are examples of the Facebook calls you try to avoid and how they impact game play?   
 We can make a call for facebook friend data to retrieve information about your friends playing the game. Normally, we show a friend ladder at the bottom of the game that shows friend information, including name and facebook photo.   
   Can you say where your cache is, what form it takes, and how much cached there is? Do you have a peering relationship with Facebook, as one might expect at that bandwidth?</p><p>6 0.72979379 <a title="318-lsi-6" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>7 0.70710683 <a title="318-lsi-7" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>8 0.69248974 <a title="318-lsi-8" href="../high_scalability-2009/high_scalability-2009-09-10-How_to_handle_so_many_socket_connection.html">699 high scalability-2009-09-10-How to handle so many socket connection</a></p>
<p>9 0.68513221 <a title="318-lsi-9" href="../high_scalability-2009/high_scalability-2009-01-13-Product%3A_Gearman_-_Open_Source_Message_Queuing_System.html">491 high scalability-2009-01-13-Product: Gearman - Open Source Message Queuing System</a></p>
<p>10 0.67671925 <a title="318-lsi-10" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>11 0.67187279 <a title="318-lsi-11" href="../high_scalability-2011/high_scalability-2011-03-09-Google_and_Netflix_Strategy%3A_Use_Partial_Responses_to_Reduce_Request_Sizes.html">1001 high scalability-2011-03-09-Google and Netflix Strategy: Use Partial Responses to Reduce Request Sizes</a></p>
<p>12 0.66673881 <a title="318-lsi-12" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>13 0.65554065 <a title="318-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-27-Zoosk_-_The_Engineering_behind_Real_Time_Communications.html">1312 high scalability-2012-08-27-Zoosk - The Engineering behind Real Time Communications</a></p>
<p>14 0.65469593 <a title="318-lsi-14" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>15 0.65297776 <a title="318-lsi-15" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>16 0.65218878 <a title="318-lsi-16" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>17 0.64922583 <a title="318-lsi-17" href="../high_scalability-2012/high_scalability-2012-12-17-11_Uses_For_the_Humble_Presents_Queue%2C_er%2C_Message_Queue.html">1373 high scalability-2012-12-17-11 Uses For the Humble Presents Queue, er, Message Queue</a></p>
<p>18 0.63934666 <a title="318-lsi-18" href="../high_scalability-2008/high_scalability-2008-10-27-Notify.me_Architecture_-_Synchronicity_Kills.html">431 high scalability-2008-10-27-Notify.me Architecture - Synchronicity Kills</a></p>
<p>19 0.63830608 <a title="318-lsi-19" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>20 0.63573849 <a title="318-lsi-20" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.106), (2, 0.26), (10, 0.048), (13, 0.011), (27, 0.013), (30, 0.036), (32, 0.012), (40, 0.015), (53, 0.126), (61, 0.113), (79, 0.081), (85, 0.086), (94, 0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94889963 <a title="318-lda-1" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update :  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><p>2 0.94820935 <a title="318-lda-2" href="../high_scalability-2011/high_scalability-2011-12-06-Instagram_Architecture%3A_14_Million_users%2C_Terabytes_of_Photos%2C_100s_of_Instances%2C_Dozens_of_Technologies.html">1152 high scalability-2011-12-06-Instagram Architecture: 14 Million users, Terabytes of Photos, 100s of Instances, Dozens of Technologies</a></p>
<p>Introduction: Instagram  is a free photo sharing and social networking service for your iPhone that has been an  instant success . Growing to  14 million users  in just over a year, they reached 150 million photos in August while amassing several terabytes of photos, and they did this with just 3 Instaneers, all on the Amazon stack.
 
The Instagram team has written up what can be considered the canonical description of an early stage startup in this era:  What Powers Instagram: Hundreds of Instances, Dozens of Technologies .
 
Instagram uses a pastiche of different technologies and strategies. The team is small yet has experience rapid growth riding the crest of a rising social and mobile wave, it uses a hybrid of SQL and NoSQL, it uses a ton of open source projects, they chose the cloud over colo, Amazon services are highly leveraged rather than building their own, reliability is through availability zones, async work scheduling links components together, the system is composed as much as possible</p><p>3 0.94297159 <a title="318-lda-3" href="../high_scalability-2012/high_scalability-2012-04-09-The_Instagram_Architecture_Facebook_Bought_for_a_Cool_Billion_Dollars.html">1224 high scalability-2012-04-09-The Instagram Architecture Facebook Bought for a Cool Billion Dollars</a></p>
<p>Introduction: It's been a well kept secret, but you may have heard  Facebook will Buy Photo-Sharing Service Instagram for $1 Billion . Just what is Facebook buying? Here's a quick gloss I did a little over a year ago on a presentation Instagram gave on their architecture. In that article I called Instagram's architecture the " canonical description of an early stage startup in this era." Little did we know how true that would turn out to be. If you want to learn how they did it then don't take a picture, just keep on reading...  
 
 
 
 Instagram  is a free photo sharing and social networking service for your iPhone that has been an  instant success . Growing to  14 million users  in just over a year (now 30 million users), they reached 150 million photos in August while amassing several terabytes of photos, and they did this with just 3 Instaneers, all on the Amazon stack.
 
The Instagram team has written up what can be considered the canonical description of an early stage startup in this era:  Wh</p><p>4 0.91600603 <a title="318-lda-4" href="../high_scalability-2013/high_scalability-2013-09-25-Great_Open_Source_Solution_for_Boring_HA_and_Scalability_Problems.html">1522 high scalability-2013-09-25-Great Open Source Solution for Boring HA and Scalability Problems</a></p>
<p>Introduction: This is a guest post about how boring and repetitive HA and scalability problems can be solved via Open Source so you can focus on the interesting tasks. The post was written by  Maarten Ectors , responsible for Cloud Strategy and  Frank Mueller , a Juju Core developer, at  Ubuntu / Canonical . 
 
High-availability and scalability are exciting in general but there are certain problems that experts see over and over again. The list is long but examples are setting up MySQL clustering, sharding Mongo, adding data nodes to a Hadoop cluster, monitoring with Ganglia, building continuous deployment solutions, integrating Memcached / Varnish / Nginx,… Why are we reinventing the wheel?
 
At Ubuntu we made it our goal to have the community solve these repetitive and often boring tasks. How often have you had to set-up MySQL replication and scale it? What if the next time you just simply do:
  
 juju deploy mysql  
 juju deploy mysql mysql-slave  
 juju add-relation mysql:master mysql-slave:slav</p><p>5 0.91362476 <a title="318-lda-5" href="../high_scalability-2011/high_scalability-2011-03-03-Stack_Overflow_Architecture_Update_-_Now_at_95_Million_Page_Views_a_Month.html">998 high scalability-2011-03-03-Stack Overflow Architecture Update - Now at 95 Million Page Views a Month</a></p>
<p>Introduction: A lot has happened since my first article on the  Stack Overflow Architecture . Contrary to the theme of that last article, which lavished attention on Stack Overflow's dedication to a scale-up strategy, Stack Overflow has both grown up and out in the last few years.
 
Stack Overflow has grown up by more then doubling in size to over 16 million users and multiplying its number of page views nearly 6 times to 95 million page views a month.  
 
Stack Overflow has grown out by expanding into the  Stack Exchange Network , which includes Stack Overflow, Server Fault, and Super User for a grand total of 43 different sites. That's a lot of fruitful multiplying going on.
 
What hasn't changed is Stack Overflow's openness about what they are doing. And that's what prompted this update. A recent series of posts talks a lot about how they've been handling their growth:  Stack Exchange’s Architecture in Bullet Points ,  Stack Overflow’s New York Data Center ,  Designing For Scalability of Manageme</p><p>6 0.91339135 <a title="318-lda-6" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>7 0.91333425 <a title="318-lda-7" href="../high_scalability-2008/high_scalability-2008-08-17-Strategy%3A_Drop_Memcached%2C_Add_More_MySQL_Servers.html">367 high scalability-2008-08-17-Strategy: Drop Memcached, Add More MySQL Servers</a></p>
<p>8 0.91239977 <a title="318-lda-8" href="../high_scalability-2012/high_scalability-2012-03-26-7_Years_of_YouTube_Scalability_Lessons_in_30_Minutes.html">1215 high scalability-2012-03-26-7 Years of YouTube Scalability Lessons in 30 Minutes</a></p>
<p>9 0.91115481 <a title="318-lda-9" href="../high_scalability-2008/high_scalability-2008-06-11-Pyshards_aspires_to_build_sharding_toolkit_for_Python.html">345 high scalability-2008-06-11-Pyshards aspires to build sharding toolkit for Python</a></p>
<p>10 0.91037393 <a title="318-lda-10" href="../high_scalability-2012/high_scalability-2012-09-21-Stuff_The_Internet_Says_On_Scalability_For_September_21%2C_2012.html">1327 high scalability-2012-09-21-Stuff The Internet Says On Scalability For September 21, 2012</a></p>
<p>11 0.91005915 <a title="318-lda-11" href="../high_scalability-2009/high_scalability-2009-08-07-The_Canonical_Cloud_Architecture_.html">674 high scalability-2009-08-07-The Canonical Cloud Architecture </a></p>
<p>12 0.90957326 <a title="318-lda-12" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>13 0.90894544 <a title="318-lda-13" href="../high_scalability-2007/high_scalability-2007-08-01-Product%3A_Memcached.html">52 high scalability-2007-08-01-Product: Memcached</a></p>
<p>14 0.90876478 <a title="318-lda-14" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>15 0.90863192 <a title="318-lda-15" href="../high_scalability-2011/high_scalability-2011-12-05-Stuff_The_Internet_Says_On_Scalability_For_December_5%2C_2011.html">1151 high scalability-2011-12-05-Stuff The Internet Says On Scalability For December 5, 2011</a></p>
<p>16 0.90850472 <a title="318-lda-16" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>17 0.90806127 <a title="318-lda-17" href="../high_scalability-2011/high_scalability-2011-07-15-Stuff_The_Internet_Says_On_Scalability_For_July_15%2C_2011.html">1080 high scalability-2011-07-15-Stuff The Internet Says On Scalability For July 15, 2011</a></p>
<p>18 0.90779376 <a title="318-lda-18" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<p>19 0.90761393 <a title="318-lda-19" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>20 0.90740603 <a title="318-lda-20" href="../high_scalability-2009/high_scalability-2009-04-08-N%2B1%2Bcaching_is_ok%3F.html">561 high scalability-2009-04-08-N+1+caching is ok?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
