<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-357" href="#">high_scalability-2008-357</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-357-html" href="http://highscalability.com//blog/2008/7/26/googles-paxos-made-live-an-engineering-perspective.html">html</a></p><p>Introduction: This is an unusually well written anduseful paper. It talks in detail about
experiences implementing a complex project, something we don't see very often.
They shockingly even admit that creating a working implementation of Paxos was
more difficult than just translating the pseudo code. Imagine that,
programmers aren't merely typists! I particularly like the explanation of the
Paxos algorithm and why anyone would care about it, working with disk
corruption, using leases to support simultaneous reads, using epoch numbers to
indicate a new master election, using snapshots to prevent unbounded logs,
using MultiOp to implement database transactions, how they tested the system,
and their openness with the various problems they had. A lot to learn
here.From the paper:We describe our experience building a fault-tolerant data-
base using the Paxos consensus algorithm. Despite the existing literature in
the field, building such a database proved to be non-trivial. We describe
selected algorithm</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paxos', 0.386), ('pseudo', 0.258), ('literature', 0.248), ('consensus', 0.246), ('algorithm', 0.228), ('algorithmic', 0.159), ('sequence', 0.143), ('variety', 0.135), ('replicas', 0.12), ('describe', 0.117), ('encountered', 0.109), ('algorithms', 0.106), ('measurements', 0.105), ('tolerate', 0.105), ('values', 0.103), ('mutually', 0.101), ('implementation', 0.101), ('specification', 0.097), ('indicate', 0.096), ('selected', 0.089)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="357-tfidf-1" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written anduseful paper. It talks in detail about
experiences implementing a complex project, something we don't see very often.
They shockingly even admit that creating a working implementation of Paxos was
more difficult than just translating the pseudo code. Imagine that,
programmers aren't merely typists! I particularly like the explanation of the
Paxos algorithm and why anyone would care about it, working with disk
corruption, using leases to support simultaneous reads, using epoch numbers to
indicate a new master election, using snapshots to prevent unbounded logs,
using MultiOp to implement database transactions, how they tested the system,
and their openness with the various problems they had. A lot to learn
here.From the paper:We describe our experience building a fault-tolerant data-
base using the Paxos consensus algorithm. Despite the existing literature in
the field, building such a database proved to be non-trivial. We describe
selected algorithm</p><p>2 0.22343262 <a title="357-tfidf-2" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>Introduction: If you are a normal human being and find thePaxos protocolconfusing, then this
paper, Paxos Made Moderately Complex, is a great find. Robbert van Renesse
from Cornell University has written a clear and well written paper with
excellent explanations.The Abstract:For anybody who has ever tried to
implement it, Paxos is by no means a simple protocol, even though it is based
on relatively simple invariants. This paper provides imperative pseudo-code
for the full Paxos (or Multi-Paxos) protocol without shying away from
discussing various implementation details. The initial description avoids
optimizations that complicate comprehension. Next we discuss liveness, and
list various optimizations that make the protocol practical.Related
ArticlesPaxos on HighScalability.com</p><p>3 0.21139561 <a title="357-tfidf-3" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>Introduction: Update: Barbara Liskov's Turing Award, and Byzantine Fault Tolerance.Henry
Robinson has created an excellent series of articles on consensus protocols.
We already covered his2 Phase Commitarticle and he also has a3 Phase
Commitarticle showing how to handle 2PC under single node failures.But that is
not enough! 3PC works well under node failures, but fails for network
failures. So another consensus mechanism is needed that handles both network
and node failures. And that'sPaxos.Paxos correctly handles both types of
failures, but it does this by becoming inaccessible if too many components
fail. This is the "liveness" property of protocols. Paxos waits until the
faults are fixed. Read queries can be handled, but updates will be blocked
until the protocol thinks it can make forward progress.The liveness of Paxos
is primarily dependent on network stability. In a distributed heterogeneous
environment you are at risk of losing the ability to make updates. Users hate
that.breakSo when compani</p><p>4 0.19371779 <a title="357-tfidf-4" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--
failure and latency--happen to good systems. The problem is always: how do you
do that? Murat Demirbas, Associate Professor at SUNY Buffalo, has a couple of
really good posts that can help:MDCC: Multi-Data Center Consistency andMaking
Geo-Replicated Systems Fast as Possible, Consistent when Necessary. In MDCC:
Multi-Data Center Consistency Murat discusses a paper that says synchronous
wide-area replication can be feasible. There's a quick and clear explanation
of Paxos and various optimizations that is worth the price of admission. We
find that strong consistency doesn't have to be lost across a WAN:The good
thing about using Paxos over the WAN is you /almost/ get the full CAP  (all
three properties: consistency, availability, and partition-freedom). As we
discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a
partition, Paxos keeps consistency over availability. But, Paxos can still
pr</p><p>5 0.17992176 <a title="357-tfidf-5" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus
protocols. Henry starts with a very useful discussion of what all this talk
about consensus really means:The consensus problem is the problem of getting a
set of nodes in a distributed system to agree on something - it might be a
value, a course of action or a decision. Achieving consensus allows a
distributed system to act as a single entity, with every individual node aware
of and in agreement with the actions of the whole of the network.In this
article Henry tackles Two-Phase Commit, the protocol most databases use to
arrive at a consensus for database writes. The article is very well written
with lots of pretty and informative pictures. He did a really good job.In
conclusion we learn 2PC is very efficient, a minimal number of messages are
exchanged and latency is low. The problem is when a co-ordinator fails
availability is dramatically reduced. This is why 2PC isn't generally used on
highly distributed systems</p><p>6 0.1733937 <a title="357-tfidf-6" href="../high_scalability-2009/high_scalability-2009-09-20-PaxosLease%3A_Diskless_Paxos_for_Leases.html">710 high scalability-2009-09-20-PaxosLease: Diskless Paxos for Leases</a></p>
<p>7 0.14224249 <a title="357-tfidf-7" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>8 0.14134064 <a title="357-tfidf-8" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>9 0.12472314 <a title="357-tfidf-9" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>10 0.11551641 <a title="357-tfidf-10" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>11 0.10790115 <a title="357-tfidf-11" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>12 0.10586502 <a title="357-tfidf-12" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>13 0.10380463 <a title="357-tfidf-13" href="../high_scalability-2013/high_scalability-2013-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_4th%2C_2013.html">1527 high scalability-2013-10-04-Stuff The Internet Says On Scalability For October 4th, 2013</a></p>
<p>14 0.098267891 <a title="357-tfidf-14" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>15 0.098115683 <a title="357-tfidf-15" href="../high_scalability-2009/high_scalability-2009-03-06-Cloud_Programming_Directly_Feeds_Cost_Allocation_Back_into_Software_Design.html">527 high scalability-2009-03-06-Cloud Programming Directly Feeds Cost Allocation Back into Software Design</a></p>
<p>16 0.094320893 <a title="357-tfidf-16" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>17 0.089169249 <a title="357-tfidf-17" href="../high_scalability-2009/high_scalability-2009-04-21-Thread_Pool_Engine_in_MS_CLR_4%2C_and_Work-Stealing_scheduling_algorithm.html">575 high scalability-2009-04-21-Thread Pool Engine in MS CLR 4, and Work-Stealing scheduling algorithm</a></p>
<p>18 0.088764474 <a title="357-tfidf-18" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>19 0.08468537 <a title="357-tfidf-19" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>20 0.08428663 <a title="357-tfidf-20" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.078), (2, -0.006), (3, 0.048), (4, 0.035), (5, 0.079), (6, 0.018), (7, -0.011), (8, -0.067), (9, 0.014), (10, -0.004), (11, 0.031), (12, -0.042), (13, -0.053), (14, 0.049), (15, 0.005), (16, 0.041), (17, 0.004), (18, 0.029), (19, -0.021), (20, 0.074), (21, 0.014), (22, -0.06), (23, 0.053), (24, -0.077), (25, -0.01), (26, -0.005), (27, 0.016), (28, 0.01), (29, -0.009), (30, -0.044), (31, 0.002), (32, -0.085), (33, -0.002), (34, -0.056), (35, -0.068), (36, -0.033), (37, -0.023), (38, 0.076), (39, 0.027), (40, -0.074), (41, 0.041), (42, -0.002), (43, 0.019), (44, -0.001), (45, 0.003), (46, -0.007), (47, 0.039), (48, 0.005), (49, -0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93149149 <a title="357-lsi-1" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written anduseful paper. It talks in detail about
experiences implementing a complex project, something we don't see very often.
They shockingly even admit that creating a working implementation of Paxos was
more difficult than just translating the pseudo code. Imagine that,
programmers aren't merely typists! I particularly like the explanation of the
Paxos algorithm and why anyone would care about it, working with disk
corruption, using leases to support simultaneous reads, using epoch numbers to
indicate a new master election, using snapshots to prevent unbounded logs,
using MultiOp to implement database transactions, how they tested the system,
and their openness with the various problems they had. A lot to learn
here.From the paper:We describe our experience building a fault-tolerant data-
base using the Paxos consensus algorithm. Despite the existing literature in
the field, building such a database proved to be non-trivial. We describe
selected algorithm</p><p>2 0.83774567 <a title="357-lsi-2" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>Introduction: If you are a normal human being and find thePaxos protocolconfusing, then this
paper, Paxos Made Moderately Complex, is a great find. Robbert van Renesse
from Cornell University has written a clear and well written paper with
excellent explanations.The Abstract:For anybody who has ever tried to
implement it, Paxos is by no means a simple protocol, even though it is based
on relatively simple invariants. This paper provides imperative pseudo-code
for the full Paxos (or Multi-Paxos) protocol without shying away from
discussing various implementation details. The initial description avoids
optimizations that complicate comprehension. Next we discuss liveness, and
list various optimizations that make the protocol practical.Related
ArticlesPaxos on HighScalability.com</p><p>3 0.82361931 <a title="357-lsi-3" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>Introduction: Neil Conwayfrom Berkeley CS is giving an advanced level talk ata meetup today
in San Francisco on a new paper: Logic and Lattices for Distributed
Programming \- extending set logic to support CRDT-style lattices. The
description of the meetup is probably the clearest introduction to the
paper:Developers are increasingly choosing datastores that sacrifice strong
consistency guarantees in exchange for improved performance and availability.
Unfortunately, writing reliable distributed programs without the benefit of
strong consistency can be very challenging. In this talk, I'll discuss work
from our group at UC Berkeley that aims to make it easier to write distributed
programs without relying on strong consistency. Bloom is a declarative
programming language for distributed computing, while CALM is an analysis
technique that identifies programs that are guaranteed to be eventually
consistent. I'll then discuss our recent work on extending CALM to support a
broader range of programs, drawin</p><p>4 0.81068182 <a title="357-lsi-4" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>Introduction: To scale in the large you have to partition. Data has to be spread around,
replicated, and kept consistent (keeping replicas sufficiently similar to one
another despite operations being submittedindependently at different sites).
The result is a highly available, well performing, and scalable
system.Partitioning is required, but it's a pain to do efficiently and
correctly. UntilQuantum teleportationbecomes a reality how data is kept
consistent across a bewildering number of failure scenarios is a key design
decision.This excellent paper by Yasushi Saito and Marc Shapiro takes us on a
wild ride (OK, maybe not so wild) of different approaches to achieving
consistency.What's cool about this paper is they go over some real systems
that we are familiar with and cover how they work: DNS (single-master, state-
transfer), Usenet (multi-master), PDAs (multi-master, state-transfer, manual
or application-specific conflict resolution), Bayou (multi-master, operation-
transfer, epidemic propagation</p><p>5 0.76785606 <a title="357-lsi-5" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>Introduction: The Declarative Imperative: Experiences and Conjectures in Distributed Logic
is written by UC Berkeley's Joseph Hellersteinfor a keynote speech he gave at
PODS. The video version of the talk is here. You may have heard about Mr.
Hellerstein through theBerkeley Orders Of Magnitude project (BOOM), whose
purpose is to help people build systems that are OOM (orders of magnitude)
bigger than are building today, with OOM less effort than traditional
programming methodologies. A noble goal which may be why BOOM was rated as a
top 10 emerging technology for 2010 byMIT Technology Review. Quite an
honor.The motivation for the talk is a familiar one: it's a dark period for
computer programming and if we don't learn how to write parallel programs the
children of Moore's law will destroy us all. We have more and more processors,
yet we are stuck on figuring out how the average programmer can exploit them.
The BOOM solution is the Bloom language which is based onDedalus: Dedalus is a
temporal logic</p><p>6 0.76778835 <a title="357-lsi-6" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>7 0.75265282 <a title="357-lsi-7" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>8 0.74860704 <a title="357-lsi-8" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>9 0.74391484 <a title="357-lsi-9" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>10 0.73869503 <a title="357-lsi-10" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>11 0.72240889 <a title="357-lsi-11" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>12 0.71736801 <a title="357-lsi-12" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>13 0.71025711 <a title="357-lsi-13" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>14 0.70680463 <a title="357-lsi-14" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>15 0.7057696 <a title="357-lsi-15" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>16 0.70079362 <a title="357-lsi-16" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>17 0.69965482 <a title="357-lsi-17" href="../high_scalability-2010/high_scalability-2010-12-16-7_Design_Patterns_for_Almost-infinite_Scalability.html">958 high scalability-2010-12-16-7 Design Patterns for Almost-infinite Scalability</a></p>
<p>18 0.69009179 <a title="357-lsi-18" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>19 0.68921423 <a title="357-lsi-19" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>20 0.68091685 <a title="357-lsi-20" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.121), (2, 0.193), (10, 0.052), (30, 0.025), (40, 0.033), (47, 0.018), (51, 0.024), (61, 0.099), (79, 0.104), (85, 0.024), (92, 0.12), (94, 0.094)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96781874 <a title="357-lda-1" href="../high_scalability-2008/high_scalability-2008-07-18-Robert_Scoble%27s_Rules_for_Successfully_Scaling_Startups.html">352 high scalability-2008-07-18-Robert Scoble's Rules for Successfully Scaling Startups</a></p>
<p>Introduction: Robert Scoble in an often poignantFriendFeed threadcommiserating PodTech's
unfortunate end, shared what he learned about creating a successful startup.
Here's a summary of a Robert's rules and why Machiavelli just may agree with
them:Have a story.Have everyone on board with that story.If anyone goes off of
that story, make sure they get on board immediately or fire them.Make sure
people are judged by the revenues they bring in. Those that bring in revenues
should get to run the place. People who don't bring in revenues should get
fewer and fewer responsibilities, not more and more.Work ONLY for a leader who
will make the tough decisions.Build a place where excellence is expected,
allowed, and is enabled.Fire idiots quickly.If your engineering team can't
give a media team good measurements, the entire company is in trouble. Only
things that are measured ever get improved.When your stars aren't listened to
the company is in trouble.Getting rid of the CEO, even if it's all his fault,
won'</p><p>2 0.94964904 <a title="357-lda-2" href="../high_scalability-2014/high_scalability-2014-04-23-Here%27s_a_1300_Year_Old_Solution_to_Resilience_-_Rebuild%2C_Rebuild%2C_Rebuild.html">1636 high scalability-2014-04-23-Here's a 1300 Year Old Solution to Resilience - Rebuild, Rebuild, Rebuild</a></p>
<p>Introduction: How is it possible that a wooden Shinto shrine built in the 7th century is
still standing? The answer depends on how you answer this philosophical head
scratcher: With nearly every cell in your body continually being replaced, are
you still the same person?The Ise Grand Shrine has been in continuous
existence for over 1300 years because every twenty years an exact replica has
been rebuilt on an adjacent footprint. The former temple is then
dismantled.Now that's resilience. If you want something to last make it a
living part of a culture. It's not so much the building that is remade, what
is rebuilt and passed down from generation to generation is the meme that the
shrine is important and worth preserving. The rest is an unfolding of that
imperative.You can see echoes of this same process in Open Source projects
like Linux and the libraries and frameworks that get themselves reconstructed
in each new environment.The patterns of recurrence in software are the result
of Darwinian selectio</p><p>same-blog 3 0.93255973 <a title="357-lda-3" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written anduseful paper. It talks in detail about
experiences implementing a complex project, something we don't see very often.
They shockingly even admit that creating a working implementation of Paxos was
more difficult than just translating the pseudo code. Imagine that,
programmers aren't merely typists! I particularly like the explanation of the
Paxos algorithm and why anyone would care about it, working with disk
corruption, using leases to support simultaneous reads, using epoch numbers to
indicate a new master election, using snapshots to prevent unbounded logs,
using MultiOp to implement database transactions, how they tested the system,
and their openness with the various problems they had. A lot to learn
here.From the paper:We describe our experience building a fault-tolerant data-
base using the Paxos consensus algorithm. Despite the existing literature in
the field, building such a database proved to be non-trivial. We describe
selected algorithm</p><p>4 0.93202287 <a title="357-lda-4" href="../high_scalability-2010/high_scalability-2010-06-09-Paper%3A_Propagation_Networks%3A_A_Flexible_and_Expressive_Substrate_for_Computation_.html">839 high scalability-2010-06-09-Paper: Propagation Networks: A Flexible and Expressive Substrate for Computation </a></p>
<p>Introduction: Alexey Radul in his fascinating 174 page dissertation Propagation Networks: A
Flexible and Expressive Substrate for Computation, offers to help us break
free of the tyranny of linear time by arranging computation as a network of
autonomous but interconnected machines.  We can do this byorganizing
computation as a network of interconnected machines of some kind, each of
which is free to run when it pleases, propagating  information around the
network as proves possible. The consequence of this freedom is that the
structure of the aggregate does not impose an order of time.The abstract from
his thesis is:In this dissertation I propose a shift in the foundations of
computation. Modern programming systems are not expressive enough. The
traditional image of a single computer that has global effects on a large
memory is too restrictive. The propagation paradigm replaces this with
computing by networks of local, independent, stateless machines interconnected
with stateful storage cells. In so</p><p>5 0.91583329 <a title="357-lda-5" href="../high_scalability-2011/high_scalability-2011-02-11-Stuff_The_Internet_Says_On_Scalability_For_February_11%2C_2011.html">988 high scalability-2011-02-11-Stuff The Internet Says On Scalability For February 11, 2011</a></p>
<p>Introduction: Submitted for your reading pleasure...A good night's sleep is why Facebook CTO
Bret Taylor says Friendfeed should have gone cloud, let others take the
midnight watch, even if it costs a bit more.James Urquhartwith an information
packed interview on a wide range of cloud topics: Cloud Expert Inside theCube
at Stata Conference. Highlights: cloud is an operations model, it is not a
technology, it is a way to apply technology to problems; the faster you can
get the resources into the hands of the people who use it the more money you
save overall; cloud is a cash flow story, not a savings story; services aren't
about servers or storage, they are about applications.Quotable Quotes:Ryan
Tomayko: Frameworks don't solve scalability problems, design solves
scalability problems. Via@GregSkloot@zuno: The golden rule of scalability: "it
can probably wait" look for other areas to save resources.@bihzad: joinserv
hit a scalability wall, but I'm pretty sure I can climb over it with
multiprocessingScal</p><p>6 0.91341096 <a title="357-lda-6" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>7 0.9095518 <a title="357-lda-7" href="../high_scalability-2009/high_scalability-2009-03-11-Sharding_and_Connection_Pools.html">532 high scalability-2009-03-11-Sharding and Connection Pools</a></p>
<p>8 0.90332401 <a title="357-lda-8" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>9 0.90320277 <a title="357-lda-9" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<p>10 0.8998239 <a title="357-lda-10" href="../high_scalability-2013/high_scalability-2013-02-07-Ask_HighScalability%3A_Web_asset_server_concept_-_3rd_party_software_available%3F.html">1402 high scalability-2013-02-07-Ask HighScalability: Web asset server concept - 3rd party software available?</a></p>
<p>11 0.89976221 <a title="357-lda-11" href="../high_scalability-2009/high_scalability-2009-03-06-Product%3A_Lightcloud_-_Key-Value_Database.html">528 high scalability-2009-03-06-Product: Lightcloud - Key-Value Database</a></p>
<p>12 0.89910799 <a title="357-lda-12" href="../high_scalability-2011/high_scalability-2011-01-20-75%25_Chance_of_Scale_-_Leveraging_the_New_Scaleogenic_Environment_for_Growth.html">976 high scalability-2011-01-20-75% Chance of Scale - Leveraging the New Scaleogenic Environment for Growth</a></p>
<p>13 0.89865607 <a title="357-lda-13" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>14 0.89855772 <a title="357-lda-14" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>15 0.89766395 <a title="357-lda-15" href="../high_scalability-2009/high_scalability-2009-06-30-Hot_New_Trend%3A_Linking_Clouds_Through_Cheap_IP_VPNs_Instead_of_Private_Lines_.html">645 high scalability-2009-06-30-Hot New Trend: Linking Clouds Through Cheap IP VPNs Instead of Private Lines </a></p>
<p>16 0.89583623 <a title="357-lda-16" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>17 0.89570397 <a title="357-lda-17" href="../high_scalability-2008/high_scalability-2008-03-04-Manage_Downtime_Risk_by_Connecting_Multiple_Data_Centers_into_a_Secure_Virtual_LAN.html">266 high scalability-2008-03-04-Manage Downtime Risk by Connecting Multiple Data Centers into a Secure Virtual LAN</a></p>
<p>18 0.89551806 <a title="357-lda-18" href="../high_scalability-2010/high_scalability-2010-02-12-Hot_Scalability_Links_for_February_12%2C_2010.html">776 high scalability-2010-02-12-Hot Scalability Links for February 12, 2010</a></p>
<p>19 0.89430845 <a title="357-lda-19" href="../high_scalability-2010/high_scalability-2010-08-23-Building_a_Scalable_Key-Value_Database%3A_Project_Hydracus.html">885 high scalability-2010-08-23-Building a Scalable Key-Value Database: Project Hydracus</a></p>
<p>20 0.89429432 <a title="357-lda-20" href="../high_scalability-2011/high_scalability-2011-01-28-Stuff_The_Internet_Says_On_Scalability_For_January_28%2C_2011.html">980 high scalability-2011-01-28-Stuff The Internet Says On Scalability For January 28, 2011</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
