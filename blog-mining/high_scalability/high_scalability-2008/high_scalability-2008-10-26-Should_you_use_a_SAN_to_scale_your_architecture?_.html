<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-430" href="#">high_scalability-2008-430</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-430-html" href="http://highscalability.com//blog/2008/10/26/should-you-use-a-san-to-scale-your-architecture.html">html</a></p><p>Introduction: This is a question everyone must struggle with when building out their
datacenter. Storage choices are always the ones I have the least confidence
in. David Marks in his blogYou Can Change It Later!asks the questionShould I
get a SAN to scale my site architecture?and answers no. A better solution is
to use commodity hardware, directly attach storage on servers, and partition
across servers to scale and for greater availability.David's reasoning is
interesting:A SAN creates a SPOF (single point of failure) that is dependent
on a vendor to fly and fix when there's a problem. This can lead to long down
times during this outage you have no access to your data at all.Using easily
available commodity hardware minimizes risks to your company, it's not just
about saving money. Zooming over to Fry's to buy emergency equipment provides
the kind of agility startups need in order to respond quickly to ever changing
situations.It's hard to beat the power and flexibility (backups, easy to add
storag</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('san', 0.292), ('zooming', 0.211), ('fry', 0.203), ('commodity', 0.198), ('struggle', 0.196), ('marks', 0.186), ('mirroring', 0.179), ('spof', 0.179), ('attach', 0.172), ('risks', 0.164), ('emergency', 0.16), ('minimizes', 0.158), ('confidence', 0.149), ('agility', 0.149), ('reasoning', 0.148), ('beat', 0.14), ('asks', 0.132), ('dependent', 0.132), ('fly', 0.129), ('equipment', 0.128)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="430-tfidf-1" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>Introduction: This is a question everyone must struggle with when building out their
datacenter. Storage choices are always the ones I have the least confidence
in. David Marks in his blogYou Can Change It Later!asks the questionShould I
get a SAN to scale my site architecture?and answers no. A better solution is
to use commodity hardware, directly attach storage on servers, and partition
across servers to scale and for greater availability.David's reasoning is
interesting:A SAN creates a SPOF (single point of failure) that is dependent
on a vendor to fly and fix when there's a problem. This can lead to long down
times during this outage you have no access to your data at all.Using easily
available commodity hardware minimizes risks to your company, it's not just
about saving money. Zooming over to Fry's to buy emergency equipment provides
the kind of agility startups need in order to respond quickly to ever changing
situations.It's hard to beat the power and flexibility (backups, easy to add
storag</p><p>2 0.12453565 <a title="430-tfidf-2" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>Introduction: Update:Presentation: Behind the Scenes at MySpace.com. Dan Farino, Chief
Systems Architect at MySpace shares details of some of MySpace's cool internal
operations tools.MySpace.com is one of the fastest growing site on the
Internet with 65 million subscribers and 260,000 new users registering each
day. Often criticized for poor performance, MySpace has had to tackle
scalability issues few other sites have faced. How did they do it?Site:
http://myspace.comInformation SourcesPresentation: Behind the Scenes at
MySpace.comInside MySpace.comPlatformASP.NET 2.0WindowsIISSQL ServerWhat's
Inside?300 million users.Pushes 100 gigabits/second to the internet. 10Gb/sec
is HTML content.4,500+ web servers windows 2003/IIS 6.0/APS.NET.1,200+ cache
servers running 64-bit Windows 2003. 16GB of objects cached in RAM.500+
database servers running 64-bit Windows and SQL Server 2005.MySpace processes
1.5 Billion page views per day and handles 2.3 million concurrent users during
the dayMembership Milestones</p><p>3 0.097318396 <a title="430-tfidf-3" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>Introduction: Update: Parascale's CTO on what's different about Parascale.Let's say you have
gigglebytes of data to store and you aren't sure you want to use aCDN.
Amazon'sS3doesn't excite you. And you aren't quite ready to join
thegridnation. You want to keep it all in house. Wouldn't it be nice to have
something like theGoogle File Systemyou could use to create a unified file
system out of all your disks sitting on all your nodes?According to Robin
Harris, a.k.aStorageMojo(a great blog BTW), you can now have your own
GFS:Parascale launches Google-like storage software.Parascalecalls their
softwate a Virtual Storage Network (VSN). It "aggregates disks across
commodity Linux x86 servers to deliver petabyte-scale file storage. With
features such as automated, transparent file replication and file migration,
Parascale eliminates storage hotspots and delivers massive read/write
bandwidth." Why should you care?I don't know about you, but the "storage
problem" is one the most frustrating parts of buildin</p><p>4 0.093723908 <a title="430-tfidf-4" href="../high_scalability-2013/high_scalability-2013-08-28-Sean_Hull%27s_20_Biggest_Bottlenecks_that_Reduce_and_Slow_Down_Scalability.html">1508 high scalability-2013-08-28-Sean Hull's 20 Biggest Bottlenecks that Reduce and Slow Down Scalability</a></p>
<p>Introduction: This article is a lightly edited version of 20 Obstacles to Scalability bySean
Hull (with permission) from the always excellent and thought provoking ACM
Queue.1. TWO-PHASE COMMITNormally when data is changed in a database, it is
written both to memory and to disk. When a commit happens, a relational
database makes a commitment to freeze the data somewhere on real storage
media. Remember, memory doesn't survive a crash or reboot. Even if the data is
cached in memory, the database still has to write it to disk. MySQL binary
logs or Oracle redo logs fit the bill.With a MySQL cluster or distributed file
system such as DRBD (Distributed Replicated Block Device) or Amazon Multi-AZ
(Multi-Availability Zone), a commit occurs not only locally, but also at the
remote end. A two-phase commit means waiting for an acknowledgment from the
far end. Because of network and other latency, those commits can be slowed
down by milliseconds, as though all the cars on a highway were slowed down by
heavy loa</p><p>5 0.093581229 <a title="430-tfidf-5" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>Introduction: This is a guest post by Steve Newman, co-founder of Writely (Google Docs),
tech lead on the Paxos-based synchronous replication in Megastore, and founder
of cloud service providerScalyr.com.Microsoft's Azure service suffered a
widely publicized outage on February 28th / 29th. Microsoft recently published
an excellentpostmortem. For anyone trying to run a high-availability service,
this incident can teach several important lessons.The central lesson is that,
no matter how much work you put into redundancy, problems will arise. Murphy
is strong and, I might say, creative; things go wrong. So preventative
measures are important, but how you react to problems is just as important.
It's interesting to review the Azure incident in this light.The postmortem is
worth reading in its entirety, but here's a quick summary: each time Azure
launches a new VM, it creates a "transfer certificate" to secure
communications with that VM. There was a bug in the code that determines the
certificate expirat</p><p>6 0.089793131 <a title="430-tfidf-6" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>7 0.088350751 <a title="430-tfidf-7" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>8 0.08715076 <a title="430-tfidf-8" href="../high_scalability-2014/high_scalability-2014-06-05-Cloud_Architecture_Revolution.html">1654 high scalability-2014-06-05-Cloud Architecture Revolution</a></p>
<p>9 0.084866628 <a title="430-tfidf-9" href="../high_scalability-2007/high_scalability-2007-07-30-allowed_contributed.html">49 high scalability-2007-07-30-allowed contributed</a></p>
<p>10 0.083716385 <a title="430-tfidf-10" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>11 0.083591409 <a title="430-tfidf-11" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>12 0.083528884 <a title="430-tfidf-12" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>13 0.081136025 <a title="430-tfidf-13" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>14 0.079767704 <a title="430-tfidf-14" href="../high_scalability-2011/high_scalability-2011-07-07-Myth%3A_Google_Uses_Server_Farms_So_You_Should_Too_-_Resurrection_of_the_Big-Ass_Machines.html">1075 high scalability-2011-07-07-Myth: Google Uses Server Farms So You Should Too - Resurrection of the Big-Ass Machines</a></p>
<p>15 0.078978747 <a title="430-tfidf-15" href="../high_scalability-2011/high_scalability-2011-04-27-Heroku_Emergency_Strategy%3A_Incident_Command_System_and_8_Hour_Ops_Rotations_for_Fresh_Minds.html">1030 high scalability-2011-04-27-Heroku Emergency Strategy: Incident Command System and 8 Hour Ops Rotations for Fresh Minds</a></p>
<p>16 0.078256339 <a title="430-tfidf-16" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>17 0.077447027 <a title="430-tfidf-17" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>18 0.077332467 <a title="430-tfidf-18" href="../high_scalability-2010/high_scalability-2010-01-04-11_Strategies_to_Rock_Your_Startup%E2%80%99s_Scalability_in_2010.html">757 high scalability-2010-01-04-11 Strategies to Rock Your Startup’s Scalability in 2010</a></p>
<p>19 0.077037603 <a title="430-tfidf-19" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>20 0.07696455 <a title="430-tfidf-20" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, 0.05), (2, 0.039), (3, -0.016), (4, -0.003), (5, -0.022), (6, 0.011), (7, -0.032), (8, -0.002), (9, -0.022), (10, -0.023), (11, -0.032), (12, -0.003), (13, 0.025), (14, 0.043), (15, 0.039), (16, 0.046), (17, 0.015), (18, -0.022), (19, 0.037), (20, 0.01), (21, 0.027), (22, 0.003), (23, 0.009), (24, -0.047), (25, -0.029), (26, 0.02), (27, -0.021), (28, -0.023), (29, 0.028), (30, -0.012), (31, 0.015), (32, 0.029), (33, -0.026), (34, -0.004), (35, 0.031), (36, -0.026), (37, 0.049), (38, 0.027), (39, 0.055), (40, 0.015), (41, -0.085), (42, -0.054), (43, 0.002), (44, 0.06), (45, -0.009), (46, 0.006), (47, 0.012), (48, -0.021), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95597541 <a title="430-lsi-1" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>Introduction: This is a question everyone must struggle with when building out their
datacenter. Storage choices are always the ones I have the least confidence
in. David Marks in his blogYou Can Change It Later!asks the questionShould I
get a SAN to scale my site architecture?and answers no. A better solution is
to use commodity hardware, directly attach storage on servers, and partition
across servers to scale and for greater availability.David's reasoning is
interesting:A SAN creates a SPOF (single point of failure) that is dependent
on a vendor to fly and fix when there's a problem. This can lead to long down
times during this outage you have no access to your data at all.Using easily
available commodity hardware minimizes risks to your company, it's not just
about saving money. Zooming over to Fry's to buy emergency equipment provides
the kind of agility startups need in order to respond quickly to ever changing
situations.It's hard to beat the power and flexibility (backups, easy to add
storag</p><p>2 0.76829934 <a title="430-lsi-2" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-
back power outages power outages. More details atlaughingsquid.breakSites like
SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties,
TypePad, LiveJournal and Vox are all down. The cause was an underground
explosion in a transformer vault under a manhole at 560 Mission Street. Flames
shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without
power.What's perplexing is the UPS backup and diesel generators didn't kick in
to bring the datacenter back on line. I've never toured that datacenter, but
they usually have massive backup systems. It's probably one of those multiple
simultaneous failure situations that you hope never happen in real life, but
too often do. Or maybe the infrastructure wasn't rolled out completely.Update:
the cause was a cascade of failures in a tightly couples system that could
never happen :-) Details atFailure Happens: A summary of the power outage a</p><p>3 0.72591639 <a title="430-lsi-3" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI
seized multiple racks of computers from DigitalOne, theseracks host websites
from many clients that just happened to be in the same racks as whomever they
are investigating. Downed sites include Instapaper, Curbed Network,
andPinboard. With thedensity of serversthese days many 1000s of sites could
easily have been effected.Sites like Pinboard were victims by association,
they did not inhale. This is an association sites have no control over. On a
shared hosting service, you have no control over your fellow VM mates. In a
cloud or a managed service, you have no control over which racks your servers
are in. So like second hand smoke, you get the disease by random association.
There's something inherently unfair about that.Acomment by illumin8 shows just
how Darth insidious this process can be:A popular method used by hackers is to
sign up for a virtual server with a stolen credit card. If they are careful
and only a</p><p>4 0.70412356 <a title="430-lsi-4" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>Introduction: This is a guest post by Steve Newman, co-founder of Writely (Google Docs),
tech lead on the Paxos-based synchronous replication in Megastore, and founder
of cloud service providerScalyr.com.Microsoft's Azure service suffered a
widely publicized outage on February 28th / 29th. Microsoft recently published
an excellentpostmortem. For anyone trying to run a high-availability service,
this incident can teach several important lessons.The central lesson is that,
no matter how much work you put into redundancy, problems will arise. Murphy
is strong and, I might say, creative; things go wrong. So preventative
measures are important, but how you react to problems is just as important.
It's interesting to review the Azure incident in this light.The postmortem is
worth reading in its entirety, but here's a quick summary: each time Azure
launches a new VM, it creates a "transfer certificate" to secure
communications with that VM. There was a bug in the code that determines the
certificate expirat</p><p>5 0.68293136 <a title="430-lsi-5" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>Introduction: Update: Parascale's CTO on what's different about Parascale.Let's say you have
gigglebytes of data to store and you aren't sure you want to use aCDN.
Amazon'sS3doesn't excite you. And you aren't quite ready to join
thegridnation. You want to keep it all in house. Wouldn't it be nice to have
something like theGoogle File Systemyou could use to create a unified file
system out of all your disks sitting on all your nodes?According to Robin
Harris, a.k.aStorageMojo(a great blog BTW), you can now have your own
GFS:Parascale launches Google-like storage software.Parascalecalls their
softwate a Virtual Storage Network (VSN). It "aggregates disks across
commodity Linux x86 servers to deliver petabyte-scale file storage. With
features such as automated, transparent file replication and file migration,
Parascale eliminates storage hotspots and delivers massive read/write
bandwidth." Why should you care?I don't know about you, but the "storage
problem" is one the most frustrating parts of buildin</p><p>6 0.67691016 <a title="430-lsi-6" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>7 0.66045427 <a title="430-lsi-7" href="../high_scalability-2010/high_scalability-2010-01-04-11_Strategies_to_Rock_Your_Startup%E2%80%99s_Scalability_in_2010.html">757 high scalability-2010-01-04-11 Strategies to Rock Your Startup’s Scalability in 2010</a></p>
<p>8 0.6558761 <a title="430-lsi-8" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>9 0.65331775 <a title="430-lsi-9" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>10 0.65277815 <a title="430-lsi-10" href="../high_scalability-2007/high_scalability-2007-07-25-Product%3A_NetApp_MetroCluster_Software.html">28 high scalability-2007-07-25-Product: NetApp MetroCluster Software</a></p>
<p>11 0.63767958 <a title="430-lsi-11" href="../high_scalability-2007/high_scalability-2007-11-06-Product%3A_ChironFS.html">143 high scalability-2007-11-06-Product: ChironFS</a></p>
<p>12 0.63622755 <a title="430-lsi-12" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>13 0.63586557 <a title="430-lsi-13" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>14 0.63377905 <a title="430-lsi-14" href="../high_scalability-2012/high_scalability-2012-10-11-RAMCube%3A_Exploiting_Network_Proximity_for_RAM-Based_Key-Value_Store.html">1338 high scalability-2012-10-11-RAMCube: Exploiting Network Proximity for RAM-Based Key-Value Store</a></p>
<p>15 0.6263364 <a title="430-lsi-15" href="../high_scalability-2009/high_scalability-2009-08-05-Stack_Overflow_Architecture.html">671 high scalability-2009-08-05-Stack Overflow Architecture</a></p>
<p>16 0.62484086 <a title="430-lsi-16" href="../high_scalability-2012/high_scalability-2012-12-14-Stuff_The_Internet_Says_On_Scalability_For_December_14%2C_2012.html">1372 high scalability-2012-12-14-Stuff The Internet Says On Scalability For December 14, 2012</a></p>
<p>17 0.624663 <a title="430-lsi-17" href="../high_scalability-2011/high_scalability-2011-08-26-Stuff_The_Internet_Says_On_Scalability_For_August_26%2C_2011.html">1106 high scalability-2011-08-26-Stuff The Internet Says On Scalability For August 26, 2011</a></p>
<p>18 0.62282377 <a title="430-lsi-18" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>19 0.62131542 <a title="430-lsi-19" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>20 0.62099755 <a title="430-lsi-20" href="../high_scalability-2007/high_scalability-2007-10-30-Paper%3A_Dynamo%3A_Amazon%E2%80%99s_Highly_Available_Key-value_Store.html">139 high scalability-2007-10-30-Paper: Dynamo: Amazon’s Highly Available Key-value Store</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.112), (2, 0.11), (10, 0.589), (79, 0.082)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93574607 <a title="430-lda-1" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>Introduction: This is a question everyone must struggle with when building out their
datacenter. Storage choices are always the ones I have the least confidence
in. David Marks in his blogYou Can Change It Later!asks the questionShould I
get a SAN to scale my site architecture?and answers no. A better solution is
to use commodity hardware, directly attach storage on servers, and partition
across servers to scale and for greater availability.David's reasoning is
interesting:A SAN creates a SPOF (single point of failure) that is dependent
on a vendor to fly and fix when there's a problem. This can lead to long down
times during this outage you have no access to your data at all.Using easily
available commodity hardware minimizes risks to your company, it's not just
about saving money. Zooming over to Fry's to buy emergency equipment provides
the kind of agility startups need in order to respond quickly to ever changing
situations.It's hard to beat the power and flexibility (backups, easy to add
storag</p><p>2 0.93445557 <a title="430-lda-2" href="../high_scalability-2007/high_scalability-2007-12-10-1_Master%2C_N_Slaves.html">178 high scalability-2007-12-10-1 Master, N Slaves</a></p>
<p>Introduction: Hello all,Reading the site you can note that "1 Master for writes, N Slaves
for reads" scheme is used offen.How is this implemented? Who decides where
writes and reads go? Something in application level or specific database
proxies, like Slony-I?Thanks.</p><p>3 0.92592174 <a title="430-lda-3" href="../high_scalability-2010/high_scalability-2010-08-07-ArchCamp%3A_Scalable_Databases_%28NoSQL%29.html">874 high scalability-2010-08-07-ArchCamp: Scalable Databases (NoSQL)</a></p>
<p>Introduction: ArchCamp: Scalable Databasess (NoSQL)The ArchCamp unconference was held this
past Friday at HackerDojo in Mountain View, CA.  There was plenty of pizza,
beer, and great conversation.  This session started out free-form, but shaped
up pretty quickly into a discussion of the popular open source scalable NoSQL
databases and the architectural categories in which they belong.</p><p>4 0.91671646 <a title="430-lda-4" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>Introduction: If you stayed up all night watching the life reaffirmingCuriosity landing on
Mars, then this paper,High-Performance Concurrency Control Mechanisms for
Main-Memory Databases, has nothing to do with that at all, but it is an
excellent look at how to use optimistic MVCC schemes to reduce lock overhead
on in-memory datastructures:A database system optimized for in-memory storage
can support much higher transaction rates than current systems. However,
standard concurrency control methods used today do not scale to the high
transaction rates achievable by such systems. In this paper we introduce two
efficient concurrency control methods specifically designed for main-memory
databases. Both use multiversioning to isolate read-only transactions from
updates but differ in how atomicity is ensured: one is optimistic and one is
pessimistic. To avoid expensive context switching, transactions never block
during normal processing but they may have to wait before commit to ensure
correct serializatio</p><p>5 0.91460711 <a title="430-lda-5" href="../high_scalability-2011/high_scalability-2011-06-22-It%27s_the_Fraking_IOPS_-_1_SSD_is_44%2C000_IOPS%2C_Hard_Drive_is_180.html">1066 high scalability-2011-06-22-It's the Fraking IOPS - 1 SSD is 44,000 IOPS, Hard Drive is 180</a></p>
<p>Introduction: Planning your next buildout and thinking SSDs are still far in the future?
Still too expensive, too low density. Hard disks are cheap, familiar, and
store lots of stuff. In this short and entertaining video Wikia's Artur
Bergmanwants to change your mind about SSDs. SSDs are for today, get with the
math already.Here's Artur's logic:Wikia is all SSD in production. The new
Wikia file servers have a theoretical read rate of ~10GB/sec sequential,
6GB/sec random and 1.2 million IOPs. If you can't do math or love the past,
you love spinning rust. If you are awesome you love SSDs.SSDs are cheaper than
drives using the most relevant metric: $/GB/IOPS. 1 SSD is 44,000 IOPS and one
hard drive is 180 IOPS. Need 1 SSD instead of 50 hard drives.With 8 million
files there's a 9 minute fsck. Full backup in 12 minutes (X-25M based).4
GB/sec random read average latency 1 msec.2.2 GB/sec random write average
latency 1 msec.50TBs of SSDs in one machine for $80,000. With the densities
most products can ski</p><p>6 0.91358399 <a title="430-lda-6" href="../high_scalability-2007/high_scalability-2007-12-02-a8cjdbc_-_update_verision_1.3.html">171 high scalability-2007-12-02-a8cjdbc - update verision 1.3</a></p>
<p>7 0.91358048 <a title="430-lda-7" href="../high_scalability-2007/high_scalability-2007-12-02-Database-Clustering%3A_a8cjdbc_-_update%3A_version_1.3.html">170 high scalability-2007-12-02-Database-Clustering: a8cjdbc - update: version 1.3</a></p>
<p>8 0.9041276 <a title="430-lda-8" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>9 0.86957723 <a title="430-lda-9" href="../high_scalability-2010/high_scalability-2010-01-27-Hot_Scalability_Links_for_January_28_2010.html">767 high scalability-2010-01-27-Hot Scalability Links for January 28 2010</a></p>
<p>10 0.86438817 <a title="430-lda-10" href="../high_scalability-2014/high_scalability-2014-04-21-This_is_why_Microsoft_won._And_why_they_lost..html">1635 high scalability-2014-04-21-This is why Microsoft won. And why they lost.</a></p>
<p>11 0.81191444 <a title="430-lda-11" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>12 0.79611188 <a title="430-lda-12" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>13 0.79072416 <a title="430-lda-13" href="../high_scalability-2009/high_scalability-2009-04-27-Some_Questions_from_a_newbie.html">584 high scalability-2009-04-27-Some Questions from a newbie</a></p>
<p>14 0.77965158 <a title="430-lda-14" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>15 0.75469798 <a title="430-lda-15" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>16 0.74341571 <a title="430-lda-16" href="../high_scalability-2009/high_scalability-2009-08-28-Strategy%3A_Solve_Only_80_Percent_of_the_Problem.html">689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</a></p>
<p>17 0.7153585 <a title="430-lda-17" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>18 0.70070559 <a title="430-lda-18" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>19 0.69267923 <a title="430-lda-19" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>20 0.67351949 <a title="430-lda-20" href="../high_scalability-2008/high_scalability-2008-02-22-Kevin%27s_Great_Adventures_in_SSDland.html">257 high scalability-2008-02-22-Kevin's Great Adventures in SSDland</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
