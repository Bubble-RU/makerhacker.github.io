<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1611" href="#">high_scalability-2014-1611</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1611-html" href="http://highscalability.com//blog/2014/3/12/paper-scalable-eventually-consistent-counters-over-unreliabl.html">html</a></p><p>Introduction: Counting at scale in a distributed environment issurprisingly hard. And it's a
subject we've covered before in various ways:Big Data Counting: How to count a
billion distinct objects using only 1.5KB of Memory,How to update video views
count effectively?,Numbers Everyone Should Know (sharded
counters).Kellabyte(which is an excellent blog) inScalable Eventually
Consistent Counterstalks about how the Cassandra counter implementation scores
well on the scalability and high availability front, but in so doing has "over
and under counting problem in partitioned environments."Which is often fine.
But if you want more accuracy there's a PN-counter, which is aCRDT (convergent
replicated data type)where "all the changes made to a counter on each node
rather than storing and modifying a single value so that you can merge all the
values into the proper final value. Of course the trade-off here is additional
storage and processing but there are ways to optimize this."And there's a
paper you can co</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('counters', 0.59), ('counting', 0.266), ('ecdc', 0.235), ('handoff', 0.191), ('counter', 0.17), ('count', 0.156), ('consistent', 0.152), ('unreliable', 0.138), ('classic', 0.129), ('eventually', 0.122), ('inscalable', 0.106), ('unsuitable', 0.106), ('overcomes', 0.106), ('cap', 0.106), ('incremented', 0.1), ('crdt', 0.095), ('associative', 0.092), ('retaining', 0.089), ('convergent', 0.089), ('replicated', 0.089)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1611-tfidf-1" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>Introduction: Counting at scale in a distributed environment issurprisingly hard. And it's a
subject we've covered before in various ways:Big Data Counting: How to count a
billion distinct objects using only 1.5KB of Memory,How to update video views
count effectively?,Numbers Everyone Should Know (sharded
counters).Kellabyte(which is an excellent blog) inScalable Eventually
Consistent Counterstalks about how the Cassandra counter implementation scores
well on the scalability and high availability front, but in so doing has "over
and under counting problem in partitioned environments."Which is often fine.
But if you want more accuracy there's a PN-counter, which is aCRDT (convergent
replicated data type)where "all the changes made to a counter on each node
rather than storing and modifying a single value so that you can merge all the
values into the proper final value. Of course the trade-off here is additional
storage and processing but there are ways to optimize this."And there's a
paper you can co</p><p>2 0.21638148 <a title="1611-tfidf-2" href="../high_scalability-2007/high_scalability-2007-07-16-Blog%3A_MySQL_Performance_Blog_-_Everything_about_MySQL_Performance._.html">15 high scalability-2007-07-16-Blog: MySQL Performance Blog - Everything about MySQL Performance. </a></p>
<p>Introduction: Follow this blog and you'll learn a lot about MySQL and how to make it sing.A
Quick Hit of What's InsideWorking with large data sets in MySQL, PHP Large
result sets and summary tables, Implementing efficient counters with
MySQL.Site:http://www.mysqlperformanceblog.com/</p><p>3 0.17256439 <a title="1611-tfidf-3" href="../high_scalability-2012/high_scalability-2012-04-05-Big_Data_Counting%3A_How_to_count_a_billion_distinct_objects_using_only_1.5KB_of_Memory.html">1222 high scalability-2012-04-05-Big Data Counting: How to count a billion distinct objects using only 1.5KB of Memory</a></p>
<p>Introduction: This is a guest post by Matt Abrams (@abramsm), from Clearspring, discussing
how they are able to accurately estimate the cardinality of sets with billions
of distinct elements using surprisingly small data structures. Their servers
receive well over 100 billion events per month.AtClearspringwe like to count
things. Counting the number of distinct elements (the cardinality) of a set is
challenge when the cardinality of the set is large.To better understand the
challenge of determining the cardinality of large sets let's imagine that you
have a 16 character ID and you'd like to count the number of distinct IDs that
you've seen in your logs. Here is an example:4f67bfc603106cb2These 16
characters represent 128 bits. 65K IDs would require 1 megabyte of space. We
receive over 3 billion events per day, and each event has an ID. Those IDs
require 384,000,000,000 bits or 45 gigabytes of storage. And that is just the
space that the ID field requires! To get the cardinality of IDs in our daily
e</p><p>4 0.14431967 <a title="1611-tfidf-4" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>Introduction: Facebook did it again. They've built another system capable of doing something
useful with ginormous streams of realtime data. Last time we saw Facebook
release their New Real-Time Messaging System: HBase To Store 135+ Billion
Messages A Month. This time it's a realtime analytics system handlingover 20
billion events per day (200,000 events per second) with a lag of less than 30
seconds. Alex Himel, Engineering Manager at Facebook, explains what they've
built (video) and the scale required:Social plugins have become an important
and growing source of traffic for millions of websites over the past year. We
released a new version of Insights for Websites last week to give site owners
better analytics on how people interact with their content and to help them
optimize their websites in real time. To accomplish this, we had to engineer a
system that could process over 20 billion events per day (200,000 events per
second) with a lag of less than 30 seconds. Alex does an excellent job with
t</p><p>5 0.13634582 <a title="1611-tfidf-5" href="../high_scalability-2009/high_scalability-2009-04-10-counting_%23_of_views%2C_calculating_most-least_viewed.html">564 high scalability-2009-04-10-counting # of views, calculating most-least viewed</a></p>
<p>Introduction: I'm seeking for a design pattern or advice or directions.I need to count
views/downloads of a set of resources, let them to be identified by their
respective URLs.This is not a big problem. I also need to keep a list of
viewed/downloaded resources in the last X days. This list needs to be updated
every now and then to reflect real last X days of usage. So resources that
were requested prior to X days get evicted from it.So it's sort of a black
box, you feed messages (download request) in and it gives you that list of
URLs with counters on the other end.How would you go about designing it?</p><p>6 0.13354024 <a title="1611-tfidf-6" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>7 0.12355543 <a title="1611-tfidf-7" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>8 0.12287726 <a title="1611-tfidf-8" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>9 0.12263577 <a title="1611-tfidf-9" href="../high_scalability-2008/high_scalability-2008-02-21-Tracking_usage_of_public_resources_-_throttling_accesses_per_hour.html">256 high scalability-2008-02-21-Tracking usage of public resources - throttling accesses per hour</a></p>
<p>10 0.090922162 <a title="1611-tfidf-10" href="../high_scalability-2008/high_scalability-2008-04-01-How_to_update_video_views_count_effectively%3F.html">294 high scalability-2008-04-01-How to update video views count effectively?</a></p>
<p>11 0.090259872 <a title="1611-tfidf-11" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>12 0.089800522 <a title="1611-tfidf-12" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>13 0.088625304 <a title="1611-tfidf-13" href="../high_scalability-2008/high_scalability-2008-02-05-SLA_monitoring.html">241 high scalability-2008-02-05-SLA monitoring</a></p>
<p>14 0.086022824 <a title="1611-tfidf-14" href="../high_scalability-2012/high_scalability-2012-04-25-The_Anatomy_of_Search_Technology%3A_blekko%E2%80%99s_NoSQL_database.html">1233 high scalability-2012-04-25-The Anatomy of Search Technology: blekko’s NoSQL database</a></p>
<p>15 0.083944321 <a title="1611-tfidf-15" href="../high_scalability-2008/high_scalability-2008-04-19-How_to_build_a_real-time_analytics_system%3F.html">304 high scalability-2008-04-19-How to build a real-time analytics system?</a></p>
<p>16 0.081078246 <a title="1611-tfidf-16" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>17 0.076323852 <a title="1611-tfidf-17" href="../high_scalability-2010/high_scalability-2010-11-30-NoCAP_%E2%80%93_Part_III_%E2%80%93_GigaSpaces_clustering_explained...html">950 high scalability-2010-11-30-NoCAP – Part III – GigaSpaces clustering explained..</a></p>
<p>18 0.075819463 <a title="1611-tfidf-18" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>19 0.072061993 <a title="1611-tfidf-19" href="../high_scalability-2013/high_scalability-2013-07-17-Steve_Ballmer_Says_Microsoft_has_Over_1_Million_Servers_-_What_Does_that_Really_Mean%3F.html">1493 high scalability-2013-07-17-Steve Ballmer Says Microsoft has Over 1 Million Servers - What Does that Really Mean?</a></p>
<p>20 0.070910625 <a title="1611-tfidf-20" href="../high_scalability-2009/high_scalability-2009-04-06-How_do_you_monitor_the_performance_of_your_cluster%3F.html">558 high scalability-2009-04-06-How do you monitor the performance of your cluster?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, 0.058), (2, -0.006), (3, 0.027), (4, 0.007), (5, 0.068), (6, 0.001), (7, 0.005), (8, -0.014), (9, 0.014), (10, 0.028), (11, 0.029), (12, -0.039), (13, -0.008), (14, 0.02), (15, 0.027), (16, 0.021), (17, -0.01), (18, 0.026), (19, -0.031), (20, 0.021), (21, 0.057), (22, -0.022), (23, 0.066), (24, -0.026), (25, -0.04), (26, 0.044), (27, 0.034), (28, 0.048), (29, -0.018), (30, 0.038), (31, -0.043), (32, -0.011), (33, 0.06), (34, -0.03), (35, -0.048), (36, -0.024), (37, -0.044), (38, 0.03), (39, -0.001), (40, -0.006), (41, 0.014), (42, 0.02), (43, 0.017), (44, 0.072), (45, 0.027), (46, -0.01), (47, 0.027), (48, 0.032), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94997692 <a title="1611-lsi-1" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>Introduction: Counting at scale in a distributed environment issurprisingly hard. And it's a
subject we've covered before in various ways:Big Data Counting: How to count a
billion distinct objects using only 1.5KB of Memory,How to update video views
count effectively?,Numbers Everyone Should Know (sharded
counters).Kellabyte(which is an excellent blog) inScalable Eventually
Consistent Counterstalks about how the Cassandra counter implementation scores
well on the scalability and high availability front, but in so doing has "over
and under counting problem in partitioned environments."Which is often fine.
But if you want more accuracy there's a PN-counter, which is aCRDT (convergent
replicated data type)where "all the changes made to a counter on each node
rather than storing and modifying a single value so that you can merge all the
values into the proper final value. Of course the trade-off here is additional
storage and processing but there are ways to optimize this."And there's a
paper you can co</p><p>2 0.77026021 <a title="1611-lsi-2" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>Introduction: For a great Christmas read forgetThe Night Before Christmas, a heart warming
poem written by Clement Moore for his children, that created the modern idea
of Santa Clause we all know and anticipate each Christmas eve. Instead, curl
up with a some potent eggnog, nog being any drink made with rum, and read
CRDTs: Consistency without concurrency control by Mihai Letia, Nuno Preguiรงa,
and Marc Shapiro, which talks about CRDTs (Commutative Replicated Data Type),a
data type whose operations commute when they are concurrent.From the
introduction, which also serves as a nice concise overview of distributed
consistency issues:Shared read-only data is easy to scale by using well-
understood replication techniques. However, sharing mutable data at a large
scale is a difficult problem, because of the CAP impossibility result [5]. Two
approaches dominate in practice. One ensures scalability by giving up
consistency guarantees, for instance using the Last-Writer-Wins (LWW) approach
[7]. The alternati</p><p>3 0.74314463 <a title="1611-lsi-3" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>Introduction: To scale in the large you have to partition. Data has to be spread around,
replicated, and kept consistent (keeping replicas sufficiently similar to one
another despite operations being submittedindependently at different sites).
The result is a highly available, well performing, and scalable
system.Partitioning is required, but it's a pain to do efficiently and
correctly. UntilQuantum teleportationbecomes a reality how data is kept
consistent across a bewildering number of failure scenarios is a key design
decision.This excellent paper by Yasushi Saito and Marc Shapiro takes us on a
wild ride (OK, maybe not so wild) of different approaches to achieving
consistency.What's cool about this paper is they go over some real systems
that we are familiar with and cover how they work: DNS (single-master, state-
transfer), Usenet (multi-master), PDAs (multi-master, state-transfer, manual
or application-specific conflict resolution), Bayou (multi-master, operation-
transfer, epidemic propagation</p><p>4 0.70332676 <a title="1611-lsi-4" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>Introduction: We are not yet at the End of History for database theory asPeter Bailisand
theDatabase Groupat UC Berkeley continue to prove with a greatcompanion blog
post to their new paper: Scalable Atomic Visibility with RAMP Transactions. I
like the approach of pairing a blog post with a paper. A paper almost by
definition is formal, but a blog post can help put a paper in context and give
it some heart.From the abstract:Databases can provide scalability by
partitioning data across several servers. However, multi-partition, multi-
operation transactional access is often expensive, employing coordination-
intensive locking, validation, or scheduling mechanisms. Accordingly, many
real-world systems avoid mechanisms that provide useful semantics formulti-
partition operations. This leads to incorrect behavior for a large class of
applications including secondary indexing,foreign key enforcement, and
materialized view maintenance. In this work, we identify a new isolation model
--Read Atomic (RA) iso</p><p>5 0.69121647 <a title="1611-lsi-5" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>Introduction: We've seen a lot ofNoSQLaction lately built around distributed hash tables.
Btrees are getting jealous. Btrees, once the king of the database world, want
their throne back.Paul Buchheitsurfaced a paper:A practical scalable
distributed B-treeby Marcos K. Aguilera and Wojciech Golab, that might help
spark a revolution.From the Abstract:We propose a new algorithm for a
practical, fault tolerant, and scalable B-tree distributed over a set of
servers. Our algorithm supports practical features not present in prior work:
transactions that allow atomic execution of multiple operations over multiple
B-trees, online migration of B-tree nodes between servers, and dynamic
addition and removal of servers. Moreover, our algorithm is conceptually
simple: we use transactions to manipulate B-tree nodes so that clients need
not use complicated concurrency and locking protocols used in prior work. To
execute these transactions quickly, we rely on three techniques: (1) We use
optimistic concurrency contro</p><p>6 0.68626225 <a title="1611-lsi-6" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>7 0.67897242 <a title="1611-lsi-7" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>8 0.67581123 <a title="1611-lsi-8" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>9 0.67300957 <a title="1611-lsi-9" href="../high_scalability-2009/high_scalability-2009-09-20-PaxosLease%3A_Diskless_Paxos_for_Leases.html">710 high scalability-2009-09-20-PaxosLease: Diskless Paxos for Leases</a></p>
<p>10 0.65500718 <a title="1611-lsi-10" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>11 0.65412539 <a title="1611-lsi-11" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>12 0.63915533 <a title="1611-lsi-12" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>13 0.63580769 <a title="1611-lsi-13" href="../high_scalability-2012/high_scalability-2012-04-05-Big_Data_Counting%3A_How_to_count_a_billion_distinct_objects_using_only_1.5KB_of_Memory.html">1222 high scalability-2012-04-05-Big Data Counting: How to count a billion distinct objects using only 1.5KB of Memory</a></p>
<p>14 0.63534915 <a title="1611-lsi-14" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>15 0.63166577 <a title="1611-lsi-15" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>16 0.62728965 <a title="1611-lsi-16" href="../high_scalability-2009/high_scalability-2009-06-22-Improving_performance_and_scalability_with_DDD.html">635 high scalability-2009-06-22-Improving performance and scalability with DDD</a></p>
<p>17 0.62672871 <a title="1611-lsi-17" href="../high_scalability-2009/high_scalability-2009-06-10-Managing_cross_partition_transactions_in_a_distributed_KV_system.html">625 high scalability-2009-06-10-Managing cross partition transactions in a distributed KV system</a></p>
<p>18 0.62189656 <a title="1611-lsi-18" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>19 0.62036788 <a title="1611-lsi-19" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>20 0.61979616 <a title="1611-lsi-20" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.102), (2, 0.225), (5, 0.029), (10, 0.027), (28, 0.229), (61, 0.025), (79, 0.078), (85, 0.055), (91, 0.011), (94, 0.115)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90611041 <a title="1611-lda-1" href="../high_scalability-2009/high_scalability-2009-04-10-Facebook%27s_Aditya_giving_presentation_on_Facebook_Architecture.html">562 high scalability-2009-04-10-Facebook's Aditya giving presentation on Facebook Architecture</a></p>
<p>Introduction: Facebook's engg. director aditya talks about facebook architecture. How they
use mysql, php and memcache. How they have modified the above to suit their
requirements.</p><p>2 0.88911134 <a title="1611-lda-2" href="../high_scalability-2012/high_scalability-2012-08-01-Prismatic_Update%3A_Machine_Learning_on_Documents_and_Users.html">1294 high scalability-2012-08-01-Prismatic Update: Machine Learning on Documents and Users</a></p>
<p>Introduction: In update toPrismatic Architecture - Using Machine Learning on Social Networks
to Figure Out What You Should Read on the Web, Jason Wolfe, even in the face
of deadening fatigue from long nights spent getting their iPhone app out, has
gallantly agreed to talk a little more about Primatic's approach to Machine
Learning.Documents and users are two areas where Prismatic applies ML (machine
learning):ML on DocumentsGiven an HTML document: learn how to extract the main
text of the page (rather than the sidebar, footer, comments, etc), its title,
author, best images, etcdetermine features for relevance (e.g., what the
article is about, topics, etc.)The setup for most of these tasks is pretty
typical. Models are trained using big batch jobs on other machines that read
data from s3, save the learned parameter files to s3, and then read (and
periodically refresh) the models from s3 in the ingest pipeline.All of the
data that flows out of the system can be fed back into this pipeline, which
helps</p><p>same-blog 3 0.8602162 <a title="1611-lda-3" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>Introduction: Counting at scale in a distributed environment issurprisingly hard. And it's a
subject we've covered before in various ways:Big Data Counting: How to count a
billion distinct objects using only 1.5KB of Memory,How to update video views
count effectively?,Numbers Everyone Should Know (sharded
counters).Kellabyte(which is an excellent blog) inScalable Eventually
Consistent Counterstalks about how the Cassandra counter implementation scores
well on the scalability and high availability front, but in so doing has "over
and under counting problem in partitioned environments."Which is often fine.
But if you want more accuracy there's a PN-counter, which is aCRDT (convergent
replicated data type)where "all the changes made to a counter on each node
rather than storing and modifying a single value so that you can merge all the
values into the proper final value. Of course the trade-off here is additional
storage and processing but there are ways to optimize this."And there's a
paper you can co</p><p>4 0.85846317 <a title="1611-lda-4" href="../high_scalability-2009/high_scalability-2009-05-25-non-sequential%2C_unique_identifier%2C_strategy_question.html">606 high scalability-2009-05-25-non-sequential, unique identifier, strategy question</a></p>
<p>Introduction: (Please bare with me, I'm a new, passionate, confident and terrified
programmer :D )Background:I'm pre-launch and 1 year into the development of my
application. My target is to be able to eventually handle millions of
registered users with 5-10% of them concurrent. Up to this point I've used
auto-increment to assign unique identifiers to rows. I am now considering
switching to a non-sequential strategy. Oh, I'm using the LAMP
configuration.My reasons for avoiding auto-increment:1. Complicates
replication when scaling horizontally. Risk of collision is significant (when
running multiple masters). Note: I've read the other entries in this forum
that relate to ID generation and there have been some great suggestions --
including a strategy that uses auto-increment in a way that avoids this
pitfall... That said, I'm still nervous about it.2. Potential bottleneck when
retrieving/assigning IDs -- IDs assigned at the database.My reasons for being
nervous about non-sequential IDs:1. To guarant</p><p>5 0.84279424 <a title="1611-lda-5" href="../high_scalability-2009/high_scalability-2009-12-17-Oracle_and_IBM_databases%3A_Disk-based_vs_In-memory_databases_.html">752 high scalability-2009-12-17-Oracle and IBM databases: Disk-based vs In-memory databases </a></p>
<p>Introduction: Current disk based RDBMS can run out of steam when processing large data. Can
these problems be solved by migrating from a disk based RDBMS to an IMDB? Any
limitations? To find out, I tested one of each from the two leading vendors
who together hold 70% of the market share - Oracle's11gandTimesTen 11g, and
IBM'sDB2 v9.5andsolidDB 6.3.read more atBigDataMatters.com</p><p>6 0.82804435 <a title="1611-lda-6" href="../high_scalability-2011/high_scalability-2011-04-27-Heroku_Emergency_Strategy%3A_Incident_Command_System_and_8_Hour_Ops_Rotations_for_Fresh_Minds.html">1030 high scalability-2011-04-27-Heroku Emergency Strategy: Incident Command System and 8 Hour Ops Rotations for Fresh Minds</a></p>
<p>7 0.80878431 <a title="1611-lda-7" href="../high_scalability-2010/high_scalability-2010-09-17-Hot_Scalability_Links_For_Sep_17%2C_2010.html">903 high scalability-2010-09-17-Hot Scalability Links For Sep 17, 2010</a></p>
<p>8 0.80735362 <a title="1611-lda-8" href="../high_scalability-2013/high_scalability-2013-08-23-Stuff_The_Internet_Says_On_Scalability_For_August_23%2C_2013.html">1506 high scalability-2013-08-23-Stuff The Internet Says On Scalability For August 23, 2013</a></p>
<p>9 0.80250937 <a title="1611-lda-9" href="../high_scalability-2012/high_scalability-2012-06-08-Stuff_The_Internet_Says_On_Scalability_For_June_8%2C_2012.html">1261 high scalability-2012-06-08-Stuff The Internet Says On Scalability For June 8, 2012</a></p>
<p>10 0.78307199 <a title="1611-lda-10" href="../high_scalability-2013/high_scalability-2013-01-28-DuckDuckGo_Architecture_-_1_Million_Deep_Searches_a_Day_and_Growing.html">1395 high scalability-2013-01-28-DuckDuckGo Architecture - 1 Million Deep Searches a Day and Growing</a></p>
<p>11 0.77902716 <a title="1611-lda-11" href="../high_scalability-2013/high_scalability-2013-04-12-Stuff_The_Internet_Says_On_Scalability_For_April_12%2C_2013.html">1439 high scalability-2013-04-12-Stuff The Internet Says On Scalability For April 12, 2013</a></p>
<p>12 0.77296811 <a title="1611-lda-12" href="../high_scalability-2010/high_scalability-2010-04-08-Hot_Scalability_Links_for_April_8%2C_2010.html">806 high scalability-2010-04-08-Hot Scalability Links for April 8, 2010</a></p>
<p>13 0.7676515 <a title="1611-lda-13" href="../high_scalability-2012/high_scalability-2012-07-30-Prismatic_Architecture_-_Using_Machine_Learning_on_Social_Networks_to_Figure_Out_What_You_Should_Read_on_the_Web_.html">1293 high scalability-2012-07-30-Prismatic Architecture - Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web </a></p>
<p>14 0.76557863 <a title="1611-lda-14" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>15 0.76511627 <a title="1611-lda-15" href="../high_scalability-2011/high_scalability-2011-04-14-Strategy%3A_Cache_Application_Start_State_to_Reduce_Spin-up_Times.html">1023 high scalability-2011-04-14-Strategy: Cache Application Start State to Reduce Spin-up Times</a></p>
<p>16 0.76403832 <a title="1611-lda-16" href="../high_scalability-2012/high_scalability-2012-01-13-Stuff_The_Internet_Says_On_Scalability_For_January_13%2C_2012.html">1174 high scalability-2012-01-13-Stuff The Internet Says On Scalability For January 13, 2012</a></p>
<p>17 0.76336926 <a title="1611-lda-17" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<p>18 0.76304305 <a title="1611-lda-18" href="../high_scalability-2010/high_scalability-2010-07-22-How_can_we_spark_the_movement_of_research_out_of_the_Ivory_Tower_and_into_production%3F.html">863 high scalability-2010-07-22-How can we spark the movement of research out of the Ivory Tower and into production?</a></p>
<p>19 0.75356239 <a title="1611-lda-19" href="../high_scalability-2008/high_scalability-2008-03-04-Manage_Downtime_Risk_by_Connecting_Multiple_Data_Centers_into_a_Secure_Virtual_LAN.html">266 high scalability-2008-03-04-Manage Downtime Risk by Connecting Multiple Data Centers into a Secure Virtual LAN</a></p>
<p>20 0.75319338 <a title="1611-lda-20" href="../high_scalability-2011/high_scalability-2011-07-22-Stuff_The_Internet_Says_On_Scalability_For_July_22%2C_2011.html">1084 high scalability-2011-07-22-Stuff The Internet Says On Scalability For July 22, 2011</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
