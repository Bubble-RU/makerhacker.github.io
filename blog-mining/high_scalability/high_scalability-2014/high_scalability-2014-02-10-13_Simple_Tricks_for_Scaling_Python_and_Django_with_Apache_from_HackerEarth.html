<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1593" href="#">high_scalability-2014-1593</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1593-html" href="http://highscalability.com//blog/2014/2/10/13-simple-tricks-for-scaling-python-and-django-with-apache-f.html">html</a></p><p>Introduction: HackerEarth  is a coding skill practice and testing service that in a series of well written articles describes the trials and tribulations of building their site and how they overcame them:  Scaling Python/Django application with Apache and mod_wsgi ,  Programming challenges, uptime, and mistakes in 2013 , P ost-mortem: The big outage on January 25, 2014 ,  The Robust Realtime Server ,  100,000 strong - CodeFactory server ,  Scaling database with Django and HAProxy ,  Continuous Deployment System ,  HackerEarth Technology Stack .
 
What characterizes these articles and makes them especially helpful is a drive for improvement and an openness towards reporting what didn't work and how they figured out what would work.
 
As they say, mistakes happen when you are building a complex product with a team of just 3-4 engineers, but investing in infrastructure allowed them to take more breaks, roam the streets of Bangalore while their servers are happily serving thousands of requests every min</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 What characterizes these articles and makes them especially helpful is a drive for improvement and an openness towards reporting what didn't work and how they figured out what would work. [sent-2, score-0.453]
</p><p>2 By including only what you need you can cut in half the number of modules loaded. [sent-8, score-0.193]
</p><p>3 Generally a better choice for high-traffic servers because it has a smaller memory footprint than the prefork MPM. [sent-10, score-0.19]
</p><p>4 Static files are served from CloudFront and experimentation showed this was more efficient, processes/threads are free to handle new requests instantaneously rather than waiting for a request to arrive on the older connection. [sent-12, score-0.343]
</p><p>5 The number of threads and processes is constant, which makes resource consumption predictable and protects against traffic spikes. [sent-14, score-0.159]
</p><p>6 They show the configuration they use after much experimentation, which favors their application type, which is more CPU intensive than memory intensive. [sent-16, score-0.105]
</p><p>7 This information helped them significantly reduced the number of servers we had to run and made the application more stable and resilient to traffic bursts. [sent-21, score-0.495]
</p><p>8 There's no pride in throwing servers at a large number of requests. [sent-27, score-0.447]
</p><p>9 This means making sure, for example, that a request doesn't query the database 20 times. [sent-28, score-0.095]
</p><p>10 Asynchronous code-checker server queueing system . [sent-29, score-0.323]
</p><p>11 Rewriting the code-checker server queueing system to make it asynchronous significantly reduced the process overhead on their frontend servers. [sent-30, score-0.846]
</p><p>12 io” module is not able to scale past 150 simultaneous connections. [sent-33, score-0.128]
</p><p>13 Sharding the database reduced overhead on single database and further reduced query latencies. [sent-36, score-0.699]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hackerearth', 0.246), ('reduced', 0.209), ('apache', 0.19), ('tornado', 0.187), ('server', 0.178), ('experimentation', 0.177), ('pride', 0.177), ('queueing', 0.145), ('uptime', 0.131), ('frontend', 0.131), ('module', 0.128), ('realtime', 0.128), ('mistakes', 0.12), ('servers', 0.112), ('characterizes', 0.112), ('mpm', 0.112), ('modules', 0.111), ('overcame', 0.105), ('streets', 0.105), ('favors', 0.105), ('celery', 0.105), ('leaked', 0.105), ('towards', 0.1), ('trials', 0.1), ('toolchain', 0.1), ('mortem', 0.1), ('constant', 0.097), ('database', 0.095), ('roam', 0.093), ('significantly', 0.092), ('overhead', 0.091), ('crunching', 0.091), ('instantaneously', 0.091), ('bangalore', 0.091), ('happily', 0.089), ('openness', 0.087), ('investing', 0.085), ('number', 0.082), ('articles', 0.081), ('footprint', 0.078), ('protects', 0.077), ('cloudfront', 0.077), ('deployment', 0.076), ('throwing', 0.076), ('arrive', 0.075), ('skill', 0.074), ('january', 0.074), ('tweaking', 0.073), ('gloss', 0.073), ('figured', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9999997 <a title="1593-tfidf-1" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>Introduction: HackerEarth  is a coding skill practice and testing service that in a series of well written articles describes the trials and tribulations of building their site and how they overcame them:  Scaling Python/Django application with Apache and mod_wsgi ,  Programming challenges, uptime, and mistakes in 2013 , P ost-mortem: The big outage on January 25, 2014 ,  The Robust Realtime Server ,  100,000 strong - CodeFactory server ,  Scaling database with Django and HAProxy ,  Continuous Deployment System ,  HackerEarth Technology Stack .
 
What characterizes these articles and makes them especially helpful is a drive for improvement and an openness towards reporting what didn't work and how they figured out what would work.
 
As they say, mistakes happen when you are building a complex product with a team of just 3-4 engineers, but investing in infrastructure allowed them to take more breaks, roam the streets of Bangalore while their servers are happily serving thousands of requests every min</p><p>2 0.14006311 <a title="1593-tfidf-2" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>Introduction: This is a guest post by  Dave Hagler  Systems Architect at AOL. 
  The AOL homepages receive more than  8 million visitors per day .  That’s more daily viewers than Good Morning America or the Today Show on television.  Over a billion page views are served each month.  AOL.com has been a major internet destination since 1996, and still has a strong following of loyal users.
   The architecture for AOL.com is in it’s 5th generation .  It has essentially been rebuilt from scratch 5 times over two decades.  The current architecture was designed 6 years ago.  Pieces have been upgraded and new components have been added along the way, but the overall design remains largely intact.  The code, tools, development and deployment processes are highly tuned over 6 years of continual improvement, making the AOL.com architecture battle tested and very stable.
  The engineering team is made up of developers, testers, and operations and  totals around 25 people .  The majority are in Dulles, Virginia</p><p>3 0.12530622 <a title="1593-tfidf-3" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>Introduction: In  Don’t panic! Here’s how to quickly scale your mobile apps   Mike Maelzer  paints a wonderful picture of how  Avocado , a mobile app for connecting couples, evolved to handle 30x traffic within a few weeks. If you are just getting started then this is a great example to learn from.
 
What I liked: it's well written, packing a lot of useful information in a little space; it's failure driven, showing the process of incremental change driven by purposeful testing and production experience; it shows awareness of what's important, in their case, user signup; a replica setup was used for testing, a nice cloud benefit. 
 
Their Biggest lesson learned is a good one:
  It would have been great to start the scaling process much earlier. Due to time pressure we had to make compromises –like dropping four of our media resizer boxes. While throwing more hardware at some scaling problems does work, it’s less than ideal.  
Here's my gloss on the article:
  Evolution One - Make it Work  
When just</p><p>4 0.12187757 <a title="1593-tfidf-4" href="../high_scalability-2013/high_scalability-2013-08-28-Sean_Hull%27s_20_Biggest_Bottlenecks_that_Reduce_and_Slow_Down_Scalability.html">1508 high scalability-2013-08-28-Sean Hull's 20 Biggest Bottlenecks that Reduce and Slow Down Scalability</a></p>
<p>Introduction: This article is a lightly edited version of  20 Obstacles to Scalability  by  Sean Hull  (  with permission) from the always excellent and thought provoking  ACM Queue  .
  1. TWO-PHASE COMMIT  
Normally when data is changed in a database, it is written both to memory and to disk. When a commit happens, a relational database makes a commitment to freeze the data somewhere on real storage media. Remember, memory doesn't survive a crash or reboot. Even if the data is cached in memory, the database still has to write it to disk. MySQL binary logs or Oracle redo logs fit the bill.
 
With a MySQL cluster or distributed file system such as DRBD (Distributed Replicated Block Device) or Amazon Multi-AZ (Multi-Availability Zone), a commit occurs not only locally, but also at the remote end. A two-phase commit means waiting for an acknowledgment from the far end. Because of network and other latency, those commits can be slowed down by milliseconds, as though all the cars on a highway were slowe</p><p>5 0.11665728 <a title="1593-tfidf-5" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>Introduction: When you are on the bleeding edge of scale like Facebook is, you run into some interesting problems. As of 2008  Facebook had over 800 memcached servers  supplying over 28 terabytes of cache. With those staggering numbers it's a fair bet to think they've seen their share of Dr. House worthy memcached problems.
 
Jeff Rothschild, Vice President of Technology at Facebook,  describes one such problem  they've dubbed the  Multiget Hole. 
 
  You fall into the multiget hole when memcached servers are   CPU bound  , adding more memcached servers seems like the right way to add more capacity so more requests can be served, but against all logic adding servers doesn't help serve more requests. This puts you in a hole that simply adding more servers can't dig you out of. What's the treatment?  
 
  Dr. House would immediately notice the hidden clue, we are talking requests not memory. We aren't running out of memory to store stuff, we are   running out of CPU power to process requests  .</p><p>6 0.1161325 <a title="1593-tfidf-6" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>7 0.11469498 <a title="1593-tfidf-7" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>8 0.1138802 <a title="1593-tfidf-8" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>9 0.11170661 <a title="1593-tfidf-9" href="../high_scalability-2011/high_scalability-2011-02-22-Is_Node.js_Becoming_a_Part_of_the_Stack%3F_SimpleGeo_Says_Yes..html">993 high scalability-2011-02-22-Is Node.js Becoming a Part of the Stack? SimpleGeo Says Yes.</a></p>
<p>10 0.11151495 <a title="1593-tfidf-10" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>11 0.11125731 <a title="1593-tfidf-11" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>12 0.11067966 <a title="1593-tfidf-12" href="../high_scalability-2009/high_scalability-2009-09-10-The_technology_behind_Tornado%2C_FriendFeed%27s_web_server.html">700 high scalability-2009-09-10-The technology behind Tornado, FriendFeed's web server</a></p>
<p>13 0.11022496 <a title="1593-tfidf-13" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>14 0.10969077 <a title="1593-tfidf-14" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<p>15 0.1062147 <a title="1593-tfidf-15" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>16 0.10614605 <a title="1593-tfidf-16" href="../high_scalability-2012/high_scalability-2012-07-16-Cinchcast_Architecture_-_Producing_1%2C500_Hours_of_Audio_Every_Day.html">1284 high scalability-2012-07-16-Cinchcast Architecture - Producing 1,500 Hours of Audio Every Day</a></p>
<p>17 0.10475542 <a title="1593-tfidf-17" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>18 0.10371152 <a title="1593-tfidf-18" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>19 0.10329507 <a title="1593-tfidf-19" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>20 0.10200936 <a title="1593-tfidf-20" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.206), (1, 0.071), (2, -0.018), (3, -0.138), (4, 0.017), (5, 0.011), (6, 0.062), (7, -0.033), (8, -0.03), (9, 0.007), (10, -0.02), (11, -0.02), (12, 0.05), (13, -0.036), (14, -0.033), (15, -0.006), (16, 0.014), (17, 0.039), (18, -0.008), (19, 0.009), (20, 0.024), (21, -0.038), (22, 0.003), (23, 0.016), (24, 0.089), (25, -0.002), (26, -0.022), (27, -0.014), (28, -0.028), (29, 0.005), (30, -0.012), (31, 0.002), (32, -0.017), (33, -0.002), (34, 0.023), (35, -0.005), (36, -0.014), (37, 0.009), (38, -0.015), (39, 0.018), (40, 0.063), (41, -0.028), (42, -0.041), (43, -0.058), (44, 0.011), (45, -0.047), (46, 0.033), (47, 0.015), (48, 0.039), (49, 0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95989841 <a title="1593-lsi-1" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>Introduction: HackerEarth  is a coding skill practice and testing service that in a series of well written articles describes the trials and tribulations of building their site and how they overcame them:  Scaling Python/Django application with Apache and mod_wsgi ,  Programming challenges, uptime, and mistakes in 2013 , P ost-mortem: The big outage on January 25, 2014 ,  The Robust Realtime Server ,  100,000 strong - CodeFactory server ,  Scaling database with Django and HAProxy ,  Continuous Deployment System ,  HackerEarth Technology Stack .
 
What characterizes these articles and makes them especially helpful is a drive for improvement and an openness towards reporting what didn't work and how they figured out what would work.
 
As they say, mistakes happen when you are building a complex product with a team of just 3-4 engineers, but investing in infrastructure allowed them to take more breaks, roam the streets of Bangalore while their servers are happily serving thousands of requests every min</p><p>2 0.7796914 <a title="1593-lsi-2" href="../high_scalability-2009/high_scalability-2009-04-16-Serving_250M_quotes-day_at_CNBC.com_with_aiCache.html">573 high scalability-2009-04-16-Serving 250M quotes-day at CNBC.com with aiCache</a></p>
<p>Introduction: As traffic  to  cnbc.com continued to grow, we found ourselves in an all-too-familiar situation where one feels that a BIG change in how things are done was in order, the status-quo was a road to nowhere. The spending on HW, amount of space and power required to host additional servers, less-than-stellar response times, having to resort to frequent "micro"-caching and similar tricks to try to improve code performance - all of these were surfacing in plain sight, hard to ignore.                     While code base could clearly be improved, the limited Dev resources and having to innovate to stay competitive always limits ability to go about refactoring. So how can one go about addressing performance and other needs without a full blown effort across the entire team ? For us, the answer was aiCache - a Web caching and application acceleration product (aicache.com).                       The idea behind caching is simple - handle the requests before they ever hit your regular Apache<->JK</p><p>3 0.77266508 <a title="1593-lsi-3" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>Introduction: Slashdot effect  : overwhelming unprepared sites with an avalanche of reader's clicks after being mentioned on Slashdot. Sure, we now have the "Digg effect" and other hot new stars, but Slashdot was the original. And like many stars from generations past, Slashdot plays the elder statesman's role with with class, dignity, and restraint. Yet with millions and millions of users Slashdot is still box office gold and more than keeps up with the young'ins. And with age comes the wisdom of learning how to handle all those users. Just how does Slashdot scale and what can you learn by going old school?       Site: http://slashdot.org       Information Sources          Slashdot's Setup, Part 1- Hardware     Slashdot's Setup, Part 2- Software     History of Slashdot Part 3- Going Corporate     The History of Slashdot Part 4 - Yesterday, Today, Tomorrow     The Platform      MySQL   Linux (CentOS/RHEL)   Pound   Apache   Perl   Memcached   LVS    The Stats      Started building the system in 1999</p><p>4 0.7659812 <a title="1593-lsi-4" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>Introduction: This is a guest a post by Alvaro Videla describing their architecture for  Poppen.de , a popular German dating site. This site is very much NSFW, so be careful before clicking on the link. What I found most interesting is how they manage to sucessfully blend a little of the old with a little of the new, using technologies like Nginx, MySQL, CouchDB, and Erlang, Memcached, RabbitMQ, PHP, Graphite, Red5, and Tsung.
  What is Poppen.de?  
Poppen.de (NSFW) is the top dating website in  Germany, and while it may be a small site compared to giants like Flickr  or Facebook, we believe it's a nice architecture to learn from if you  are starting to get some scaling problems.
  The  Stats   
 2.000.000 users 
 20.000  concurrent users 
 300.000 private messages per day 
 250.000  logins per day 
 We have a team  of  eleven developers, two designers and two sysadmins for this project. 
   Business Model  
The site works with a  freemium model, where users can do for free things like: 
  
 Search</p><p>5 0.76580602 <a title="1593-lsi-5" href="../high_scalability-2010/high_scalability-2010-08-23-6_Ways_to_Kill_Your_Servers_-__Learning_How_to_Scale_the_Hard_Way.html">884 high scalability-2010-08-23-6 Ways to Kill Your Servers -  Learning How to Scale the Hard Way</a></p>
<p>Introduction: This is a guest post by Steffen Konerow, author of the    High Performance Blog   .  
 
Learning how to scale isn’t easy without any prior experience. Nowadays you have plenty of websites like  highscalability.com  to get some inspiration, but unfortunately there is no solution that fits all websites and needs. You still have to think on your own to find a concept that works for your requirements. So did I.
 
A few years ago, my bosses came to me and said “We’ve got a new project for you. It’s the relaunch of a website that has already 1 million users a month. You have to build the website and make sure we’ll be able to grow afterwards”. I was already an experienced coder, but not in these dimensions, so I had to start learning how to scale – the hard way.
 
The software behind the website was a PHP content management system, based on Smarty and MySQL. The first task was finding a proper hosting company who had the experience and would also manage the servers for us. After some researc</p><p>6 0.76470518 <a title="1593-lsi-6" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>7 0.75963897 <a title="1593-lsi-7" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>8 0.75773448 <a title="1593-lsi-8" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>9 0.75756097 <a title="1593-lsi-9" href="../high_scalability-2014/high_scalability-2014-01-14-SharePoint_VPS_solution.html">1579 high scalability-2014-01-14-SharePoint VPS solution</a></p>
<p>10 0.75380039 <a title="1593-lsi-10" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>11 0.75057781 <a title="1593-lsi-11" href="../high_scalability-2013/high_scalability-2013-03-13-Iron.io_Moved_From_Ruby_to_Go%3A_28_Servers_Cut_and_Colossal_Clusterf%2A%2Aks_Prevented.html">1423 high scalability-2013-03-13-Iron.io Moved From Ruby to Go: 28 Servers Cut and Colossal Clusterf**ks Prevented</a></p>
<p>12 0.74792176 <a title="1593-lsi-12" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>13 0.74722719 <a title="1593-lsi-13" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>14 0.74412364 <a title="1593-lsi-14" href="../high_scalability-2012/high_scalability-2012-02-21-Pixable_Architecture_-_Crawling%2C_Analyzing%2C_and_Ranking_20_Million_Photos_a_Day.html">1197 high scalability-2012-02-21-Pixable Architecture - Crawling, Analyzing, and Ranking 20 Million Photos a Day</a></p>
<p>15 0.73910075 <a title="1593-lsi-15" href="../high_scalability-2012/high_scalability-2012-10-04-LinkedIn_Moved_from_Rails_to_Node%3A__27_Servers_Cut_and_Up_to_20x_Faster.html">1333 high scalability-2012-10-04-LinkedIn Moved from Rails to Node:  27 Servers Cut and Up to 20x Faster</a></p>
<p>16 0.73710209 <a title="1593-lsi-16" href="../high_scalability-2009/high_scalability-2009-07-28-37signals_Architecture.html">663 high scalability-2009-07-28-37signals Architecture</a></p>
<p>17 0.73661619 <a title="1593-lsi-17" href="../high_scalability-2010/high_scalability-2010-10-26-Scaling_DISQUS_to_75_Million_Comments_and_17%2C000_RPS.html">928 high scalability-2010-10-26-Scaling DISQUS to 75 Million Comments and 17,000 RPS</a></p>
<p>18 0.73424321 <a title="1593-lsi-18" href="../high_scalability-2007/high_scalability-2007-07-25-Paper%3A_Lightweight_Web_servers.html">26 high scalability-2007-07-25-Paper: Lightweight Web servers</a></p>
<p>19 0.73340762 <a title="1593-lsi-19" href="../high_scalability-2011/high_scalability-2011-02-22-Is_Node.js_Becoming_a_Part_of_the_Stack%3F_SimpleGeo_Says_Yes..html">993 high scalability-2011-02-22-Is Node.js Becoming a Part of the Stack? SimpleGeo Says Yes.</a></p>
<p>20 0.73328412 <a title="1593-lsi-20" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.128), (2, 0.259), (10, 0.045), (30, 0.015), (32, 0.186), (47, 0.02), (61, 0.051), (77, 0.026), (79, 0.12), (85, 0.019), (94, 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96970111 <a title="1593-lda-1" href="../high_scalability-2008/high_scalability-2008-02-18-How_to_deal_with_an_I-O_bottleneck_to_disk%3F.html">251 high scalability-2008-02-18-How to deal with an I-O bottleneck to disk?</a></p>
<p>Introduction: A site I'm working with has an I/O bottleneck.     They're using a static server to deliver all of the pictures/video content/zip downloads ecetera but now that the bandwith out of that server is approaching 50Mbit/second the latency on serving small files has increased to become unacceptable.     I'm curious how other people have dealt with this situation.     Seperating into two different servers would require a significant change to the sites architecutre (because the premise is that all uploads go into one server, all subdirectorie are created in one directory, etc.) and may not really solve the problem.</p><p>2 0.96368641 <a title="1593-lda-2" href="../high_scalability-2008/high_scalability-2008-08-12-Strategy%3A_Limit_The_New%2C_Not_The_Old.html">363 high scalability-2008-08-12-Strategy: Limit The New, Not The Old</a></p>
<p>Introduction: One of the most popular and effective scalability strategies is to impose limits (  GAE Quotas  ,   Fotolog  ,   Facebook  ) as a means of protecting a website against service destroying   traffic spikes  . Twitter will reportedly   limit the number followers   to 2,000 in order to thwart follow spam. This may also allow Twitter to make some bank by   going freemium   and charging for adding more followers.     Agree or disagree with Twitter's strategies, the more interesting aspect for me is how do you introduce new policies into an already established ecosystem?      One approach is the big bang. Introduce all changes at once and let everyone adjust. If users don't like it they can move on. The hope is, however, most users won't be impacted by the changes and that those who are will understand it's all for the greater good of their beloved service. Casualties are assumed, but the damage will probably be minor.      Now in Twitter's case the people with the most followers tend to be o</p><p>same-blog 3 0.91894132 <a title="1593-lda-3" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>Introduction: HackerEarth  is a coding skill practice and testing service that in a series of well written articles describes the trials and tribulations of building their site and how they overcame them:  Scaling Python/Django application with Apache and mod_wsgi ,  Programming challenges, uptime, and mistakes in 2013 , P ost-mortem: The big outage on January 25, 2014 ,  The Robust Realtime Server ,  100,000 strong - CodeFactory server ,  Scaling database with Django and HAProxy ,  Continuous Deployment System ,  HackerEarth Technology Stack .
 
What characterizes these articles and makes them especially helpful is a drive for improvement and an openness towards reporting what didn't work and how they figured out what would work.
 
As they say, mistakes happen when you are building a complex product with a team of just 3-4 engineers, but investing in infrastructure allowed them to take more breaks, roam the streets of Bangalore while their servers are happily serving thousands of requests every min</p><p>4 0.91412789 <a title="1593-lda-4" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><p>5 0.90929008 <a title="1593-lda-5" href="../high_scalability-2012/high_scalability-2012-09-07-Stuff_The_Internet_Says_On_Scalability_For_September_7%2C_2012.html">1318 high scalability-2012-09-07-Stuff The Internet Says On Scalability For September 7, 2012</a></p>
<p>Introduction: It's HighScalability Time:
  
 Quotable Quotes:                    
 
  Where did all the supercomputers go?  Inside Intel. 
  @Jacattell : I love the smell of high scalability in the morning :-) 
  @nkohari : Post on HN about GitHub scalability. Top comment? “…someone wasted valuable time making the dashboard look so pretty”  
 
 
  Evolution of SoundCloud’s Architecture : The way we develop SoundCloud is to identify the points of scale then isolate and optimize the read and write paths individually, in anticipation of the next magnitude of growth. 
  How We Build Our 60-Node (Almost Distributed Web Crawler .  Semantics3 crawls 1-3 million pages a day at a cost of ~$3 a day (excluding storage costs) using micro-instances, Grearman, redis, perl, chef, and capistrano. 
 Werner Vogels continues his 50 Shades of Programming book club with  Back-to-Basics Weekend Reading - Granularity of locks . Highlight is a touching remembrance of Jim Gray. 
 Speaking of locks and stories, the MySQL Per</p><p>6 0.90712076 <a title="1593-lda-6" href="../high_scalability-2008/high_scalability-2008-03-24-Advertise.html">287 high scalability-2008-03-24-Advertise</a></p>
<p>7 0.90160233 <a title="1593-lda-7" href="../high_scalability-2009/high_scalability-2009-04-05-At_Some_Point_the_Cost_of_Servers_Outweighs_the_Cost_of_Programmers.html">556 high scalability-2009-04-05-At Some Point the Cost of Servers Outweighs the Cost of Programmers</a></p>
<p>8 0.8988862 <a title="1593-lda-8" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>9 0.89510953 <a title="1593-lda-9" href="../high_scalability-2008/high_scalability-2008-03-29-20_New_Rules_for_Faster_Web_Pages.html">291 high scalability-2008-03-29-20 New Rules for Faster Web Pages</a></p>
<p>10 0.8788504 <a title="1593-lda-10" href="../high_scalability-2009/high_scalability-2009-07-02-It_Must_be_Crap_on_Relational_Dabases_Week_.html">648 high scalability-2009-07-02-It Must be Crap on Relational Dabases Week </a></p>
<p>11 0.87362212 <a title="1593-lda-11" href="../high_scalability-2009/high_scalability-2009-04-14-Scalability_resources.html">569 high scalability-2009-04-14-Scalability resources</a></p>
<p>12 0.86504906 <a title="1593-lda-12" href="../high_scalability-2011/high_scalability-2011-01-18-Paper%3A_Relational_Cloud%3A_A_Database-as-a-Service_for_the_Cloud.html">974 high scalability-2011-01-18-Paper: Relational Cloud: A Database-as-a-Service for the Cloud</a></p>
<p>13 0.86396205 <a title="1593-lda-13" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>14 0.86298412 <a title="1593-lda-14" href="../high_scalability-2010/high_scalability-2010-11-09-Facebook_Uses_Non-Stored_Procedures_to_Update_Social_Graphs.html">936 high scalability-2010-11-09-Facebook Uses Non-Stored Procedures to Update Social Graphs</a></p>
<p>15 0.86240596 <a title="1593-lda-15" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>16 0.86152399 <a title="1593-lda-16" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>17 0.86144543 <a title="1593-lda-17" href="../high_scalability-2012/high_scalability-2012-05-14-DynamoDB_Talk_Notes_and_the_SSD_Hot_S3_Cold_Pattern.html">1245 high scalability-2012-05-14-DynamoDB Talk Notes and the SSD Hot S3 Cold Pattern</a></p>
<p>18 0.86044866 <a title="1593-lda-18" href="../high_scalability-2010/high_scalability-2010-08-16-Scaling_an_AWS_infrastructure_-_Tools_and_Patterns.html">881 high scalability-2010-08-16-Scaling an AWS infrastructure - Tools and Patterns</a></p>
<p>19 0.86006576 <a title="1593-lda-19" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>20 0.85986513 <a title="1593-lda-20" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
