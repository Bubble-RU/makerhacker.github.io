<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1570 high scalability-2014-01-01-Paper: Nanocubes: Nanocubes for Real-Time Exploration of Spatiotemporal Datasets</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1570" href="#">high_scalability-2014-1570</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1570 high scalability-2014-01-01-Paper: Nanocubes: Nanocubes for Real-Time Exploration of Spatiotemporal Datasets</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1570-html" href="http://highscalability.com//blog/2014/1/1/paper-nanocubes-nanocubes-for-real-time-exploration-of-spati.html">html</a></p><p>Introduction: How do you turn Big Data into fast, useful, and interesting visualizations?
UsingRand technology calledNanocubes. The visualizations are stunning and
amazingly reactive. Almost as interesting as the technologies behind
them.David Smith wrote agreat articleexplaining the technology and showing a
demo by Simon Urbanek of a visualization that uses 32Tb of Twitter data. It
runs smoothly and interactively on a single machine with 16Gb of RAM.  For
more information and demos go tonanocubes.net.David Smith sums it up
nicely:Despite the massive number of data points and the beauty and complexity
of the real-time data visualization, it runs impressively quickly. The
underlying data structure is based on Nanocubes, a fast datastructure for in-
memory data cubes. The basic idea is that nanocubes aggregate data
hierarchically, so that as you zoom in and out of the interactive application,
one pixel on the screen is mapped to just one data point, aggregated from the
many that sit "behind" that pixe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 David Smith wrote agreat articleexplaining the technology and showing a demo by Simon Urbanek of a visualization that uses 32Tb of Twitter data. [sent-5, score-0.331]
</p><p>2 It runs smoothly and interactively on a single machine with 16Gb of RAM. [sent-6, score-0.174]
</p><p>3 David Smith sums it up nicely:Despite the massive number of data points and the beauty and complexity of the real-time data visualization, it runs impressively quickly. [sent-9, score-0.366]
</p><p>4 The underlying data structure is based on Nanocubes, a fast datastructure for in- memory data cubes. [sent-10, score-0.405]
</p><p>5 The basic idea is that nanocubes aggregate data hierarchically, so that as you zoom in and out of the interactive application, one pixel on the screen is mapped to just one data point, aggregated from the many that sit "behind" that pixel. [sent-11, score-0.945]
</p><p>6 Abstract fromNanocubes for Real-Time Exploration of Spatiotemporal Datasets:Consider real-time exploration of large multidimensional spatiotemporal datasets with billions of entries, each deďŹ ned by a location, a time, and other attributes. [sent-13, score-0.796]
</p><p>7 Answering these questions requires aggregation over arbitrary regions of the domain and attributes of the data. [sent-16, score-0.243]
</p><p>8 Many relational databases implement the well-known data cube aggregation operation, which in a sense precomputes every possible aggregate query over the database. [sent-17, score-0.478]
</p><p>9 Data cubes are sometimes assumed to take a prohibitively large amount of space, and to consequently require disk storage. [sent-18, score-0.35]
</p><p>10 In contrast, we show how to construct a data cube that ďŹ ts in a modern laptop's main memory, even for billions of entries; we call this data structure a nanocube. [sent-19, score-0.738]
</p><p>11 We present algorithms to compute and query a nanocube, and show how it can be used to generate well- known visual encodings such as heatmaps, histograms, and parallel coordinate plots. [sent-20, score-0.181]
</p><p>12 When compared to exact visualizations created by scanning an entire dataset, nanocube plots have bounded screen error across a variety of scales, thanks to a hierarchical structure in space and time. [sent-21, score-0.934]
</p><p>13 We demonstrate the effectiveness of our technique on a variety of real-world datasets, and present memory, timing, and network bandwidth measurements. [sent-22, score-0.269]
</p><p>14 We ďŹ nd that the timings for the queries in our examples are dominated by network and user- interaction latencies. [sent-23, score-0.19]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nanocubes', 0.326), ('visualizations', 0.258), ('nanocube', 0.217), ('spatiotemporal', 0.217), ('cube', 0.165), ('datasets', 0.156), ('smith', 0.148), ('screen', 0.141), ('visualization', 0.138), ('exploration', 0.134), ('attributes', 0.131), ('entries', 0.129), ('structure', 0.115), ('aggregation', 0.112), ('articleexplaining', 0.109), ('plots', 0.109), ('aggregate', 0.103), ('spatially', 0.102), ('multidimensional', 0.102), ('timings', 0.102), ('data', 0.098), ('outliers', 0.097), ('temporally', 0.097), ('histograms', 0.097), ('cubes', 0.097), ('billions', 0.096), ('prohibitively', 0.094), ('interactively', 0.094), ('impressively', 0.094), ('datastructure', 0.094), ('encodings', 0.094), ('variety', 0.094), ('zoom', 0.091), ('hierarchically', 0.091), ('ned', 0.091), ('effectiveness', 0.088), ('correlated', 0.088), ('ts', 0.088), ('pixel', 0.088), ('nd', 0.088), ('present', 0.087), ('demos', 0.086), ('agreat', 0.084), ('consequently', 0.083), ('smoothly', 0.08), ('modern', 0.078), ('answering', 0.077), ('simon', 0.077), ('assumed', 0.076), ('sums', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1570-tfidf-1" href="../high_scalability-2014/high_scalability-2014-01-01-Paper%3A_Nanocubes%3A_Nanocubes_for_Real-Time_Exploration_of_Spatiotemporal_Datasets.html">1570 high scalability-2014-01-01-Paper: Nanocubes: Nanocubes for Real-Time Exploration of Spatiotemporal Datasets</a></p>
<p>Introduction: How do you turn Big Data into fast, useful, and interesting visualizations?
UsingRand technology calledNanocubes. The visualizations are stunning and
amazingly reactive. Almost as interesting as the technologies behind
them.David Smith wrote agreat articleexplaining the technology and showing a
demo by Simon Urbanek of a visualization that uses 32Tb of Twitter data. It
runs smoothly and interactively on a single machine with 16Gb of RAM.  For
more information and demos go tonanocubes.net.David Smith sums it up
nicely:Despite the massive number of data points and the beauty and complexity
of the real-time data visualization, it runs impressively quickly. The
underlying data structure is based on Nanocubes, a fast datastructure for in-
memory data cubes. The basic idea is that nanocubes aggregate data
hierarchically, so that as you zoom in and out of the interactive application,
one pixel on the screen is mapped to just one data point, aggregated from the
many that sit "behind" that pixe</p><p>2 0.12079494 <a title="1570-tfidf-2" href="../high_scalability-2014/high_scalability-2014-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3rd%2C_2014.html">1572 high scalability-2014-01-03-Stuff The Internet Says On Scalability For January 3rd, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time, can you handle the truth?Should software
architecturesinclude parasites? They increase diversity and complexity in the
food web.10 Million: classic hockey stick growth pattern for GitHub
repositoriesQuotable Quotes:Seymour Cray: A supercomputer is a device for
turning compute-bound problems into IO-bound problems.Robert Sapolsky: And why
is self-organization so beautiful to my atheistic self? Because if complex,
adaptive systems don't require a blue print, they don't require a blue print
maker. If they don't require lightning bolts, they don't require Someone
hurtling lightning bolts.@swardley: Asked for a history of PaaS? From memory,
public launch - Zimki ('06), BungeeLabs ('06), Heroku ('07), GAE ('08),
CloudFoundry ('11) ...@neil_conway: If you're designing scalable systems, you
should understand backpressure and build mechanisms to support it.Scott
Aaronson...the brain is not a quantum computer. A quantum computer is good at
factoring integers, disc</p><p>3 0.086077966 <a title="1570-tfidf-3" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>Introduction: We are on the edge of two potent technological changes: Clouds and Memory
Based Architectures. This evolution will rip open a chasm where new players
can enter and prosper. Google is the master of disk. You can't beat them at a
game they perfected. Disk based databases like SimpleDB and BigTable are
complicated beasts, typical last gasp products of any aging technology before
a change. The next era is the age of Memory and Cloud which will allow for new
players to succeed. The tipping point will be soon.Let's take a short trip
down web architecture lane:It's 1993: Yahoo runs on FreeBSD, Apache, Perl
scripts and a SQL databaseIt's 1995: Scale-up the database.It's 1998: LAMPIt's
1999: Stateless + Load Balanced + Database + SANIt's 2001: In-memory data-
grid.It's 2003: Add a caching layer.It's 2004: Add scale-out and
partitioning.It's 2005: Add asynchronous job scheduling and maybe a
distributed file system.It's 2007: Move it all into the cloud.It's 2008: Cloud
+ web scalable database.It'</p><p>4 0.073464617 <a title="1570-tfidf-4" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>Introduction: This is a guest post byIvan de PradoandPere Ferrera, founders ofDatasalt, the
company behindPangoolandSplout SQLBig Data open-source projects.The amount of
payments performed using credit cards is huge. It is clear that there is
inherent value in the data that can be derived from analyzing all the
transactions. Client fidelity, demographics, heat maps of activity, shop
recommendations, and many other statistics are useful to both clients and
shops for improving their relationship with the market. AtDatasaltwe have
developed a system in collaboration with theBBVA bankthat is able to analyze
years of data and serve insights and statistics to different low-latency web
and mobile applications.The main challenge we faced besides processing Big
Data input is thatthe output was also Big Data, and even bigger than the
input. And this output needed to be served quickly, under high load.The
solution we developed has an infrastructure cost of just a few thousands of
dollars per month thanks to th</p><p>5 0.070158049 <a title="1570-tfidf-5" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>Introduction: As it is said in the recent article"Google: Taming the Long Latency Tail -
When More Machines Equals Worse Results" , latency variability has greater
impact in larger scale clusters where a typical request is composed of
multiple distributed/parallel requests. The overall response time dramatically
decreases if latency of each request is not consistent and low. In dynamically
scalable partitioned storage systems, whether it is a NoSQL database,
filesystem or in-memory data grid, changes in the cluster (adding or removing
a node) can lead to big data moves in the network to re-balance the cluster.
Re-balancing will be needed for both primary and backup data on those nodes.
If a node crashes for example, dead node's data has to be re-owned (become
primary) by other node(s) and also its backup has to be taken immediately to
be fail-safe again. Shuffling MBs of data around has a negative effect in the
cluster as it consumes your valuable resources such as network, CPU and RAM.
It might als</p><p>6 0.070037782 <a title="1570-tfidf-6" href="../high_scalability-2011/high_scalability-2011-09-06-Big_Data_Application_Platform.html">1110 high scalability-2011-09-06-Big Data Application Platform</a></p>
<p>7 0.068573944 <a title="1570-tfidf-7" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>8 0.067630261 <a title="1570-tfidf-8" href="../high_scalability-2014/high_scalability-2014-02-14-Stuff_The_Internet_Says_On_Scalability_For_February_14th%2C_2014.html">1596 high scalability-2014-02-14-Stuff The Internet Says On Scalability For February 14th, 2014</a></p>
<p>9 0.066641651 <a title="1570-tfidf-9" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>10 0.065170348 <a title="1570-tfidf-10" href="../high_scalability-2012/high_scalability-2012-04-10-Sponsored_Post%3A_Infragistics%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_ElasticHosts%2C_Logic_Monitor%2C_Attribution_Modeling%2C_New_Relic%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1226 high scalability-2012-04-10-Sponsored Post: Infragistics, Reality Check Network, Gigaspaces, AiCache, ElasticHosts, Logic Monitor, Attribution Modeling, New Relic, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>11 0.065170348 <a title="1570-tfidf-11" href="../high_scalability-2012/high_scalability-2012-04-24-Sponsored_Post%3A_Reality_Check_Network%2C_Infragistics%2C_Gigaspaces%2C_AiCache%2C_ElasticHosts%2C_Logic_Monitor%2C_Attribution_Modeling%2C_New_Relic%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1232 high scalability-2012-04-24-Sponsored Post: Reality Check Network, Infragistics, Gigaspaces, AiCache, ElasticHosts, Logic Monitor, Attribution Modeling, New Relic, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>12 0.065107584 <a title="1570-tfidf-12" href="../high_scalability-2008/high_scalability-2008-05-27-How_I_Learned_to_Stop_Worrying_and_Love_Using_a_Lot_of_Disk_Space_to_Scale.html">327 high scalability-2008-05-27-How I Learned to Stop Worrying and Love Using a Lot of Disk Space to Scale</a></p>
<p>13 0.064674333 <a title="1570-tfidf-13" href="../high_scalability-2007/high_scalability-2007-12-14-The_Current_Pros_and_Cons_List_for_SimpleDB.html">187 high scalability-2007-12-14-The Current Pros and Cons List for SimpleDB</a></p>
<p>14 0.063152909 <a title="1570-tfidf-14" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>15 0.06282194 <a title="1570-tfidf-15" href="../high_scalability-2007/high_scalability-2007-07-15-Blog%3A_Occam%E2%80%99s_Razor_by_Avinash_Kaushik.html">9 high scalability-2007-07-15-Blog: Occam’s Razor by Avinash Kaushik</a></p>
<p>16 0.061512541 <a title="1570-tfidf-16" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>17 0.061242484 <a title="1570-tfidf-17" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>18 0.061109297 <a title="1570-tfidf-18" href="../high_scalability-2012/high_scalability-2012-05-08-Sponsored_Post%3A_Infragistics%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_ElasticHosts%2C_Logic_Monitor%2C_Attribution_Modeling%2C_New_Relic%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1241 high scalability-2012-05-08-Sponsored Post: Infragistics, Velocity, Reality Check Network, Gigaspaces, AiCache, ElasticHosts, Logic Monitor, Attribution Modeling, New Relic, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>19 0.060889512 <a title="1570-tfidf-19" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>20 0.060810342 <a title="1570-tfidf-20" href="../high_scalability-2012/high_scalability-2012-05-22-Sponsored_Post%3A_Torbit%2C_Infragistics%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_Attribution_Modeling%2C_New_Relic%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1249 high scalability-2012-05-22-Sponsored Post: Torbit, Infragistics, Velocity, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, Attribution Modeling, New Relic, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.061), (2, 0.005), (3, 0.028), (4, -0.021), (5, 0.059), (6, 0.019), (7, 0.027), (8, -0.007), (9, 0.022), (10, 0.016), (11, 0.011), (12, 0.002), (13, -0.005), (14, 0.006), (15, -0.001), (16, 0.008), (17, 0.008), (18, 0.016), (19, 0.003), (20, -0.026), (21, -0.023), (22, 0.017), (23, 0.031), (24, 0.021), (25, 0.002), (26, -0.031), (27, -0.049), (28, -0.011), (29, 0.021), (30, -0.011), (31, 0.003), (32, 0.01), (33, 0.007), (34, -0.032), (35, 0.025), (36, 0.03), (37, -0.008), (38, -0.028), (39, -0.001), (40, 0.007), (41, 0.011), (42, 0.014), (43, 0.006), (44, 0.03), (45, -0.005), (46, -0.025), (47, -0.041), (48, -0.017), (49, 0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93994093 <a title="1570-lsi-1" href="../high_scalability-2014/high_scalability-2014-01-01-Paper%3A_Nanocubes%3A_Nanocubes_for_Real-Time_Exploration_of_Spatiotemporal_Datasets.html">1570 high scalability-2014-01-01-Paper: Nanocubes: Nanocubes for Real-Time Exploration of Spatiotemporal Datasets</a></p>
<p>Introduction: How do you turn Big Data into fast, useful, and interesting visualizations?
UsingRand technology calledNanocubes. The visualizations are stunning and
amazingly reactive. Almost as interesting as the technologies behind
them.David Smith wrote agreat articleexplaining the technology and showing a
demo by Simon Urbanek of a visualization that uses 32Tb of Twitter data. It
runs smoothly and interactively on a single machine with 16Gb of RAM.  For
more information and demos go tonanocubes.net.David Smith sums it up
nicely:Despite the massive number of data points and the beauty and complexity
of the real-time data visualization, it runs impressively quickly. The
underlying data structure is based on Nanocubes, a fast datastructure for in-
memory data cubes. The basic idea is that nanocubes aggregate data
hierarchically, so that as you zoom in and out of the interactive application,
one pixel on the screen is mapped to just one data point, aggregated from the
many that sit "behind" that pixe</p><p>2 0.82974941 <a title="1570-lsi-2" href="../high_scalability-2012/high_scalability-2012-11-26-BigData_using_Erlang%2C_C_and_Lisp_to_Fight_the_Tsunami_of_Mobile_Data.html">1362 high scalability-2012-11-26-BigData using Erlang, C and Lisp to Fight the Tsunami of Mobile Data</a></p>
<p>Introduction: This is a guest post byJon Vlachogiannis. Jon is the founder and CTO
ofBugSense.BugSense, is an error-reporting and quality metrics service that
tracks thousand of apps every day. When mobile apps crash, BugSense helps
developers pinpoint and fix the problem. The startup delivers first-class
service to its customers, which include VMWare, Samsung, Skype and thousands
of independent app developers. Tracking more than 200M devices requires fast,
fault tolerant and cheap infrastructure.The last six months, we've decided to
use our BigData infrastructure, to provide the users with metrics about their
apps performance and stability and let them know how the errors affect their
user base and revenues.We knew that our solution should be scalable from day
one, because more than 4% of the smartphones out there, will start DDOSing us
with data.We wanted to be able to:Abstract the application logic and feed
browsers with JSONRun complex algorithms on the flyExperiment with data,
without the need</p><p>3 0.78513318 <a title="1570-lsi-3" href="../high_scalability-2012/high_scalability-2012-10-10-Antirez%3A_You_Need_to_Think_in_Terms_of_Organizing_Your_Data_for_Fetching.html">1337 high scalability-2012-10-10-Antirez: You Need to Think in Terms of Organizing Your Data for Fetching</a></p>
<p>Introduction: Salvatore Sanfilippowrote a response to Michel Martens' An Open Minded Reader.
There's nothing in the post or response that's controversial. I was just
struck at what a clear explication the conversation was on all the effort that
goes into optimizing read paths. We optimize reads through denormalisation, a
crazy quilt of caching layers, key-value databases, clustering of related
tables, SSD/RAM, DHTs, moving functions to storage, secondary indexes,
separating OLAP from OLTP, etc etc. We often focus so much on specific
techniques that we can forget the bigger picture of what's going on. This
little exchange made me look again at the forest, not just the trees.Michel
Martens:What does it mean to use Redis as a traditional database? If it means
to save all your data and expect to retrieve it later in new and creative
ways, then we have to agree that better tools are available. It is one of
Redis tradeoffs: you have to think in advance how you will want to get your
data back. Another trad</p><p>4 0.77985185 <a title="1570-lsi-4" href="../high_scalability-2011/high_scalability-2011-12-22-Architecting_Massively-Scalable_Near-Real-Time_Risk_Analysis_Solutions.html">1161 high scalability-2011-12-22-Architecting Massively-Scalable Near-Real-Time Risk Analysis Solutions</a></p>
<p>Introduction: Constructing a scalablerisk analysis solution is a fascinating architectural
challenge. If you come from Financial Services you are sure to appreciate
that. But even architects from other domains are bound to find the challenges
fascinating, and the architectural patterns of my suggested solution highly
useful in other domains.Recently I held an interesting webinar around
architecting solutions for scalable and near-real-time risk analysis solutions
based on experience gathered with Financial Services customers. Seeing the
vast interest in the webinar, I would like to share the highlights with you
here.From an architectural point of view, risk analysis is a data-intensive
and a compute-intensive process, which also has an elaborate orchestration
logic. volumes in this domain are massive and ever-increasing, together with
an ever-increasing demand to reduce response time. These trends are aggravated
by global financial regulatory reforms set following the late-2000s financial
crisis, wh</p><p>5 0.77231014 <a title="1570-lsi-5" href="../high_scalability-2009/high_scalability-2009-06-19-GemFire_6.0%3A_New_innovations_in_data_management.html">633 high scalability-2009-06-19-GemFire 6.0: New innovations in data management</a></p>
<p>Introduction: GemStone has unveiled GemFire 6.0 which is the culmination of several years of
development and the continuous solving of the hardest data management problems
in the world. With this release GemFire touts some of the latest innovative
features in data management.In this release:- GemFire introduces a resource
manager to continuously monitor and protect cache instances from running out
of memory, triggering rebalancing to migrate data to less loaded nodes or
allow dynamic increase/decrease in the number of nodes hosting data for linear
scalability without impeding ongoing operations (no contention points).-
GemFire provides explicit control over when rebalancing can be triggered, on
what class of data and even allows the administrator to simulate a "rebalance"
operation to quantify the benefits before actually doing it.- With built in
instrumentation that captures throughput and latency metrics, GemFire now
enables applications to sense changing performance patterns and proactively
provi</p><p>6 0.76663047 <a title="1570-lsi-6" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>7 0.75821579 <a title="1570-lsi-7" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>8 0.75679851 <a title="1570-lsi-8" href="../high_scalability-2012/high_scalability-2012-04-05-Big_Data_Counting%3A_How_to_count_a_billion_distinct_objects_using_only_1.5KB_of_Memory.html">1222 high scalability-2012-04-05-Big Data Counting: How to count a billion distinct objects using only 1.5KB of Memory</a></p>
<p>9 0.75189823 <a title="1570-lsi-9" href="../high_scalability-2008/high_scalability-2008-12-17-Ringo_-_Distributed_key-value_storage_for_immutable_data.html">468 high scalability-2008-12-17-Ringo - Distributed key-value storage for immutable data</a></p>
<p>10 0.74875498 <a title="1570-lsi-10" href="../high_scalability-2014/high_scalability-2014-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3rd%2C_2014.html">1572 high scalability-2014-01-03-Stuff The Internet Says On Scalability For January 3rd, 2014</a></p>
<p>11 0.74843818 <a title="1570-lsi-11" href="../high_scalability-2009/high_scalability-2009-08-01-15_Scalability_and_Performance_Best_Practices.html">668 high scalability-2009-08-01-15 Scalability and Performance Best Practices</a></p>
<p>12 0.74581701 <a title="1570-lsi-12" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>13 0.74034607 <a title="1570-lsi-13" href="../high_scalability-2010/high_scalability-2010-12-08-How_To_Get_Experience_Working_With_Large_Datasets.html">956 high scalability-2010-12-08-How To Get Experience Working With Large Datasets</a></p>
<p>14 0.73661882 <a title="1570-lsi-14" href="../high_scalability-2009/high_scalability-2009-05-28-Scaling_PostgreSQL_using_CUDA.html">609 high scalability-2009-05-28-Scaling PostgreSQL using CUDA</a></p>
<p>15 0.73653269 <a title="1570-lsi-15" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>16 0.73416501 <a title="1570-lsi-16" href="../high_scalability-2010/high_scalability-2010-05-05-How_will_memristors_change_everything%3F_.html">823 high scalability-2010-05-05-How will memristors change everything? </a></p>
<p>17 0.73278046 <a title="1570-lsi-17" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>18 0.73207188 <a title="1570-lsi-18" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>19 0.73064417 <a title="1570-lsi-19" href="../high_scalability-2010/high_scalability-2010-09-16-How_Can_the_Large_Hadron_Collider_Withstand_One_Petabyte_of_Data_a_Second%3F.html">901 high scalability-2010-09-16-How Can the Large Hadron Collider Withstand One Petabyte of Data a Second?</a></p>
<p>20 0.7297346 <a title="1570-lsi-20" href="../high_scalability-2012/high_scalability-2012-05-30-Strategy%3A_Get_Servers_for_Free_and_Make_Users_Happy_by_Turning_on_Compression.html">1254 high scalability-2012-05-30-Strategy: Get Servers for Free and Make Users Happy by Turning on Compression</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.026), (2, 0.184), (5, 0.019), (10, 0.011), (26, 0.378), (61, 0.09), (79, 0.107), (85, 0.045), (94, 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.87918878 <a title="1570-lda-1" href="../high_scalability-2007/high_scalability-2007-08-23-Postgresql_on_high_availability_websites%3F.html">73 high scalability-2007-08-23-Postgresql on high availability websites?</a></p>
<p>Introduction: I was looking at the pingdom infrastructure matrix
(http://royal.pingdom.com/royalfiles/0702_infrastructure_matrix.pdf) and I saw
that no sites are using Postgresql, and then I searched through
highscalability.com and saw very few mentions of postgresql. Are there any
examples of high-traffic sites that use postgresql? Does anyone have any
experience with it? I'm having trouble finding good, recent studies of
postgres (and postgres compared w/ mysql) online.</p><p>2 0.87824982 <a title="1570-lda-2" href="../high_scalability-2009/high_scalability-2009-02-25-Enterprise_Architecture_Conference_by_-_John_Zachman._Johannesburg_%2825th_March%29_%2C_Cape_Town_%2827Th_March%29__Dubai_%2823rd_March%29.html">521 high scalability-2009-02-25-Enterprise Architecture Conference by - John Zachman. Johannesburg (25th March) , Cape Town (27Th March)  Dubai (23rd March)</a></p>
<p>Introduction: Why You Need To Attend THIS CONFERENCE• Understand the multi-dimensional view
of business-technology alignment• A sense of urgency for aggressively pursuing
Enterprise Architecture• A "language" (ie., a Framework) for improving
enterprise communications about architecture issues• An understanding of the
cultural changes implied by process evolution. How to effectively use the
framework to anchor processes and procedures for delivering service and
support for applications• An understanding of basic Enterprise physics•
Recommendations for the Sr. Managers to understand the political realities and
organizational resistance in realizing EA vision and some excellent advices
for overcoming these barriers• Number of practical examples of how to work
with people who affect decisions on EA implementation• How to create value for
your organization by systematically recording assets, processes, connectivity,
people, timing and motivation, through a simple frameworkFor registrations,
group discoun</p><p>same-blog 3 0.83370197 <a title="1570-lda-3" href="../high_scalability-2014/high_scalability-2014-01-01-Paper%3A_Nanocubes%3A_Nanocubes_for_Real-Time_Exploration_of_Spatiotemporal_Datasets.html">1570 high scalability-2014-01-01-Paper: Nanocubes: Nanocubes for Real-Time Exploration of Spatiotemporal Datasets</a></p>
<p>Introduction: How do you turn Big Data into fast, useful, and interesting visualizations?
UsingRand technology calledNanocubes. The visualizations are stunning and
amazingly reactive. Almost as interesting as the technologies behind
them.David Smith wrote agreat articleexplaining the technology and showing a
demo by Simon Urbanek of a visualization that uses 32Tb of Twitter data. It
runs smoothly and interactively on a single machine with 16Gb of RAM.  For
more information and demos go tonanocubes.net.David Smith sums it up
nicely:Despite the massive number of data points and the beauty and complexity
of the real-time data visualization, it runs impressively quickly. The
underlying data structure is based on Nanocubes, a fast datastructure for in-
memory data cubes. The basic idea is that nanocubes aggregate data
hierarchically, so that as you zoom in and out of the interactive application,
one pixel on the screen is mapped to just one data point, aggregated from the
many that sit "behind" that pixe</p><p>4 0.78363419 <a title="1570-lda-4" href="../high_scalability-2009/high_scalability-2009-12-16-The_most_common_flaw_in_software_performance_testing.html">751 high scalability-2009-12-16-The most common flaw in software performance testing</a></p>
<p>Introduction: How many times have we all run across a situation where the performance tests
on a piece of software pass with flying colors on the test systems only to see
the software exhibit poor performance characteristics when the software is
deployed in production?Read More Here...</p><p>5 0.74393851 <a title="1570-lda-5" href="../high_scalability-2009/high_scalability-2009-09-09-GridwiseTech_revolutionizes_data_management.html">697 high scalability-2009-09-09-GridwiseTech revolutionizes data management</a></p>
<p>Introduction: GridwiseTech hasdeveloped AdHoc, an advanced framework for sharing
geographically distributed data and compute resources. It simplifies the
resource management and makes cooperation secure and effective.The premise of
AdHoc is to enable each member of the associated institution to control access
to his or her resources without an IT administrator's help, and with high
security level of any exposed data or applications assured.It takes 3 easy
steps to establish cooperation within AdHoc: create a virtual organization,
add resources and share them. The application can be implemented within any
organization to exchange data and resources or between institutions to join
forces for more efficient results.AdHoc was initially created for a consortium
of hospitals and institutions to share medical data sets. As a technical
partner in that project, GridwiseTech implemented the Security Framework to
provide access to that data and designed a graphical tool to facilitate the
administration of the</p><p>6 0.74122834 <a title="1570-lda-6" href="../high_scalability-2013/high_scalability-2013-02-20-Smart_Companies_Fail_Because_they_Do_Everything_Right_-_Staying_Alive_to_Scale.html">1410 high scalability-2013-02-20-Smart Companies Fail Because they Do Everything Right - Staying Alive to Scale</a></p>
<p>7 0.72401589 <a title="1570-lda-7" href="../high_scalability-2009/high_scalability-2009-10-06-10_Ways_to_Take_your_Site_from_One_to_One_Million_Users_by_Kevin_Rose__.html">715 high scalability-2009-10-06-10 Ways to Take your Site from One to One Million Users by Kevin Rose  </a></p>
<p>8 0.71024615 <a title="1570-lda-8" href="../high_scalability-2012/high_scalability-2012-11-07-Gone_Fishin%27%3A_10_Ways_to_Take_your_Site_from_One_to_One_Million_Users_by_Kevin_Rose__.html">1356 high scalability-2012-11-07-Gone Fishin': 10 Ways to Take your Site from One to One Million Users by Kevin Rose  </a></p>
<p>9 0.65624768 <a title="1570-lda-9" href="../high_scalability-2009/high_scalability-2009-06-22-Improving_performance_and_scalability_with_DDD.html">635 high scalability-2009-06-22-Improving performance and scalability with DDD</a></p>
<p>10 0.64920849 <a title="1570-lda-10" href="../high_scalability-2014/high_scalability-2014-05-19-A_Short_On_How_the_Wayback_Machine_Stores_More_Pages_than_Stars_in_the_Milky_Way.html">1650 high scalability-2014-05-19-A Short On How the Wayback Machine Stores More Pages than Stars in the Milky Way</a></p>
<p>11 0.63200152 <a title="1570-lda-11" href="../high_scalability-2007/high_scalability-2007-11-11-Linkedin_architecture.html">148 high scalability-2007-11-11-Linkedin architecture</a></p>
<p>12 0.61711973 <a title="1570-lda-12" href="../high_scalability-2008/high_scalability-2008-09-08-Guerrilla_Capacity_Planning_and_the_Law_of_Universal_Scalability.html">381 high scalability-2008-09-08-Guerrilla Capacity Planning and the Law of Universal Scalability</a></p>
<p>13 0.61144435 <a title="1570-lda-13" href="../high_scalability-2013/high_scalability-2013-12-23-What_Happens_While_Your_Brain_Sleeps_is_Surprisingly_Like_How_Computers_Stay_Sane.html">1568 high scalability-2013-12-23-What Happens While Your Brain Sleeps is Surprisingly Like How Computers Stay Sane</a></p>
<p>14 0.61029929 <a title="1570-lda-14" href="../high_scalability-2007/high_scalability-2007-10-30-Database_parallelism_choices_greatly_impact_scalability.html">137 high scalability-2007-10-30-Database parallelism choices greatly impact scalability</a></p>
<p>15 0.59996843 <a title="1570-lda-15" href="../high_scalability-2007/high_scalability-2007-09-15-The_Role_of_Memory_within_Web_2.0_Architectures_and_Deployments.html">92 high scalability-2007-09-15-The Role of Memory within Web 2.0 Architectures and Deployments</a></p>
<p>16 0.55594003 <a title="1570-lda-16" href="../high_scalability-2008/high_scalability-2008-06-04-LinkedIn_Architecture.html">339 high scalability-2008-06-04-LinkedIn Architecture</a></p>
<p>17 0.55308837 <a title="1570-lda-17" href="../high_scalability-2013/high_scalability-2013-10-21-Google%27s_Sanjay_Ghemawat_on_What_Made_Google_Google_and_Great_Big_Data_Career_Advice.html">1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</a></p>
<p>18 0.54803598 <a title="1570-lda-18" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>19 0.546085 <a title="1570-lda-19" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>20 0.53923899 <a title="1570-lda-20" href="../high_scalability-2014/high_scalability-2014-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3rd%2C_2014.html">1572 high scalability-2014-01-03-Stuff The Internet Says On Scalability For January 3rd, 2014</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
