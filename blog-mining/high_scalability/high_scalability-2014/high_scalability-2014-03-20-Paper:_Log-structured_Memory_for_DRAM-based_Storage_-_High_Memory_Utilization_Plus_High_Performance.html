<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1616 high scalability-2014-03-20-Paper: Log-structured Memory for DRAM-based Storage - High Memory Utilization Plus High Performance</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1616" href="#">high_scalability-2014-1616</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1616 high scalability-2014-03-20-Paper: Log-structured Memory for DRAM-based Storage - High Memory Utilization Plus High Performance</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1616-html" href="http://highscalability.com//blog/2014/3/20/paper-log-structured-memory-for-dram-based-storage-high-memo.html">html</a></p><p>Introduction: Most every programmer who gets sucked into deep performance analysis for long
running processes eventually realizes memory allocation is the heart of evil
at the center of many of their problems. So you replace malloc with something
less worse. Or you tune your garbage collector like a fine ukulele. But
there's a smarter approach brought to you from the folks atRAMCloud, a
Stanford University production, which is a large scale, distributed, in-memory
key-value database.What they've found is that typical memory management
approaches don't work and using a log structured approach yields massive
benefits:Performance measurements of log-structured memory in RAMCloud show
that it enables high client through- put at 80-90% memory utilization, even
with artificially stressful workloads. In the most stressful workload, a
single RAMCloud server can support 270,000-410,000 durable 100-byte writes per
second at 90% memory utilization. The two-level approach to cleaning improves
performance by up</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Most every programmer who gets sucked into deep performance analysis for long running processes eventually realizes memory allocation is the heart of evil at the center of many of their problems. [sent-1, score-0.657]
</p><p>2 Or you tune your garbage collector like a fine ukulele. [sent-3, score-0.253]
</p><p>3 But there's a smarter approach brought to you from the folks atRAMCloud, a Stanford University production, which is a large scale, distributed, in-memory key-value database. [sent-4, score-0.189]
</p><p>4 In the most stressful workload, a single RAMCloud server can support 270,000-410,000 durable 100-byte writes per second at 90% memory utilization. [sent-6, score-0.618]
</p><p>5 The two-level approach to cleaning improves performance by up to 6x over a single-level approach at high memory utilization, and reduces disk bandwidth overhead by 7-87x for medium-sized objects (1 to 10 KB). [sent-7, score-1.126]
</p><p>6 Parallel cleaning effectively hides the cost of cleaning: an active cleaner adds only about 2% to the latency of typical client write requests. [sent-8, score-0.619]
</p><p>7 Log-structured Memory for DRAM-based Storage:Traditional memory allocation mechanisms are not suitable for new DRAM-based storage systems because they use memory inefficiently, particularly under changing access patterns. [sent-10, score-1.024]
</p><p>8 In contrast, a log-structured approach to memory management allows 80-90% memory utilization while offering high performance. [sent-11, score-1.039]
</p><p>9 The RAMCloud storage system implements a unified log-structured mechanism both for active information in memory and backup data on disk. [sent-12, score-0.605]
</p><p>10 The RAMCloud implementation of log-structured memory uses a two-level cleaning policy, which conserves disk bandwidth and improves perfor- mance up to 6x at high memory utilization. [sent-13, score-1.332]
</p><p>11 The cleaner runs concurrently with normal operations and employs multiple threads to hide most of the cost of cleaning. [sent-14, score-0.319]
</p><p>12 Logging has been used for decades to ensure durability and consistency in storage systems. [sent-15, score-0.214]
</p><p>13 When we began designing RAMCloud, it was a natural choice to use a logging approach on disk to back up the data stored in main memory. [sent-16, score-0.443]
</p><p>14 However, it was surprising to discover that logging also makes sense as a technique for managing the data in DRAM. [sent-17, score-0.286]
</p><p>15 Log-structured memory takes advantage of the restricted use of pointers in storage systems to eliminate the global memory scans that fundamentally limit existing garbage collectors. [sent-18, score-1.269]
</p><p>16 The result is an efficient and highly incremental form of copying garbage collector that allows memory to be used efficiently even at utilizations of 80-90%. [sent-19, score-0.791]
</p><p>17 A pleasant side effect of this discovery was that we were able to use a single technique for managing both disk and main memory, with small policy differences that optimize the usage of each medium. [sent-20, score-0.532]
</p><p>18 Although we developed log-structured memory for RAMCloud, we believe that the ideas are generally applicable and that log- structured memory is a good candidate for managing memory in DRAM-based storage systems. [sent-21, score-1.491]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ramcloud', 0.53), ('memory', 0.384), ('cleaning', 0.271), ('stressful', 0.169), ('cleaner', 0.143), ('utilization', 0.138), ('garbage', 0.136), ('approach', 0.133), ('collector', 0.117), ('improves', 0.109), ('allocation', 0.106), ('policy', 0.101), ('managing', 0.1), ('disk', 0.096), ('technique', 0.095), ('utilizations', 0.094), ('storage', 0.093), ('logging', 0.091), ('realizes', 0.088), ('inefficiently', 0.088), ('conserves', 0.088), ('structured', 0.084), ('malloc', 0.081), ('sucked', 0.079), ('kb', 0.076), ('pleasant', 0.076), ('artificially', 0.076), ('scans', 0.075), ('pointers', 0.071), ('active', 0.07), ('hides', 0.069), ('typical', 0.066), ('durable', 0.065), ('fundamentally', 0.064), ('decades', 0.064), ('main', 0.064), ('measurements', 0.063), ('restricted', 0.062), ('applicable', 0.062), ('concurrently', 0.06), ('copying', 0.06), ('stanford', 0.06), ('yields', 0.06), ('began', 0.059), ('employs', 0.059), ('unified', 0.058), ('hide', 0.057), ('durability', 0.057), ('suitable', 0.057), ('smarter', 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="1616-tfidf-1" href="../high_scalability-2014/high_scalability-2014-03-20-Paper%3A_Log-structured_Memory_for_DRAM-based_Storage_-_High_Memory_Utilization_Plus_High_Performance.html">1616 high scalability-2014-03-20-Paper: Log-structured Memory for DRAM-based Storage - High Memory Utilization Plus High Performance</a></p>
<p>Introduction: Most every programmer who gets sucked into deep performance analysis for long
running processes eventually realizes memory allocation is the heart of evil
at the center of many of their problems. So you replace malloc with something
less worse. Or you tune your garbage collector like a fine ukulele. But
there's a smarter approach brought to you from the folks atRAMCloud, a
Stanford University production, which is a large scale, distributed, in-memory
key-value database.What they've found is that typical memory management
approaches don't work and using a log structured approach yields massive
benefits:Performance measurements of log-structured memory in RAMCloud show
that it enables high client through- put at 80-90% memory utilization, even
with artificially stressful workloads. In the most stressful workload, a
single RAMCloud server can support 270,000-410,000 durable 100-byte writes per
second at 90% memory utilization. The two-level approach to cleaning improves
performance by up</p><p>2 0.18522185 <a title="1616-tfidf-2" href="../high_scalability-2009/high_scalability-2009-10-22-Paper%3A_The_Case_for_RAMClouds%3A_Scalable_High-Performance_Storage_Entirely_in_DRAM_.html">726 high scalability-2009-10-22-Paper: The Case for RAMClouds: Scalable High-Performance Storage Entirely in DRAM </a></p>
<p>Introduction: Stanford Info Lab is taking pains to document a direction we've been moving
for a while now, using RAM not just as a cache, but as the primary storage
medium. Manyquality products have built on this model. Even if the vision
isn't radical, the paper does produce a lot of data backing up the transition,
which is in itself helpful. From the The Abstract:Disk-oriented approaches to
online storage are becoming increasingly problematic: they do not scale grace-
fully to meet the needs of large-scale Web applications, and improvements in
disk capacity have far out-stripped improvements in access latency and
bandwidth. This paper argues for a new approach to datacenter storage called
RAMCloud, where information is kept entirely in DRAM and large-scale systems
are created by aggregating the main memories of thousands of commodity
servers. We believe that RAMClouds can provide durable and available storage
with 100-1000x the throughput of disk-based systems and 100-1000x lower access
latency. T</p><p>3 0.18368509 <a title="1616-tfidf-3" href="../high_scalability-2011/high_scalability-2011-09-19-Big_Iron_Returns_with_BigMemory.html">1118 high scalability-2011-09-19-Big Iron Returns with BigMemory</a></p>
<p>Introduction: This is a guest post by Greg Luck Founder and CTO,EhcacheTerracotta Inc.Note:
this article contains a bit too much of a product pitch, but the points are
still generally valid and useful.The legendary Moore's Law, which states that
the number of transistors that can be placed inexpensively on an integrated
circuit doubles approximately every two years, has held true since 1965. It
follows that integrated circuits will continue to get smaller, with chip
fabrication currently at a minuscule 22nm process (1). Users of big iron
hardware, or servers that are dense in terms of CPU power and memory capacity,
benefit from this trend as their hardware becomes cheaper and more powerful
over time. At some point soon, however, density limits imposed by quantum
mechanics will preclude further density increases.At the same time, low-cost
commodity hardware influences enterprise architects to scale their
applications horizontally, where processing is spread across clusters of low-
cost commodity serv</p><p>4 0.17382428 <a title="1616-tfidf-4" href="../high_scalability-2007/high_scalability-2007-09-15-The_Role_of_Memory_within_Web_2.0_Architectures_and_Deployments.html">92 high scalability-2007-09-15-The Role of Memory within Web 2.0 Architectures and Deployments</a></p>
<p>Introduction: Although I have a basic working knowledge of memory, SSDs and the like, I am
not technical...I have never developed or deployed a system. I was exposed to
ram-disks years ago, when their expense limited their use to very small files
or DB applications. I am looking to "get current" on what role memory plays in
curremt WEB 2.0 design and deployments.How is memory commonly used to remove
latency and accelerate performance in typical Web 2.0 architectures? What role
can memory play in massive scale-out implementations? Are there such a thing
as memory "best practives"? If memory were cheap, would that significantly
change the way systems are designed and deployed?What commercial and open
source products that use memory are used, what are the benefits and trade-
offs?Can anyone suggest what sources - people, books, papers, products - I
might look into to gain a practical understanding of this topic?</p><p>5 0.16950452 <a title="1616-tfidf-5" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>Introduction: We are on the edge of two potent technological changes: Clouds and Memory
Based Architectures. This evolution will rip open a chasm where new players
can enter and prosper. Google is the master of disk. You can't beat them at a
game they perfected. Disk based databases like SimpleDB and BigTable are
complicated beasts, typical last gasp products of any aging technology before
a change. The next era is the age of Memory and Cloud which will allow for new
players to succeed. The tipping point will be soon.Let's take a short trip
down web architecture lane:It's 1993: Yahoo runs on FreeBSD, Apache, Perl
scripts and a SQL databaseIt's 1995: Scale-up the database.It's 1998: LAMPIt's
1999: Stateless + Load Balanced + Database + SANIt's 2001: In-memory data-
grid.It's 2003: Add a caching layer.It's 2004: Add scale-out and
partitioning.It's 2005: Add asynchronous job scheduling and maybe a
distributed file system.It's 2007: Move it all into the cloud.It's 2008: Cloud
+ web scalable database.It'</p><p>6 0.14879966 <a title="1616-tfidf-6" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<p>7 0.13923413 <a title="1616-tfidf-7" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>8 0.13325399 <a title="1616-tfidf-8" href="../high_scalability-2014/high_scalability-2014-05-21-9_Principles_of_High_Performance_Programs.html">1652 high scalability-2014-05-21-9 Principles of High Performance Programs</a></p>
<p>9 0.13150035 <a title="1616-tfidf-9" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>10 0.1268606 <a title="1616-tfidf-10" href="../high_scalability-2013/high_scalability-2013-06-06-Paper%3A_Memory_Barriers%3A_a_Hardware_View_for_Software_Hackers.html">1471 high scalability-2013-06-06-Paper: Memory Barriers: a Hardware View for Software Hackers</a></p>
<p>11 0.11866452 <a title="1616-tfidf-11" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<p>12 0.11562736 <a title="1616-tfidf-12" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>13 0.11397497 <a title="1616-tfidf-13" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>14 0.11303524 <a title="1616-tfidf-14" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>15 0.10885688 <a title="1616-tfidf-15" href="../high_scalability-2013/high_scalability-2013-12-23-What_Happens_While_Your_Brain_Sleeps_is_Surprisingly_Like_How_Computers_Stay_Sane.html">1568 high scalability-2013-12-23-What Happens While Your Brain Sleeps is Surprisingly Like How Computers Stay Sane</a></p>
<p>16 0.10829823 <a title="1616-tfidf-16" href="../high_scalability-2014/high_scalability-2014-01-20-8_Ways_Stardog_Made_its_Database_Insanely_Scalable.html">1582 high scalability-2014-01-20-8 Ways Stardog Made its Database Insanely Scalable</a></p>
<p>17 0.10140995 <a title="1616-tfidf-17" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>18 0.094190709 <a title="1616-tfidf-18" href="../high_scalability-2008/high_scalability-2008-12-03-Java_World_Interview_on_Scalability_and_Other_Java_Scalability_Secrets.html">459 high scalability-2008-12-03-Java World Interview on Scalability and Other Java Scalability Secrets</a></p>
<p>19 0.091947421 <a title="1616-tfidf-19" href="../high_scalability-2013/high_scalability-2013-11-20-How_Twitter_Improved_JVM_Performance_by_Reducing_GC_and_Faster_Memory_Allocation.html">1551 high scalability-2013-11-20-How Twitter Improved JVM Performance by Reducing GC and Faster Memory Allocation</a></p>
<p>20 0.090597115 <a title="1616-tfidf-20" href="../high_scalability-2013/high_scalability-2013-07-05-Stuff_The_Internet_Says_On_Scalability_For_July_5%2C_2013.html">1487 high scalability-2013-07-05-Stuff The Internet Says On Scalability For July 5, 2013</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.144), (1, 0.081), (2, -0.014), (3, 0.013), (4, -0.045), (5, 0.088), (6, 0.095), (7, 0.044), (8, -0.095), (9, 0.023), (10, 0.011), (11, -0.061), (12, 0.028), (13, 0.036), (14, -0.064), (15, 0.002), (16, 0.002), (17, 0.007), (18, 0.001), (19, 0.023), (20, -0.029), (21, 0.006), (22, -0.015), (23, 0.085), (24, -0.002), (25, -0.052), (26, 0.009), (27, -0.103), (28, -0.023), (29, -0.044), (30, 0.034), (31, -0.039), (32, 0.049), (33, -0.078), (34, -0.043), (35, -0.007), (36, -0.021), (37, 0.021), (38, 0.005), (39, 0.045), (40, 0.023), (41, 0.033), (42, 0.095), (43, 0.048), (44, -0.011), (45, -0.043), (46, 0.002), (47, -0.03), (48, 0.079), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98926902 <a title="1616-lsi-1" href="../high_scalability-2014/high_scalability-2014-03-20-Paper%3A_Log-structured_Memory_for_DRAM-based_Storage_-_High_Memory_Utilization_Plus_High_Performance.html">1616 high scalability-2014-03-20-Paper: Log-structured Memory for DRAM-based Storage - High Memory Utilization Plus High Performance</a></p>
<p>Introduction: Most every programmer who gets sucked into deep performance analysis for long
running processes eventually realizes memory allocation is the heart of evil
at the center of many of their problems. So you replace malloc with something
less worse. Or you tune your garbage collector like a fine ukulele. But
there's a smarter approach brought to you from the folks atRAMCloud, a
Stanford University production, which is a large scale, distributed, in-memory
key-value database.What they've found is that typical memory management
approaches don't work and using a log structured approach yields massive
benefits:Performance measurements of log-structured memory in RAMCloud show
that it enables high client through- put at 80-90% memory utilization, even
with artificially stressful workloads. In the most stressful workload, a
single RAMCloud server can support 270,000-410,000 durable 100-byte writes per
second at 90% memory utilization. The two-level approach to cleaning improves
performance by up</p><p>2 0.86245948 <a title="1616-lsi-2" href="../high_scalability-2014/high_scalability-2014-01-20-8_Ways_Stardog_Made_its_Database_Insanely_Scalable.html">1582 high scalability-2014-01-20-8 Ways Stardog Made its Database Insanely Scalable</a></p>
<p>Introduction: Stardog makes a commercial graph database that is a great example of what can
be accomplished with a scale-up strategy on BigIron. In a recent article
StarDog described how they made their new 2.1 release insanely scalable,
improving query scalability by about 3 orders of magnitude and it can now
handle 50 billion triples on a $10,000 server with 32 cores and 256 GB RAM. It
can also load 20B datasets at 300,000 triples per second. What did they do
that you can also do?Avoid locks by using non-blocking algorithms and data
structures. For example, moving from BitSet to ConcurrentLinkedQueue.Use
ThreadLocal aggressively to reduce thread contention and avoid
synchronization.Batch LRU evictions in a single thread. Triggered by several
LRU caches becoming problematic when evictions were being swamped by
additions. Downside is batching increases memory pressure and GC times.Move to
SHA1 for hashing URIs, bnodes, and literal values. Making hash collisions
nearly impossible enable significant s</p><p>3 0.86241806 <a title="1616-lsi-3" href="../high_scalability-2007/high_scalability-2007-09-15-The_Role_of_Memory_within_Web_2.0_Architectures_and_Deployments.html">92 high scalability-2007-09-15-The Role of Memory within Web 2.0 Architectures and Deployments</a></p>
<p>Introduction: Although I have a basic working knowledge of memory, SSDs and the like, I am
not technical...I have never developed or deployed a system. I was exposed to
ram-disks years ago, when their expense limited their use to very small files
or DB applications. I am looking to "get current" on what role memory plays in
curremt WEB 2.0 design and deployments.How is memory commonly used to remove
latency and accelerate performance in typical Web 2.0 architectures? What role
can memory play in massive scale-out implementations? Are there such a thing
as memory "best practives"? If memory were cheap, would that significantly
change the way systems are designed and deployed?What commercial and open
source products that use memory are used, what are the benefits and trade-
offs?Can anyone suggest what sources - people, books, papers, products - I
might look into to gain a practical understanding of this topic?</p><p>4 0.85525435 <a title="1616-lsi-4" href="../high_scalability-2013/high_scalability-2013-06-06-Paper%3A_Memory_Barriers%3A_a_Hardware_View_for_Software_Hackers.html">1471 high scalability-2013-06-06-Paper: Memory Barriers: a Hardware View for Software Hackers</a></p>
<p>Introduction: It's not often you get so enthusiastic a recommendation for a paper as Sergio
Bossa gives Memory Barriers: a Hardware View for Software Hackers: If you only
want to read one piece about CPUs architecture, cache coherency and memory
barriers, make it this one.It is a clear and well written article. It even has
a quiz. What's it about?So what possessed CPU designers to cause them to
inﬂict memory barriers on poor unsuspecting SMP software designers?In short,
because reordering memory references allows much better performance, and so
memory barriers are needed to force ordering in things like synchronization
primitives whose correct operation depends on ordered memory
references.Getting a more detailed answer to this question requires a good
understanding of how CPU caches work, and especially what is required to make
caches really work well. The following sections:present the structure of a
cache,describe how cache-coherency protocols ensure that CPUs agree on the
value of each location</p><p>5 0.81187844 <a title="1616-lsi-5" href="../high_scalability-2014/high_scalability-2014-05-21-9_Principles_of_High_Performance_Programs.html">1652 high scalability-2014-05-21-9 Principles of High Performance Programs</a></p>
<p>Introduction: Arvid Norbergon thelibtorrent bloghas put together an excellent list
ofprinciples of high performance programs, obviously derived from hard won
experience programming on bittorrent:Two fundamental causes of performance
problems:Memory Latency. A big performance problem on modern computers is the
latency of SDRAM. The CPU waits idle for a read from memory to come
back.Context Switching. When a CPU switches context "the memory it will access
is most likely unrelated to the memory the previous context was accessing.
This often results in significant eviction of the previous cache, and requires
the switched-to context to load much of its data from RAM, which is
slow."Rules to help balance the forces of evil:Batch work. Avoid context
switching by batching work. For example, there are vector versions of system
calls like writev() and readv() that operate on more than one item per call.
An implication is that you want to merge as many writes as possible.Avoid
Magic Numbers. They don't scale.</p><p>6 0.78569084 <a title="1616-lsi-6" href="../high_scalability-2011/high_scalability-2011-09-19-Big_Iron_Returns_with_BigMemory.html">1118 high scalability-2011-09-19-Big Iron Returns with BigMemory</a></p>
<p>7 0.76951486 <a title="1616-lsi-7" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<p>8 0.75937468 <a title="1616-lsi-8" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<p>9 0.72340417 <a title="1616-lsi-9" href="../high_scalability-2008/high_scalability-2008-10-19-Alternatives_to_Google_App_Engine.html">423 high scalability-2008-10-19-Alternatives to Google App Engine</a></p>
<p>10 0.71031398 <a title="1616-lsi-10" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>11 0.7083137 <a title="1616-lsi-11" href="../high_scalability-2013/high_scalability-2013-11-20-How_Twitter_Improved_JVM_Performance_by_Reducing_GC_and_Faster_Memory_Allocation.html">1551 high scalability-2013-11-20-How Twitter Improved JVM Performance by Reducing GC and Faster Memory Allocation</a></p>
<p>12 0.70456326 <a title="1616-lsi-12" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>13 0.69355875 <a title="1616-lsi-13" href="../high_scalability-2012/high_scalability-2012-05-16-Big_List_of_20_Common_Bottlenecks.html">1246 high scalability-2012-05-16-Big List of 20 Common Bottlenecks</a></p>
<p>14 0.68902874 <a title="1616-lsi-14" href="../high_scalability-2009/high_scalability-2009-10-22-Paper%3A_The_Case_for_RAMClouds%3A_Scalable_High-Performance_Storage_Entirely_in_DRAM_.html">726 high scalability-2009-10-22-Paper: The Case for RAMClouds: Scalable High-Performance Storage Entirely in DRAM </a></p>
<p>15 0.66830575 <a title="1616-lsi-15" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>16 0.66203743 <a title="1616-lsi-16" href="../high_scalability-2012/high_scalability-2012-11-29-Performance_data_for_LevelDB%2C_Berkley_DB_and_BangDB_for_Random_Operations.html">1364 high scalability-2012-11-29-Performance data for LevelDB, Berkley DB and BangDB for Random Operations</a></p>
<p>17 0.65370446 <a title="1616-lsi-17" href="../high_scalability-2009/high_scalability-2009-03-05-Strategy%3A__In_Cloud_Computing_Systematically_Drive_Load_to_the_CPU.html">526 high scalability-2009-03-05-Strategy:  In Cloud Computing Systematically Drive Load to the CPU</a></p>
<p>18 0.64878964 <a title="1616-lsi-18" href="../high_scalability-2012/high_scalability-2012-05-02-12_Ways_to_Increase_Throughput_by_32X_and_Reduce_Latency_by__20X.html">1237 high scalability-2012-05-02-12 Ways to Increase Throughput by 32X and Reduce Latency by  20X</a></p>
<p>19 0.64787555 <a title="1616-lsi-19" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<p>20 0.64228332 <a title="1616-lsi-20" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.144), (2, 0.217), (5, 0.013), (10, 0.025), (20, 0.014), (30, 0.021), (40, 0.03), (43, 0.013), (47, 0.013), (54, 0.187), (61, 0.031), (73, 0.029), (77, 0.011), (79, 0.099), (94, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92188573 <a title="1616-lda-1" href="../high_scalability-2008/high_scalability-2008-01-04-For_%245_Million_You_Can_Buy_Enough_Storage_to_Compete_with_Google.html">201 high scalability-2008-01-04-For $5 Million You Can Buy Enough Storage to Compete with Google</a></p>
<p>Introduction: Kevin Burton calculates thatBlekko, one of the barbarian hoard storming
Google's search fortress, would need to spend $5 million just to buy enough
weapons, er storage.Kevin estimates storing a deep crawl of the internet would
take about 5 petabytes. At a projected $1 million per petabyte that's a paltry
$5 million. Less than expected. Imagine in days of old an ambitious noble
itching to raise an army to conquer a land and become its new prince. For a
fine land, and the search market is one of the richest, that would be a smart
investment for a VC to make.In these situations I always ask: What would
Machiavelli do?breakMachiavelli taught some lands are hard to conquer and easy
to keep and some are easy to conquer and hard to keep. A land like France was
easy to conquer because it was filled with nobles. You can turn nobles on each
other because they always hate each other for some reason or another. But it's
hard to keep a land of nobles because they all think they are as good as you
a</p><p>same-blog 2 0.89860791 <a title="1616-lda-2" href="../high_scalability-2014/high_scalability-2014-03-20-Paper%3A_Log-structured_Memory_for_DRAM-based_Storage_-_High_Memory_Utilization_Plus_High_Performance.html">1616 high scalability-2014-03-20-Paper: Log-structured Memory for DRAM-based Storage - High Memory Utilization Plus High Performance</a></p>
<p>Introduction: Most every programmer who gets sucked into deep performance analysis for long
running processes eventually realizes memory allocation is the heart of evil
at the center of many of their problems. So you replace malloc with something
less worse. Or you tune your garbage collector like a fine ukulele. But
there's a smarter approach brought to you from the folks atRAMCloud, a
Stanford University production, which is a large scale, distributed, in-memory
key-value database.What they've found is that typical memory management
approaches don't work and using a log structured approach yields massive
benefits:Performance measurements of log-structured memory in RAMCloud show
that it enables high client through- put at 80-90% memory utilization, even
with artificially stressful workloads. In the most stressful workload, a
single RAMCloud server can support 270,000-410,000 durable 100-byte writes per
second at 90% memory utilization. The two-level approach to cleaning improves
performance by up</p><p>3 0.88487315 <a title="1616-lda-3" href="../high_scalability-2008/high_scalability-2008-01-28-Howto_setup_GFS-GNBD.html">227 high scalability-2008-01-28-Howto setup GFS-GNBD</a></p>
<p>Introduction: Before you proceed make sure you have physical volume(something like
/dev/sda1, /dev/sda4, etc) with no data. This is going to be the gfs volume
which you will export to other nodes. It should be on the node which is going
to be your gnbd server. If you dont have such volume create one using fdisk.I
used mounted gfs volume as a DOCUMENT ROOT for my Apache server nodes(Load
Balanced).I tried it on FC4 64-bit. If you plan to try it on any other
distribution or 32-bit arch.. still the procedure remains same. Since I built
it from source but not RPMs, you may have to simply supply config options with
a different CFLAGS.Full details at http://linuxsutra.chakravaka.com/redhat-
cluster/2006/11/01/howto-gfs-gnbd</p><p>4 0.86924887 <a title="1616-lda-4" href="../high_scalability-2013/high_scalability-2013-03-27-The_Changing_Face_of_Scale_-_The_Downside_of_Scaling_in_the_Contextual_Age_.html">1430 high scalability-2013-03-27-The Changing Face of Scale - The Downside of Scaling in the Contextual Age </a></p>
<p>Introduction: Robert Scobleis a kind of Brothers Grimm for the digital age. Instead of
inspired romantics walking around the country side collecting the folk tales
of past ages, he is an inspired technologist documenting the current mythology
of startups.One of the developments Robert is exploring is the rise of
thecontextual age. Where every bit of information about you is continually
being prodded, pulled, and observed, shoveled into a great learning machine,
and turned into a fully actionableknowledge graphof context. A digital
identity more real to software than your physical body ever was. Sinner or
saviour, the Age of Context has interesting implications for startups. It
raises the entrance bar to dizzying heights. Much of the reason companies are
tearing down the Golden Age of the Web, one open protocol at a time, is to
create a walled garden of monopolized information.To operate in this world you
will have to somehow create a walled garden of your own. And it will be a damn
big garden. So yo</p><p>5 0.85891253 <a title="1616-lda-5" href="../high_scalability-2013/high_scalability-2013-02-04-Is_Provisioned_IOPS_Better%3F_Yes%2C_it_Delivers_More_Consistent_and_Higher_Performance_IO.html">1398 high scalability-2013-02-04-Is Provisioned IOPS Better? Yes, it Delivers More Consistent and Higher Performance IO</a></p>
<p>Introduction: Amazon created a whole new class of service with theirProvisioned IOPSfor RDS,
EBS, and DynamoDB. The idea is simple. If you want more performance, you turn
a dial up. If you want less, you turn a dial down. A beautifully simple model.
You pay for the performance you want, which is different than their previous
cloud model, where performance varied, but you paid only for what you used.
The question: Do these higher priced services really work better?Rodrigo
Campos put this question to the test (only for EBS) by running a benchmark he
describes inIOMelt Provisioned IOPS EBS Benchmark Results - December 2012.The
result?Yes, AWS Provisioned IOPS Volumes Really Deliver More Consistent and
Higher Performance IO:It is clear that the provisioned IOPS EBS volumes offer
a huge performance upgrade when compared to the non-optimized EBS volumes, but
as data has to be spread among more underlying disks or systems, it seems that
the volume is increasingly more susceptible to performance
fluctuation</p><p>6 0.84941339 <a title="1616-lda-6" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<p>7 0.8297739 <a title="1616-lda-7" href="../high_scalability-2011/high_scalability-2011-09-19-Big_Iron_Returns_with_BigMemory.html">1118 high scalability-2011-09-19-Big Iron Returns with BigMemory</a></p>
<p>8 0.82905126 <a title="1616-lda-8" href="../high_scalability-2008/high_scalability-2008-04-18-Scaling_Mania_at_MySQL_Conference_2008.html">303 high scalability-2008-04-18-Scaling Mania at MySQL Conference 2008</a></p>
<p>9 0.82757396 <a title="1616-lda-9" href="../high_scalability-2011/high_scalability-2011-03-25-Did_the_Microsoft_Stack_Kill_MySpace%3F.html">1011 high scalability-2011-03-25-Did the Microsoft Stack Kill MySpace?</a></p>
<p>10 0.8239457 <a title="1616-lda-10" href="../high_scalability-2012/high_scalability-2012-05-14-DynamoDB_Talk_Notes_and_the_SSD_Hot_S3_Cold_Pattern.html">1245 high scalability-2012-05-14-DynamoDB Talk Notes and the SSD Hot S3 Cold Pattern</a></p>
<p>11 0.82363778 <a title="1616-lda-11" href="../high_scalability-2008/high_scalability-2008-09-03-Some_Facebook_Secrets_to_Better_Operations.html">378 high scalability-2008-09-03-Some Facebook Secrets to Better Operations</a></p>
<p>12 0.82209235 <a title="1616-lda-12" href="../high_scalability-2010/high_scalability-2010-06-07-Six_Ways_Twitter_May_Reach_its_Big_Hairy_Audacious_Goal_of_One_Billion_Users.html">837 high scalability-2010-06-07-Six Ways Twitter May Reach its Big Hairy Audacious Goal of One Billion Users</a></p>
<p>13 0.82154411 <a title="1616-lda-13" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>14 0.82105476 <a title="1616-lda-14" href="../high_scalability-2009/high_scalability-2009-10-28-And_the_winner_is%3A_MySQL_or_Memcached_or_Tokyo_Tyrant%3F.html">729 high scalability-2009-10-28-And the winner is: MySQL or Memcached or Tokyo Tyrant?</a></p>
<p>15 0.82083285 <a title="1616-lda-15" href="../high_scalability-2014/high_scalability-2014-03-19-Strategy%3A_Three_Techniques_to_Survive_Traffic_Surges_by_Quickly_Scaling_Your_Site.html">1615 high scalability-2014-03-19-Strategy: Three Techniques to Survive Traffic Surges by Quickly Scaling Your Site</a></p>
<p>16 0.82080781 <a title="1616-lda-16" href="../high_scalability-2014/high_scalability-2014-01-31-Stuff_The_Internet_Says_On_Scalability_For_January_31st%2C_2014.html">1588 high scalability-2014-01-31-Stuff The Internet Says On Scalability For January 31st, 2014</a></p>
<p>17 0.82071859 <a title="1616-lda-17" href="../high_scalability-2012/high_scalability-2012-04-20-Stuff_The_Internet_Says_On_Scalability_For_April_20%2C_2012.html">1231 high scalability-2012-04-20-Stuff The Internet Says On Scalability For April 20, 2012</a></p>
<p>18 0.82003182 <a title="1616-lda-18" href="../high_scalability-2009/high_scalability-2009-06-29-How_to_Succeed_at_Capacity_Planning_Without_Really_Trying_%3A__An_Interview_with_Flickr%27s_John_Allspaw_on_His_New_Book.html">643 high scalability-2009-06-29-How to Succeed at Capacity Planning Without Really Trying :  An Interview with Flickr's John Allspaw on His New Book</a></p>
<p>19 0.81976378 <a title="1616-lda-19" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>20 0.81952053 <a title="1616-lda-20" href="../high_scalability-2012/high_scalability-2012-12-07-Stuff_The_Internet_Says_On_Scalability_For_December_7%2C_2012.html">1368 high scalability-2012-12-07-Stuff The Internet Says On Scalability For December 7, 2012</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
