<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1602" href="#">high_scalability-2014-1602</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1602-html" href="http://highscalability.com//blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html">html</a></p><p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We've faced many challenges in meeting the ever- growing demand for our messaging services, but as we continue to push the envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of our serving system. [sent-3, score-0.258]
</p><p>2 It's for the450 million active users, with a user based growing at one million users a day, with a potential for a billion users. [sent-43, score-0.329]
</p><p>3 Benedict Evansmakes a great casethat Mobile is a 1+ trillion dollar business, WhatsApp is disrupting the SMS part of this industry, which globally has over $100 billion in revenue, by sending 18 billion SMS messages a day when the global SMS system only sends 20 billion SMS messages a day. [sent-49, score-0.513]
</p><p>4 Rick Reed's main talk is about the optimization process used to get to 2 million connections a server while using Erlang, which is interesting, but it's not a complete architecture talk. [sent-114, score-0.248]
</p><p>5 32 engineers, one developer supports 14 million active users50 billion messages every day across seven platforms (inbound + outbound)1+ million people sign up every day$0 invested in advertising$60 millioninvestment from Sequoia Capital; $3. [sent-125, score-0.401]
</p><p>6 4 billion is the amount Sequoia will make35% is how much of Facebook's cash isbeing used for the dealHundreds of nodes>8000 coresHundreds of terabytes of RAM>70M Erlang messages per secondIn 2011 WhatsApp achieved1 million established tcp sessionson a single machine with memory and cpu to spare. [sent-126, score-0.366]
</p><p>7 To handle 50 billion messages a day the focus is on making a reliable system that works. [sent-147, score-0.261]
</p><p>8 The successful retrieval of a message is sent back to the whatsapp server which forwards this status back to the original sender (which will see that as a "checkmark" icon next to the message). [sent-162, score-0.924]
</p><p>9 If supporting a million connections on a server it would take 30 hosts to open enough IP ports to generate enough connections to test just one server. [sent-209, score-0.395]
</p><p>10 Don't want to tee traffic and do things that would affect the permanent state of a user or result in multiple messages going to users. [sent-216, score-0.246]
</p><p>11 BEAM emulator running at 45% utilization, which matches closely to user percentage, which is good because the emulator runs as user. [sent-236, score-0.25]
</p><p>12 A month later tackling bottlenecks 2 million connections per server was achieved. [sent-238, score-0.296]
</p><p>13 Either a single message queue or a sum of message queues. [sent-250, score-0.268]
</p><p>14 Added to BEAM instrumentation on message queue stats per process. [sent-251, score-0.315]
</p><p>15 Sampling every 10 seconds, could see a process had 600K messages in its message queue with a dequeue rate of 40K with a delay of 15 seconds. [sent-253, score-0.313]
</p><p>16 The longer a server has been up it will accumulate long running connections that are mostly idle so it can handle more connections because these connections are not as busy per connection. [sent-261, score-0.405]
</p><p>17 Every time a message is delivered from a port it looks to update the time-of-day which is a single lock across all schedulers which means all CPUs are hitting one lock. [sent-267, score-0.273]
</p><p>18 When message queue backlogs became large garbage collection would destabilize the system. [sent-277, score-0.25]
</p><p>19 Instrument scheduler to get utilization information, statistics for message queues, number of sleeps, send rates, message counts, etc. [sent-288, score-0.391]
</p><p>20 When Rick is going through all the changes that he made to get to 2 million connections a server it was mind numbing. [sent-309, score-0.248]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('whatsapp', 0.715), ('erlang', 0.204), ('facebook', 0.142), ('beam', 0.122), ('messages', 0.11), ('message', 0.107), ('emulator', 0.104), ('messaging', 0.1), ('connections', 0.099), ('rick', 0.095), ('koum', 0.094), ('million', 0.089), ('sms', 0.086), ('phone', 0.082), ('mobile', 0.08), ('freebsd', 0.075), ('utilization', 0.074), ('billion', 0.071), ('jan', 0.068), ('stats', 0.066), ('port', 0.066), ('scheduler', 0.065), ('server', 0.06), ('queue', 0.054), ('schedulers', 0.051), ('reed', 0.051), ('lock', 0.049), ('portal', 0.049), ('glitches', 0.049), ('would', 0.048), ('per', 0.048), ('cpu', 0.048), ('traffic', 0.046), ('fixes', 0.045), ('events', 0.044), ('day', 0.042), ('see', 0.042), ('user', 0.042), ('backlogs', 0.041), ('benedict', 0.041), ('gimmicks', 0.041), ('simultaneous', 0.04), ('instrumentation', 0.04), ('desktop', 0.04), ('outbound', 0.039), ('apps', 0.038), ('users', 0.038), ('system', 0.038), ('vm', 0.038), ('number', 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999946 <a title="1602-tfidf-1" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>2 0.28476787 <a title="1602-tfidf-2" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>Introduction: When we last  visited WhatsApp  theyâ€™d just been acquired by Facebook for $19 billion. We learned about their early architecture, which centered around a maniacal focus on optimizing Erlang into handling 2 million connections a server, working on All The Phones, and making users happy through simplicity.
   Two years later traffic has grown 10x. How did WhatsApp make that jump to the next level of scalability? 
    Rick Reed    tells us in a talk he gave at the Erlang Factory:    That's 'Billion' with a 'B': Scaling to the next level at WhatsApp    (   slides   ), which revealed some eye popping WhatsApp stats: 
  
  What has hundreds of nodes, thousands of cores, hundreds of terabytes of RAM, and hopes to serve the billions of smartphones that will soon be a reality around the globe? The Erlang/FreeBSD-based server infrastructure at WhatsApp. We've faced many challenges in meeting the ever-growing demand for our messaging services, but as we continue to push the envelope on size (>800</p><p>3 0.19097701 <a title="1602-tfidf-3" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update :Â  Erlang at Facebook Â by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.Â  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><p>4 0.16117351 <a title="1602-tfidf-4" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: With Lavabit  shutting down Â under  murky circumstances , it seems fitting to  repost an old  (2009), yet still very good post by  Ladar Levison  on Lavabit's architecture. I don't know how much of this information is still current, but it should give you a general idea what Lavabit was all about.
  
 Getting to Know You 
 What is the name of your system and where can we find out more about it? 

 Note: these links are no longer valid... 


Lavabit   http://lavabit.com      http://lavabit.com/network.html    http://lavabit.com/about.html 

 What is your system for? 

Lavabit is a mid-sized email service provider. We currently have about 140,000 registered users with more than 260,000 email addresses. While most of our accounts belong to individual users, we also provide corporate email services to approximately 70 companies.

 Why did you decide to build this system? 

We built the system to compete against the other large free email providers, with an emphasis on serving the privacy c</p><p>5 0.15279245 <a title="1602-tfidf-5" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>Introduction: This is a guest post by Jamie Hall, Co-founder & CTO of  MocoSpace , describing the architecture for their mobile social network. This is a timely architecture to learn from as it combines several hot trends: it is very large, mobile, and social. What they think is especially cool about their system is: how it optimizes for device/browser fragmentation on the mobile Web; their multi-tiered,  read/write, local/distributed caching system; selecting PostgreSQL over MySQL as a relational DB that can scale.
 
MocoSpace is a mobile social network, with 12 million members and 3 billion page views a month, which makes it one of the most highly trafficked mobile Websites in the US. Members access the site mainly from their mobile phone Web browser, ranging from high end smartphones to lower end devices, as well as the Web. Activities on the site include customizing profiles, chat, instant messaging, music, sharing photos & videos, games, eCards and blogs. The monetization strategy is focused on</p><p>6 0.15210548 <a title="1602-tfidf-6" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>7 0.14497665 <a title="1602-tfidf-7" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>8 0.1445719 <a title="1602-tfidf-8" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>9 0.14419691 <a title="1602-tfidf-9" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>10 0.14398831 <a title="1602-tfidf-10" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>11 0.14259937 <a title="1602-tfidf-11" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>12 0.141729 <a title="1602-tfidf-12" href="../high_scalability-2014/high_scalability-2014-02-28-Stuff_The_Internet_Says_On_Scalability_For_February_28th%2C_2014.html">1603 high scalability-2014-02-28-Stuff The Internet Says On Scalability For February 28th, 2014</a></p>
<p>13 0.14107437 <a title="1602-tfidf-13" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>14 0.13880192 <a title="1602-tfidf-14" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>15 0.13438234 <a title="1602-tfidf-15" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Monthâ€¦</a></p>
<p>16 0.13363327 <a title="1602-tfidf-16" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>17 0.13245511 <a title="1602-tfidf-17" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>18 0.12941849 <a title="1602-tfidf-18" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>19 0.12930253 <a title="1602-tfidf-19" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>20 0.12724733 <a title="1602-tfidf-20" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.261), (1, 0.12), (2, -0.003), (3, -0.041), (4, 0.024), (5, -0.075), (6, 0.012), (7, 0.13), (8, -0.012), (9, -0.011), (10, 0.027), (11, 0.087), (12, 0.073), (13, -0.008), (14, -0.062), (15, 0.069), (16, 0.024), (17, 0.022), (18, -0.001), (19, 0.031), (20, 0.052), (21, 0.019), (22, 0.007), (23, -0.031), (24, 0.069), (25, -0.026), (26, 0.038), (27, -0.001), (28, 0.072), (29, -0.002), (30, -0.055), (31, -0.037), (32, -0.026), (33, -0.028), (34, 0.04), (35, 0.027), (36, 0.026), (37, -0.014), (38, -0.006), (39, 0.007), (40, 0.03), (41, -0.012), (42, 0.047), (43, 0.023), (44, -0.052), (45, -0.011), (46, -0.039), (47, 0.057), (48, -0.028), (49, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96843678 <a title="1602-lsi-1" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>2 0.83950782 <a title="1602-lsi-2" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update :Â  Erlang at Facebook Â by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.Â  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><p>3 0.80907315 <a title="1602-lsi-3" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>Introduction: Our latest strategy is taken from a  great post by Paul Saab of Facebook , detailing how with changes Facebook has made to memcached they have:
  ...been able to scale memcached to handle 200,000 UDP requests per second with an average latency of 173 microseconds. The total throughput achieved is 300,000 UDP requests/s, but the latency at that request rate is too high to be useful in our system. This is an amazing increase from 50,000 UDP requests/s using the stock version of Linux and memcached.  
To scale Facebook has hundreds of thousands of TCP connections open to their memcached processes. First, this is still amazing. It's not so long ago you could have never done this. Optimizing connection use was always a priority because the OS simply couldn't handle large numbers of connections or large numbers of threads or large numbers of CPUs. To get to this point is a big accomplishment. Still, at that scale there are problems that are often solved.  Some of the problem Facebook faced a</p><p>4 0.79886419 <a title="1602-lsi-4" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to Mike Swift, inÂ  Facebook gets ready for New Year's Eve , we get a little insight as to their method for the madness, nothing really detailed, but still interesting.
  Problem Setup   
 Facebook expects tha one billion+ photos will be shared on New Year's eve. 
 Facebook's 800 million users are scattered around the world. Three quarters live outside the US. Each user is linked to an average of 130 friends. 
 Photos and posts must appear in less than a second. Opening a homepage requires executing requests on a 100 different servers, and those requests have to be ranked, sorted, and privacy-checked, and then rendered. 
 Different events put different stresses on different parts of Facebook.Â       
 
 Photo and Video Uploads -Â Holidays require hundreds of terabytes of capacityÂ  
 News Feed - News events like big sports events and the death of Steve Jobs drive user status updates 
 
 
   Coping Strategies   
  Try</p><p>5 0.78543609 <a title="1602-lsi-5" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>Introduction: When we last  visited WhatsApp  theyâ€™d just been acquired by Facebook for $19 billion. We learned about their early architecture, which centered around a maniacal focus on optimizing Erlang into handling 2 million connections a server, working on All The Phones, and making users happy through simplicity.
   Two years later traffic has grown 10x. How did WhatsApp make that jump to the next level of scalability? 
    Rick Reed    tells us in a talk he gave at the Erlang Factory:    That's 'Billion' with a 'B': Scaling to the next level at WhatsApp    (   slides   ), which revealed some eye popping WhatsApp stats: 
  
  What has hundreds of nodes, thousands of cores, hundreds of terabytes of RAM, and hopes to serve the billions of smartphones that will soon be a reality around the globe? The Erlang/FreeBSD-based server infrastructure at WhatsApp. We've faced many challenges in meeting the ever-growing demand for our messaging services, but as we continue to push the envelope on size (>800</p><p>6 0.78344089 <a title="1602-lsi-6" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>7 0.78055638 <a title="1602-lsi-7" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>8 0.76425588 <a title="1602-lsi-8" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>9 0.75733924 <a title="1602-lsi-9" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>10 0.75634062 <a title="1602-lsi-10" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>11 0.75566512 <a title="1602-lsi-11" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>12 0.75421369 <a title="1602-lsi-12" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>13 0.74102801 <a title="1602-lsi-13" href="../high_scalability-2013/high_scalability-2013-08-16-Stuff_The_Internet_Says_On_Scalability_For_August_16%2C_2013.html">1502 high scalability-2013-08-16-Stuff The Internet Says On Scalability For August 16, 2013</a></p>
<p>14 0.7393173 <a title="1602-lsi-14" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>15 0.72944677 <a title="1602-lsi-15" href="../high_scalability-2013/high_scalability-2013-11-22-Stuff_The_Internet_Says_On_Scalability_For_November_22th%2C_2013.html">1552 high scalability-2013-11-22-Stuff The Internet Says On Scalability For November 22th, 2013</a></p>
<p>16 0.72651732 <a title="1602-lsi-16" href="../high_scalability-2010/high_scalability-2010-12-31-Facebook_in_20_Minutes%3A_2.7M_Photos%2C_10.2M_Comments%2C_4.6M_Messages.html">966 high scalability-2010-12-31-Facebook in 20 Minutes: 2.7M Photos, 10.2M Comments, 4.6M Messages</a></p>
<p>17 0.72353286 <a title="1602-lsi-17" href="../high_scalability-2014/high_scalability-2014-04-04-Stuff_The_Internet_Says_On_Scalability_For_April_4th%2C_2014.html">1626 high scalability-2014-04-04-Stuff The Internet Says On Scalability For April 4th, 2014</a></p>
<p>18 0.72326797 <a title="1602-lsi-18" href="../high_scalability-2012/high_scalability-2012-06-29-Stuff_The_Internet_Says_On_Scalability_For_June_29%2C_2012_-_The_Velocity_Edition.html">1274 high scalability-2012-06-29-Stuff The Internet Says On Scalability For June 29, 2012 - The Velocity Edition</a></p>
<p>19 0.72025299 <a title="1602-lsi-19" href="../high_scalability-2011/high_scalability-2011-12-30-Stuff_The_Internet_Says_On_Scalability_For_December_30%2C_2011.html">1166 high scalability-2011-12-30-Stuff The Internet Says On Scalability For December 30, 2011</a></p>
<p>20 0.71924675 <a title="1602-lsi-20" href="../high_scalability-2008/high_scalability-2008-10-27-Notify.me_Architecture_-_Synchronicity_Kills.html">431 high scalability-2008-10-27-Notify.me Architecture - Synchronicity Kills</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.118), (2, 0.213), (10, 0.064), (30, 0.03), (40, 0.029), (49, 0.015), (52, 0.031), (56, 0.015), (61, 0.065), (77, 0.072), (79, 0.102), (85, 0.048), (94, 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97823584 <a title="1602-lda-1" href="../high_scalability-2013/high_scalability-2013-12-20-Stuff_The_Internet_Says_On_Scalability_For_December_20th%2C_2013.html">1567 high scalability-2013-12-20-Stuff The Internet Says On Scalability For December 20th, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time (with so much cool info this week it will blow your mind):
     Amazing microscope image of a carnivorous bladderwort    
  How many drones would it take to replace Santa? Â With a fleet of some 80 million or so F-16 drones the entire worldwide delivery could be completed in just over eight hours. Impressive, but a world without Rudolf is not a world I wish to contemplate. 
 Quotable Quotes:                                                
 
  @Loh : Always wanted to travel back in time to try fighting a younger version of yourself? Software development is the career for you! 
  @mraleph :Â often devs still approach performance of JS code as if they are riding a horse cart but the horse had long been replaced with fusion reactor 
  @peakscale : "The c3.large is 40% faster and has more than double the memory than the c1.medium but costs about the same" 
  @techmilind : Conversation with an ex-Yahoo, now at a Telecom company. Replaced $22M of Teradata by $450K</p><p>2 0.97389758 <a title="1602-lda-2" href="../high_scalability-2011/high_scalability-2011-12-16-Stuff_The_Internet_Says_On_Scalability_For_December_16%2C_2011.html">1158 high scalability-2011-12-16-Stuff The Internet Says On Scalability For December 16, 2011</a></p>
<p>Introduction: A HighScalability is forever:
  
 eBay:  tens of millions Â of lines of code; Google code base change rate per month:  50% ; Apple:  100 million downloads ; Internet:  186 Gbps  
 Quotable quotes:                  
 
  @OttmarAmann  : Scalability is not as important as managing complexityÂ  
  @amankapur91  : Does scalability imply standardization, and then does standardization imply loss of innovation? 
 
 
 Spotify uses a P2P architecture and this paper,  Spotify â€“ Large Scale, Low Latency, P2P Music-on-Demand Streaming , describes it. 
  The Faving spam counter-measures . Ironically, deviantART relates a gripping story of how they detected and stopped a deviant user from attacking their servers with an automated faving script which faved every 10 seconds for 24 hours a day. The same spam filter they use on the rest of the site was used. Problem solved. Would like detail on their spam filter though. 
 Interesting Google Group's thread on the  best practices for simulating transactions</p><p>same-blog 3 0.97232884 <a title="1602-lda-3" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>4 0.9711653 <a title="1602-lda-4" href="../high_scalability-2011/high_scalability-2011-09-16-Stuff_The_Internet_Says_On_Scalability_For_September_16%2C_2011.html">1117 high scalability-2011-09-16-Stuff The Internet Says On Scalability For September 16, 2011</a></p>
<p>Introduction: Between love and madness liesÂ  HighScalability : 
  
  Google now 10x better : MapReduce sorts 1 petabyte of data using 8000 computers in 33 minutes;Â  1 Billion on Social Networks ;  Tumblr at 10 Billion Posts ;  Twitter at 100 Million Users ;  Testing at Google Scale : 1800 builds, 120 million test suites, 60 million tests run daily. 
 From the  Dash Memo  on Google's Plan:  Go is a very promising systems-programming language in the vein of C++. We fully hope and expect that Go becomes the standard back-end language at Google over the next few years.  On GAEÂ  Go can load from Â a cold start in 100ms and the typical instance size is 4MB. Is it any wonder Go is a go? Should we expect to see Java and Python deprecated because Go is so much cheaper to run at scale? 
 Potent Quotables:               
 
  @caciufo  : 30x more scalability w/ many-core. So perf doesn't have to level out or vex programmers. #IDF2011 
  @joerglew  : Evaluating divide&conquer; vs. master-slave architecture for wor</p><p>5 0.96846807 <a title="1602-lda-5" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:
    ( Giant Hurricane on Saturn , here's one  in New Orleans )  
Â 
  
  1,966,080 cores : Time Warp synchronization protocol using up to 7.8M MPI tasks on 1,966,080 cores of the {Sequoia} Blue Gene/Q supercomputer system. 33 trillion events processed in 65 seconds yielding a peak event-rate in excess of 504 billion events/second using 120 racks of Sequoia. 
 Quotable Quotes:                             
 
  Thad Starner : the longer accessing a device exceeds 2s, the more its actually usage would decrease exponentially. Thus, he made a claim that wrist watch interface always sitting on one's wrist ready to use should be more successful than mobile phones which have to pulled out of the pocket.Â  
  @joedevon :Â We came for scalability but we stayed for agility #NoSQL 
  @jahmailay :Â "Our user base is exploding. I really wish we spent more time on scalability instead of features customers don't use." - Everybody, always. 
  @bsletten : I donâ€™t think it is a</p><p>6 0.96364892 <a title="1602-lda-6" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>7 0.96196413 <a title="1602-lda-7" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>8 0.96082592 <a title="1602-lda-8" href="../high_scalability-2011/high_scalability-2011-09-02-Stuff_The_Internet_Says_On_Scalability_For_September_2%2C_2011.html">1109 high scalability-2011-09-02-Stuff The Internet Says On Scalability For September 2, 2011</a></p>
<p>9 0.96081978 <a title="1602-lda-9" href="../high_scalability-2014/high_scalability-2014-04-25-Stuff_The_Internet_Says_On_Scalability_For_April_25th%2C_2014.html">1637 high scalability-2014-04-25-Stuff The Internet Says On Scalability For April 25th, 2014</a></p>
<p>10 0.96061444 <a title="1602-lda-10" href="../high_scalability-2011/high_scalability-2011-11-29-DataSift_Architecture%3A_Realtime_Datamining_at_120%2C000_Tweets_Per_Second.html">1148 high scalability-2011-11-29-DataSift Architecture: Realtime Datamining at 120,000 Tweets Per Second</a></p>
<p>11 0.95963788 <a title="1602-lda-11" href="../high_scalability-2012/high_scalability-2012-12-14-Stuff_The_Internet_Says_On_Scalability_For_December_14%2C_2012.html">1372 high scalability-2012-12-14-Stuff The Internet Says On Scalability For December 14, 2012</a></p>
<p>12 0.95835394 <a title="1602-lda-12" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>13 0.95773226 <a title="1602-lda-13" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>14 0.95761412 <a title="1602-lda-14" href="../high_scalability-2009/high_scalability-2009-08-05-Stack_Overflow_Architecture.html">671 high scalability-2009-08-05-Stack Overflow Architecture</a></p>
<p>15 0.95749784 <a title="1602-lda-15" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>16 0.9572441 <a title="1602-lda-16" href="../high_scalability-2012/high_scalability-2012-12-26-Ask_HS%3A_What_will_programming_and_architecture_look_like_in_2020%3F.html">1377 high scalability-2012-12-26-Ask HS: What will programming and architecture look like in 2020?</a></p>
<p>17 0.95714021 <a title="1602-lda-17" href="../high_scalability-2014/high_scalability-2014-03-14-Stuff_The_Internet_Says_On_Scalability_For_March_14th%2C_2014.html">1612 high scalability-2014-03-14-Stuff The Internet Says On Scalability For March 14th, 2014</a></p>
<p>18 0.95666885 <a title="1602-lda-18" href="../high_scalability-2013/high_scalability-2013-02-15-Stuff_The_Internet_Says_On_Scalability_For_February_15%2C_2013.html">1407 high scalability-2013-02-15-Stuff The Internet Says On Scalability For February 15, 2013</a></p>
<p>19 0.95648587 <a title="1602-lda-19" href="../high_scalability-2012/high_scalability-2012-01-09-The_Etsy_Saga%3A_From_Silos_to_Happy_to_Billions_of_Pageviews_a_Month.html">1171 high scalability-2012-01-09-The Etsy Saga: From Silos to Happy to Billions of Pageviews a Month</a></p>
<p>20 0.95640588 <a title="1602-lda-20" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
