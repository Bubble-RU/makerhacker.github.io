<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1644" href="#">high_scalability-2014-1644</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1644-html" href="http://highscalability.com//blog/2014/5/7/update-on-disqus-its-still-about-realtime-but-go-demolishes.html">html</a></p><p>Introduction: Our last article on Disqus:  How Disqus Went Realtime With 165K Messages Per Second And Less Than .2 Seconds Latency , was a little out of date, but the folks at Disqus have been busy implementing, not talking, so we don't know a lot about what they are doing now, but we do have a short update in  C1MM and NGINX  by John Watson and an article  Trying out this Go thing .
 
So Disqus has grown a bit:
  
 1.3 billion unique visitors 
 10 billion page views 
 500 million users engaged in discussions 
 3 million communities 
 25 million comments 
  
They are still all about realtime, but Go replaced Python in their Realtime system:
  
 Original Realtime backend was written in a pretty lightweight Python + gevent. 
 The realtime service is a hybrid of CPU intensive tasks + lots of network IO. Gevent was handling the network IO without an issue, but at higher contention, the CPU was choking everything. Switching over to Go removed that contention, which was the primary issue that was being se</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our last article on Disqus:  How Disqus Went Realtime With 165K Messages Per Second And Less Than . [sent-1, score-0.062]
</p><p>2 The realtime service is a hybrid of CPU intensive tasks + lots of network IO. [sent-5, score-0.589]
</p><p>3 Gevent was handling the network IO without an issue, but at higher contention, the CPU was choking everything. [sent-6, score-0.073]
</p><p>4 Switching over to Go removed that contention, which was the primary issue that was being seen. [sent-7, score-0.093]
</p><p>5 On a normal day each machine sees 3200 connections/s, 1 million connections, 150K packets/s TX and 130K packets/s RX, 150 mbits/s TX and 80 mbits/s RC, with <15ms delay end-to-end (which is faster than Javascript can render a comment)   Had many issues with resource exhaustion at first. [sent-11, score-0.391]
</p><p>6 The configuration for Nginx and the OS are given that help alleviate the problems, tuning them to handle a scenario with many connections moving little data. [sent-12, score-0.163]
</p><p>7 Ran out of network bandwidth before anything else. [sent-13, score-0.073]
</p><p>8 Using 10 gigabit network interface cards helped a lot. [sent-14, score-0.32]
</p><p>9 Enabling gzip helped a lot, but Nginx preallocates a lot of memory per connection for gzip, but since comments are small this was overkill. [sent-15, score-0.29]
</p><p>10 As message rates increased, at peak processing 10k+ messages per second, the machines maxed out, and end-to-end latency went to seconds and minutes in the worst case. [sent-17, score-0.482]
</p><p>11 Liked Go because of its performance, native concurrency, and familiarity for Python programmers. [sent-19, score-0.078]
</p><p>12 In only a week a replacement system was built with impressive results:                      End-to-end latency is on average, less than 10ms. [sent-20, score-0.143]
</p><p>13 Currently consuming roughly 10-20% of available CPU. [sent-21, score-0.07]
</p><p>14 Node was not selected because it does not handle CPU intensive tasks well   Go does not directly access the database. [sent-23, score-0.257]
</p><p>15 It consumes a queue from RabbitMQ and publishes to the Nginx frontends. [sent-24, score-0.178]
</p><p>16 In the end, having a faster product yields its own benefits as well. [sent-29, score-0.072]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disqus', 0.472), ('realtime', 0.324), ('nginx', 0.266), ('tx', 0.203), ('gzip', 0.179), ('python', 0.131), ('eventsource', 0.113), ('helped', 0.111), ('contention', 0.109), ('intensive', 0.107), ('exhaustion', 0.101), ('rx', 0.101), ('gevent', 0.101), ('go', 0.1), ('million', 0.098), ('publishes', 0.094), ('issue', 0.093), ('machines', 0.091), ('websocket', 0.09), ('seconds', 0.088), ('engaged', 0.086), ('maxed', 0.086), ('tasks', 0.085), ('watson', 0.084), ('alleviate', 0.084), ('consumes', 0.084), ('messages', 0.083), ('went', 0.081), ('connections', 0.079), ('cpu', 0.078), ('forever', 0.078), ('familiarity', 0.078), ('throwing', 0.077), ('latency', 0.077), ('network', 0.073), ('yields', 0.072), ('consuming', 0.07), ('gigabit', 0.07), ('polling', 0.069), ('articleson', 0.069), ('rabbitmq', 0.069), ('render', 0.066), ('replacement', 0.066), ('cards', 0.066), ('selected', 0.065), ('delay', 0.064), ('article', 0.062), ('sees', 0.062), ('worst', 0.062), ('lightweight', 0.062)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1644-tfidf-1" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>Introduction: Our last article on Disqus:  How Disqus Went Realtime With 165K Messages Per Second And Less Than .2 Seconds Latency , was a little out of date, but the folks at Disqus have been busy implementing, not talking, so we don't know a lot about what they are doing now, but we do have a short update in  C1MM and NGINX  by John Watson and an article  Trying out this Go thing .
 
So Disqus has grown a bit:
  
 1.3 billion unique visitors 
 10 billion page views 
 500 million users engaged in discussions 
 3 million communities 
 25 million comments 
  
They are still all about realtime, but Go replaced Python in their Realtime system:
  
 Original Realtime backend was written in a pretty lightweight Python + gevent. 
 The realtime service is a hybrid of CPU intensive tasks + lots of network IO. Gevent was handling the network IO without an issue, but at higher contention, the CPU was choking everything. Switching over to Go removed that contention, which was the primary issue that was being se</p><p>2 0.49882594 <a title="1644-tfidf-2" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>Introduction: Here's an  Update On Disqus: It's Still About Realtime, But Go Demolishes Python . 
 
How do you add realtime functionality to a web scale application? That's what  Adam Hitchcock , a Software Engineer at Disqus talks about in an excellent talk:  Making DISQUS Realtime  ( slides ).
 
Disqus had to take their commenting system and add realtime capabilities to it. Not something that's easy to do when at the time of the talk (2013) they had had just hit a billion unique visitors a month.
 
What Disqus developed is a realtime commenting system called “realertime” that was tested to handle 1.5 million concurrently connected users, 45,000 new connections per second, 165,000 messages/second, with less than .2 seconds latency end-to-end.
 
The nature of a commenting system is that it is IO bound and has a high fanout, that is a comment comes in and must be sent out to a lot of readers. It's a problem very similar to what  Twitter must solve . 
 
Disqus' solution was quite interesting as was th</p><p>3 0.21414435 <a title="1644-tfidf-3" href="../high_scalability-2010/high_scalability-2010-10-26-Scaling_DISQUS_to_75_Million_Comments_and_17%2C000_RPS.html">928 high scalability-2010-10-26-Scaling DISQUS to 75 Million Comments and 17,000 RPS</a></p>
<p>Introduction: This  presentation  and  video  by Jason Yan and David Cramer discusses how they scaled DISQUS, a comments as a service service for easily adding comments to your site and connecting communities. The presentation is very good, so here are just a few highlights: 
  
  Traffic : 17,000 requests/second peak; 450,000 websites; 15 million profiles; 75 million comments; 250 million visitors; 40 million monthly users / developer. 
  Forces : unpredictable traffic patterns because of celebrity gossip and events like disasters; discussion never expire which means they can't fit in memory; must always be up. 
  Machines : 100 servers; 30% web servers (Appache + mod_wsgi); 10% databases (PostgreSQL); 25% cache servers (memcached); 20% load balancing / high availability (HAProxy + heartbeat); 15% Utility servers (Python scripts). 
  Architecture : Requests are load balanced across an Apache cluster. Apache talks to memcached, HAProxy/pgbouncer to handle connection pooling to the database, and a ce</p><p>4 0.21390386 <a title="1644-tfidf-4" href="../high_scalability-2008/high_scalability-2008-05-03-Product%3A_nginx.html">314 high scalability-2008-05-03-Product: nginx</a></p>
<p>Introduction: Update 6 :  nginx_http_push_module . Turn nginx into a long-polling message queuing HTTP push server.
 
 Update 5 : In  Load Balancer Update  Barry describes how WordPress.com moved from Pound to Nginx and are now "regularly serving about 8-9k requests/second  and about 1.2Gbit/sec through a few Nginx instances and have plenty of room to grow!".   Update 4 :  Nginx better than Pound for load balancing.  Pound spikes at 80% CPU, Nginx uses 3% and is easier to understand and better documented.  Update 3 : igvita.com combines two cool tools together for better performance in  Nginx and Memcached, a 400% boost! .  Update 2 : Software Project on  Installing Nginx Web Server w/ PHP and SSL . Breaking away from mother Apache can be a scary proposition and this kind of getting started article really helps easy the separation.  Update:  Slicehost has some nice  tutorials on setting up Nginx .  From their website:    Nginx  ("engine x") is a high-performance HTTP server and reverse proxy, as wel</p><p>5 0.17384781 <a title="1644-tfidf-5" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>Introduction: This is a guest a post by Alvaro Videla describing their architecture for  Poppen.de , a popular German dating site. This site is very much NSFW, so be careful before clicking on the link. What I found most interesting is how they manage to sucessfully blend a little of the old with a little of the new, using technologies like Nginx, MySQL, CouchDB, and Erlang, Memcached, RabbitMQ, PHP, Graphite, Red5, and Tsung.
  What is Poppen.de?  
Poppen.de (NSFW) is the top dating website in  Germany, and while it may be a small site compared to giants like Flickr  or Facebook, we believe it's a nice architecture to learn from if you  are starting to get some scaling problems.
  The  Stats   
 2.000.000 users 
 20.000  concurrent users 
 300.000 private messages per day 
 250.000  logins per day 
 We have a team  of  eleven developers, two designers and two sysadmins for this project. 
   Business Model  
The site works with a  freemium model, where users can do for free things like: 
  
 Search</p><p>6 0.12475324 <a title="1644-tfidf-6" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>7 0.11900218 <a title="1644-tfidf-7" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>8 0.11784875 <a title="1644-tfidf-8" href="../high_scalability-2013/high_scalability-2013-08-26-Reddit%3A_Lessons_Learned_from_Mistakes_Made_Scaling_to_1_Billion_Pageviews_a_Month.html">1507 high scalability-2013-08-26-Reddit: Lessons Learned from Mistakes Made Scaling to 1 Billion Pageviews a Month</a></p>
<p>9 0.11721054 <a title="1644-tfidf-9" href="../high_scalability-2012/high_scalability-2012-09-26-WordPress.com_Serves_70%2C000_req-sec_and_over_15_Gbit-sec_of_Traffic_using_NGINX.html">1329 high scalability-2012-09-26-WordPress.com Serves 70,000 req-sec and over 15 Gbit-sec of Traffic using NGINX</a></p>
<p>10 0.11094157 <a title="1644-tfidf-10" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>11 0.1106116 <a title="1644-tfidf-11" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>12 0.10993335 <a title="1644-tfidf-12" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>13 0.10910149 <a title="1644-tfidf-13" href="../high_scalability-2010/high_scalability-2010-09-17-Hot_Scalability_Links_For_Sep_17%2C_2010.html">903 high scalability-2010-09-17-Hot Scalability Links For Sep 17, 2010</a></p>
<p>14 0.10404315 <a title="1644-tfidf-14" href="../high_scalability-2014/high_scalability-2014-03-19-Strategy%3A_Three_Techniques_to_Survive_Traffic_Surges_by_Quickly_Scaling_Your_Site.html">1615 high scalability-2014-03-19-Strategy: Three Techniques to Survive Traffic Surges by Quickly Scaling Your Site</a></p>
<p>15 0.10347104 <a title="1644-tfidf-15" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<p>16 0.099580087 <a title="1644-tfidf-16" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>17 0.098080039 <a title="1644-tfidf-17" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>18 0.096528679 <a title="1644-tfidf-18" href="../high_scalability-2013/high_scalability-2013-03-22-Stuff_The_Internet_Says_On_Scalability_For_March_22%2C_2013.html">1428 high scalability-2013-03-22-Stuff The Internet Says On Scalability For March 22, 2013</a></p>
<p>19 0.095799677 <a title="1644-tfidf-19" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>20 0.095331199 <a title="1644-tfidf-20" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.111), (2, -0.029), (3, -0.065), (4, -0.024), (5, -0.031), (6, 0.031), (7, 0.103), (8, -0.034), (9, 0.007), (10, 0.008), (11, -0.011), (12, 0.087), (13, -0.019), (14, -0.104), (15, 0.005), (16, 0.001), (17, 0.003), (18, -0.03), (19, -0.003), (20, 0.019), (21, -0.025), (22, -0.027), (23, -0.037), (24, 0.063), (25, 0.011), (26, -0.0), (27, 0.045), (28, 0.004), (29, -0.006), (30, 0.02), (31, -0.033), (32, -0.024), (33, 0.015), (34, 0.09), (35, -0.022), (36, -0.003), (37, 0.065), (38, -0.042), (39, 0.099), (40, 0.009), (41, 0.048), (42, 0.005), (43, -0.007), (44, 0.021), (45, -0.027), (46, -0.024), (47, -0.002), (48, -0.054), (49, 0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96507716 <a title="1644-lsi-1" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>Introduction: Our last article on Disqus:  How Disqus Went Realtime With 165K Messages Per Second And Less Than .2 Seconds Latency , was a little out of date, but the folks at Disqus have been busy implementing, not talking, so we don't know a lot about what they are doing now, but we do have a short update in  C1MM and NGINX  by John Watson and an article  Trying out this Go thing .
 
So Disqus has grown a bit:
  
 1.3 billion unique visitors 
 10 billion page views 
 500 million users engaged in discussions 
 3 million communities 
 25 million comments 
  
They are still all about realtime, but Go replaced Python in their Realtime system:
  
 Original Realtime backend was written in a pretty lightweight Python + gevent. 
 The realtime service is a hybrid of CPU intensive tasks + lots of network IO. Gevent was handling the network IO without an issue, but at higher contention, the CPU was choking everything. Switching over to Go removed that contention, which was the primary issue that was being se</p><p>2 0.78396553 <a title="1644-lsi-2" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>Introduction: Here's an  Update On Disqus: It's Still About Realtime, But Go Demolishes Python . 
 
How do you add realtime functionality to a web scale application? That's what  Adam Hitchcock , a Software Engineer at Disqus talks about in an excellent talk:  Making DISQUS Realtime  ( slides ).
 
Disqus had to take their commenting system and add realtime capabilities to it. Not something that's easy to do when at the time of the talk (2013) they had had just hit a billion unique visitors a month.
 
What Disqus developed is a realtime commenting system called “realertime” that was tested to handle 1.5 million concurrently connected users, 45,000 new connections per second, 165,000 messages/second, with less than .2 seconds latency end-to-end.
 
The nature of a commenting system is that it is IO bound and has a high fanout, that is a comment comes in and must be sent out to a lot of readers. It's a problem very similar to what  Twitter must solve . 
 
Disqus' solution was quite interesting as was th</p><p>3 0.74813896 <a title="1644-lsi-3" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><p>4 0.72814071 <a title="1644-lsi-4" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<p>Introduction: The paper  MegaPipe: A New Programming Interface for Scalable Network I/O  ( video ,  slides ) hits the common theme that if you want to go faster you need a better car design, not just a better driver. So that's why the authors started with a clean-slate and designed a network API from the ground up with support for concurrent I/O, a requirement for achieving high performance while scaling to large numbers of connections per thread, multiple cores, etc.  What they created is MegaPipe, "a new network programming API for message-oriented workloads to avoid the performance issues of BSD Socket API."
 
The result: MegaPipe outperforms baseline Linux between  29% (for long connections)  and  582% (for short connections) . MegaPipe improves the performance of a modiﬁed version of  memcached between 15% and 320% . For a workload based on real-world HTTP traces, MegaPipe boosts the throughput of  nginx by 75% .
 
What's this most excellent and interesting paper about?
  Message-oriented netwo</p><p>5 0.71123916 <a title="1644-lsi-5" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>Introduction: This question comes from Ulysses on an  interesting thread  from the Mechanical Sympathy news group, especially given how multiple processors are now the norm:
 
Ulysses:
   
 On an 8xCPU Linux instance,  is it at all advantageous to use the Linux taskset command to pin an 8xJVM process set (co-ordinated as a www.infinispan.org distributed cache/data grid) to a specific CPU affinity set  (i.e. pin JVM0 process to CPU 0, JVM1 process to CPU1, ...., JVM7process to CPU 7) vs. just letting the Linux OS use its default mechanism for provisioning the 8xJVM process set to the available CPUs? 
 In effrort to seek an optimal point (in the full event space), what are the conceptual trade-offs in considering "searching" each permutation of provisioning an 8xJVM process set to an 8xCPU set via taskset? 
   
Given  taskset  is they key to the question, it would help to have a definition:
  

Used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with</p><p>6 0.70795941 <a title="1644-lsi-6" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>7 0.67274874 <a title="1644-lsi-7" href="../high_scalability-2008/high_scalability-2008-05-03-Product%3A_nginx.html">314 high scalability-2008-05-03-Product: nginx</a></p>
<p>8 0.6608479 <a title="1644-lsi-8" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>9 0.65014189 <a title="1644-lsi-9" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>10 0.62902308 <a title="1644-lsi-10" href="../high_scalability-2012/high_scalability-2012-02-17-Stuff_The_Internet_Says_On_Scalability_For_February_17%2C_2012.html">1195 high scalability-2012-02-17-Stuff The Internet Says On Scalability For February 17, 2012</a></p>
<p>11 0.62764347 <a title="1644-lsi-11" href="../high_scalability-2012/high_scalability-2012-06-18-The_Clever_Ways_Chrome_Hides_Latency_by_Anticipating_Your_Every_Need.html">1267 high scalability-2012-06-18-The Clever Ways Chrome Hides Latency by Anticipating Your Every Need</a></p>
<p>12 0.6270085 <a title="1644-lsi-12" href="../high_scalability-2010/high_scalability-2010-10-26-Scaling_DISQUS_to_75_Million_Comments_and_17%2C000_RPS.html">928 high scalability-2010-10-26-Scaling DISQUS to 75 Million Comments and 17,000 RPS</a></p>
<p>13 0.62545854 <a title="1644-lsi-13" href="../high_scalability-2011/high_scalability-2011-03-03-Stack_Overflow_Architecture_Update_-_Now_at_95_Million_Page_Views_a_Month.html">998 high scalability-2011-03-03-Stack Overflow Architecture Update - Now at 95 Million Page Views a Month</a></p>
<p>14 0.62511104 <a title="1644-lsi-14" href="../high_scalability-2012/high_scalability-2012-08-30-Dramatically_Improving_Performance_by_Debugging_Brutally_Complex_Prolems.html">1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</a></p>
<p>15 0.61373121 <a title="1644-lsi-15" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>16 0.61296487 <a title="1644-lsi-16" href="../high_scalability-2012/high_scalability-2012-06-01-Stuff_The_Internet_Says_On_Scalability_For_June_1%2C_2012.html">1255 high scalability-2012-06-01-Stuff The Internet Says On Scalability For June 1, 2012</a></p>
<p>17 0.60979968 <a title="1644-lsi-17" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>18 0.6075452 <a title="1644-lsi-18" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>19 0.60650992 <a title="1644-lsi-19" href="../high_scalability-2013/high_scalability-2013-03-13-Iron.io_Moved_From_Ruby_to_Go%3A_28_Servers_Cut_and_Colossal_Clusterf%2A%2Aks_Prevented.html">1423 high scalability-2013-03-13-Iron.io Moved From Ruby to Go: 28 Servers Cut and Colossal Clusterf**ks Prevented</a></p>
<p>20 0.60384297 <a title="1644-lsi-20" href="../high_scalability-2011/high_scalability-2011-12-16-Stuff_The_Internet_Says_On_Scalability_For_December_16%2C_2011.html">1158 high scalability-2011-12-16-Stuff The Internet Says On Scalability For December 16, 2011</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.066), (2, 0.274), (10, 0.073), (30, 0.061), (40, 0.024), (51, 0.23), (61, 0.051), (79, 0.08), (94, 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92973638 <a title="1644-lda-1" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>Introduction: Our last article on Disqus:  How Disqus Went Realtime With 165K Messages Per Second And Less Than .2 Seconds Latency , was a little out of date, but the folks at Disqus have been busy implementing, not talking, so we don't know a lot about what they are doing now, but we do have a short update in  C1MM and NGINX  by John Watson and an article  Trying out this Go thing .
 
So Disqus has grown a bit:
  
 1.3 billion unique visitors 
 10 billion page views 
 500 million users engaged in discussions 
 3 million communities 
 25 million comments 
  
They are still all about realtime, but Go replaced Python in their Realtime system:
  
 Original Realtime backend was written in a pretty lightweight Python + gevent. 
 The realtime service is a hybrid of CPU intensive tasks + lots of network IO. Gevent was handling the network IO without an issue, but at higher contention, the CPU was choking everything. Switching over to Go removed that contention, which was the primary issue that was being se</p><p>2 0.91462404 <a title="1644-lda-2" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data ( arround 44 gigs right now ).     For some time I have been using rsync to keep all the content equal on all servers, but the amount of data has been growing, and rsync takes a few too much time to "compare" all data from source to destination, and create a lot of I/O.     I have been taking a look at MogileFS, it seems a good and reliable option, but as the fuse module is not finished, we should have to rewrite all our apps, and its not an option atm.     Any ideas?     I just want a "real time, non resource-hungry" solution alternative for rsync. If I get more features on the way, then they are welcome :)     Why I prefer to use a Distributed File System instead of using NAS + NFS?     - I need 2 NAS, if I dont want a point of failure, and NAS hard is expensive.   - Non-shared hardware, all server has their own local disks.   - As files are replicated, I can save a lot of money, RAID is not a MUST.     Thn</p><p>3 0.90891534 <a title="1644-lda-3" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus protocols. Henry starts with a very useful discussion of what all this talk about consensus really means:  The consensus problem is the problem of getting a set of nodes in a distributed system to agree on something - it might be a value, a course of action or a decision. Achieving consensus allows a distributed system to act as a single entity, with every individual node aware of and in agreement with the actions of the whole of the network.   In this article Henry tackles Two-Phase Commit, the protocol most databases use to arrive at a consensus for database writes. The article is very well written with lots of pretty and informative pictures. He did a really good job.  In conclusion we learn 2PC is very efficient, a minimal number of messages are exchanged and latency is low. The problem is when a co-ordinator fails availability is dramatically reduced. This is why 2PC isn't generally used on highly distributed</p><p>4 0.90129596 <a title="1644-lda-4" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to Mike Swift, in  Facebook gets ready for New Year's Eve , we get a little insight as to their method for the madness, nothing really detailed, but still interesting.
  Problem Setup   
 Facebook expects tha one billion+ photos will be shared on New Year's eve. 
 Facebook's 800 million users are scattered around the world. Three quarters live outside the US. Each user is linked to an average of 130 friends. 
 Photos and posts must appear in less than a second. Opening a homepage requires executing requests on a 100 different servers, and those requests have to be ranked, sorted, and privacy-checked, and then rendered. 
 Different events put different stresses on different parts of Facebook.       
 
 Photo and Video Uploads - Holidays require hundreds of terabytes of capacity  
 News Feed - News events like big sports events and the death of Steve Jobs drive user status updates 
 
 
   Coping Strategies   
  Try</p><p>5 0.88616157 <a title="1644-lda-5" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>Introduction: Think of building websites as engineering  composite materials . A composite material is when two or more materials are combined to create a third material that does something useful that the components couldn't do on their own. Composites like reinforced concrete have revolutionized design and construction. When building websites we usually bring different component materials together, like creating a composite, to get the features we need rather than building a completely new thing from scratch that does everything we want.
 
This approach has been seen as a hack because it leads to inelegancies like data duplication; great gobs of component glue; consistency issues; and messy operations. But what if the the composite approach is really a strength, not a hack, but a messy part of the world that needs to be embraced rather than belittled?
 
They key is to  see data as a material . Right now we are arguing which is the best single material to build with. Is it  NoSQL , relational, mass</p><p>6 0.88112164 <a title="1644-lda-6" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>7 0.87826782 <a title="1644-lda-7" href="../high_scalability-2011/high_scalability-2011-10-28-Stuff_The_Internet_Says_On_Scalability_For_October_28%2C_2011.html">1134 high scalability-2011-10-28-Stuff The Internet Says On Scalability For October 28, 2011</a></p>
<p>8 0.86732507 <a title="1644-lda-8" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>9 0.85636693 <a title="1644-lda-9" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>10 0.83591819 <a title="1644-lda-10" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>11 0.82039297 <a title="1644-lda-11" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>12 0.81912255 <a title="1644-lda-12" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>13 0.80001897 <a title="1644-lda-13" href="../high_scalability-2013/high_scalability-2013-01-30-Better_Browser_Caching_is_More_Important_than_No_Javascript_or_Fast_Networks_for_HTTP_Performance.html">1396 high scalability-2013-01-30-Better Browser Caching is More Important than No Javascript or Fast Networks for HTTP Performance</a></p>
<p>14 0.79564965 <a title="1644-lda-14" href="../high_scalability-2014/high_scalability-2014-03-28-Stuff_The_Internet_Says_On_Scalability_For_March_28th%2C_2014.html">1621 high scalability-2014-03-28-Stuff The Internet Says On Scalability For March 28th, 2014</a></p>
<p>15 0.79240537 <a title="1644-lda-15" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>16 0.78967774 <a title="1644-lda-16" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>17 0.78868705 <a title="1644-lda-17" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>18 0.78863239 <a title="1644-lda-18" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>19 0.78796941 <a title="1644-lda-19" href="../high_scalability-2013/high_scalability-2013-05-31-Stuff_The_Internet_Says_On_Scalability_For_May_31%2C_2013.html">1468 high scalability-2013-05-31-Stuff The Internet Says On Scalability For May 31, 2013</a></p>
<p>20 0.78736001 <a title="1644-lda-20" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
