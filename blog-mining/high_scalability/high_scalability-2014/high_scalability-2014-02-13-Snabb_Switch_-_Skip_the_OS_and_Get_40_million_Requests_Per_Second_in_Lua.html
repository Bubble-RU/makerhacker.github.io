<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2014" href="../home/high_scalability-2014_home.html">high_scalability-2014</a> <a title="high_scalability-2014-1595" href="#">high_scalability-2014-1595</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2014-1595-html" href="http://highscalability.com//blog/2014/2/13/snabb-switch-skip-the-os-and-get-40-million-requests-per-sec.html">html</a></p><p>Introduction: Snabb Switch  - a toolkit for solving novel problems in networking. If you are building a new packet-processing network appliance then you can use Snabb Switch to get the job done more quickly.
 
Here's a great impassioned overview from  erichocean :
  Or, you could just avoid the OS altogether:  https://github.com/SnabbCo/snabbswitch    

Our current engineering target is 1 million writes/sec and > 10 million reads/sec on top of an architecture similar to that, on a single box, to our fully transactional, MVCC database (write do not block reads, and vice versa) that runs in the same process (a la SQLite), which we've also merged with our application code and our caching tier, so we're down to—literally—a single process for what would have been at least three separate tiers in a traditional setup.


The result is that we had to move to measuring request latency in microseconds exclusively. The architecture (without additional application-specific processing) supports a wire-to-wire mes</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Snabb Switch  - a toolkit for solving novel problems in networking. [sent-1, score-0.377]
</p><p>2 If you are building a new packet-processing network appliance then you can use Snabb Switch to get the job done more quickly. [sent-2, score-0.125]
</p><p>3 Here's a great impassioned overview from  erichocean :   Or, you could just avoid the OS altogether:  https://github. [sent-3, score-0.264]
</p><p>4 The result is that we had to move to measuring request latency in microseconds exclusively. [sent-5, score-0.251]
</p><p>5 The architecture (without additional application-specific processing) supports a wire-to-wire messaging speed of 26 nanoseconds, or approx. [sent-6, score-0.225]
</p><p>6 To put that in perspective, that kind of performance is about 1/3 of what you'd need to be able to do to handle Facebook's messaging load (on average, obviously, Facebook bursts higher than the average at times. [sent-9, score-0.419]
</p><p>7 Point being, the OS is just plain out-of-date for how to solve heavy data plane problems efficiently. [sent-13, score-0.387]
</p><p>8 The disparity between what the OS can do and what the hardware is capable of delivering is off by a few orders of magnitude right now. [sent-14, score-0.517]
</p><p>9 It's downright ridiculous how much performance we're giving up for supposed "convenience" today. [sent-15, score-0.516]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('snabb', 0.376), ('os', 0.193), ('impassioned', 0.188), ('disparity', 0.176), ('sqlite', 0.168), ('downright', 0.168), ('versa', 0.162), ('ridiculous', 0.162), ('altogether', 0.157), ('switch', 0.154), ('messaging', 0.153), ('nanoseconds', 0.146), ('mvcc', 0.143), ('lua', 0.143), ('convenience', 0.138), ('la', 0.135), ('microseconds', 0.135), ('bursts', 0.135), ('toolkit', 0.133), ('average', 0.131), ('appliance', 0.125), ('plane', 0.122), ('tiers', 0.122), ('vice', 0.122), ('million', 0.122), ('plain', 0.12), ('measuring', 0.116), ('merged', 0.115), ('supposed', 0.104), ('facebook', 0.101), ('date', 0.101), ('https', 0.098), ('obviously', 0.096), ('target', 0.094), ('novel', 0.094), ('orders', 0.09), ('capable', 0.087), ('magnitude', 0.086), ('perspective', 0.086), ('transactional', 0.083), ('tier', 0.082), ('block', 0.082), ('giving', 0.082), ('solving', 0.079), ('delivering', 0.078), ('overview', 0.076), ('process', 0.076), ('heavy', 0.074), ('additional', 0.072), ('problems', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1595-tfidf-1" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>Introduction: Snabb Switch  - a toolkit for solving novel problems in networking. If you are building a new packet-processing network appliance then you can use Snabb Switch to get the job done more quickly.
 
Here's a great impassioned overview from  erichocean :
  Or, you could just avoid the OS altogether:  https://github.com/SnabbCo/snabbswitch    

Our current engineering target is 1 million writes/sec and > 10 million reads/sec on top of an architecture similar to that, on a single box, to our fully transactional, MVCC database (write do not block reads, and vice versa) that runs in the same process (a la SQLite), which we've also merged with our application code and our caching tier, so we're down to—literally—a single process for what would have been at least three separate tiers in a traditional setup.


The result is that we had to move to measuring request latency in microseconds exclusively. The architecture (without additional application-specific processing) supports a wire-to-wire mes</p><p>2 0.15011945 <a title="1595-tfidf-2" href="../high_scalability-2009/high_scalability-2009-07-12-SPHiveDB%3A_A_mixture_of_the_Key-Value_Store_and_the_Relational_Database..html">655 high scalability-2009-07-12-SPHiveDB: A mixture of the Key-Value Store and the Relational Database.</a></p>
<p>Introduction: The Key/Value Store becames more and more popular. When we use the Key/Value Store to store objects, we need to serialize/deserialize the objects as binary buffer. We have many ways to serialize/deserialize objects. A possible way is to use the Relational Database. Every value we store in the Key/Value Store is a SQLite instance. We can use the power of the Relational Database to manipulate the value. The SQL is very powerful for processing query request.   SPHiveDB = TokyoCabinet + SQLite http://code.google.com/p/sphivedb/  SPHiveDB is a server for sqlite database. It use JSON-RPC over HTTP to expose a network interface to use SQLite database. It supports combining multiple SQLite databases into one file ( through tokyo cabinet ). It also supports the use of multiple files.</p><p>3 0.11307415 <a title="1595-tfidf-3" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><p>4 0.10399158 <a title="1595-tfidf-4" href="../high_scalability-2007/high_scalability-2007-09-23-HA_for_switches.html">99 high scalability-2007-09-23-HA for switches</a></p>
<p>Introduction: Hi,      Can someone teach me how you implement network switch fail over since we are paranoid for single point of failure.     For example, you have:     a dozen web servers -> switch -> DB cluster     that switch is a SPOF.  How does one implement dual switch in a fail over fashion?</p><p>5 0.09907987 <a title="1595-tfidf-5" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>Introduction: There’s some amount of debate whether Facebook  really  crossed over the one trillion page view per month threshold. While one report says it did,  another respected firm says it did not ; that its monthly page views are a mere 467 billion per month.


In the big scheme of things, the discrepancy is somewhat irrelevant, as neither show the  true  load on Facebook’s infrastructure – which is far more impressive a set of numbers than its externally measured “page view” metric.  Mashable reported in “ Facebook Surpasses 1 Trillion Pageviews per Month ” that the social networking giant saw “approximately 870 million unique visitors in June and 860 million in July” and followed up with some per visitor statistics, indicating “each visitor averaged approximately 1,160 page views in July and 40 per visit — enormous by any standard. Time spent on the site was around 25 minutes per user.”


From an architectural standpoint it’s not  just  about the page views. It’s about requests and responses,</p><p>6 0.098288924 <a title="1595-tfidf-6" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>7 0.096345432 <a title="1595-tfidf-7" href="../high_scalability-2011/high_scalability-2011-05-12-Paper%3A_Mind_the_Gap%3A_Reconnecting_Architecture_and_OS_Research.html">1039 high scalability-2011-05-12-Paper: Mind the Gap: Reconnecting Architecture and OS Research</a></p>
<p>8 0.091760643 <a title="1595-tfidf-8" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>9 0.090845518 <a title="1595-tfidf-9" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>10 0.088404395 <a title="1595-tfidf-10" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>11 0.087550595 <a title="1595-tfidf-11" href="../high_scalability-2013/high_scalability-2013-05-17-Stuff_The_Internet_Says_On_Scalability_For_May_17%2C_2013.html">1460 high scalability-2013-05-17-Stuff The Internet Says On Scalability For May 17, 2013</a></p>
<p>12 0.085496388 <a title="1595-tfidf-12" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>13 0.080416501 <a title="1595-tfidf-13" href="../high_scalability-2011/high_scalability-2011-07-26-Web_2.0_Killed_the_Middleware_Star.html">1087 high scalability-2011-07-26-Web 2.0 Killed the Middleware Star</a></p>
<p>14 0.079189144 <a title="1595-tfidf-14" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>15 0.078342952 <a title="1595-tfidf-15" href="../high_scalability-2009/high_scalability-2009-02-16-Handle_1_Billion_Events_Per_Day_Using_a_Memory_Grid.html">513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</a></p>
<p>16 0.076366678 <a title="1595-tfidf-16" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>17 0.076348022 <a title="1595-tfidf-17" href="../high_scalability-2008/high_scalability-2008-02-12-Product%3A_rPath_-_Creating_and_Managing_Virtual_Appliances.html">245 high scalability-2008-02-12-Product: rPath - Creating and Managing Virtual Appliances</a></p>
<p>18 0.076178253 <a title="1595-tfidf-18" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>19 0.075638682 <a title="1595-tfidf-19" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>20 0.075426131 <a title="1595-tfidf-20" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, 0.064), (2, -0.021), (3, -0.02), (4, 0.01), (5, -0.007), (6, 0.019), (7, 0.056), (8, -0.02), (9, 0.026), (10, 0.016), (11, 0.012), (12, 0.016), (13, 0.017), (14, -0.032), (15, 0.028), (16, 0.019), (17, -0.02), (18, 0.002), (19, 0.029), (20, 0.046), (21, 0.063), (22, 0.04), (23, -0.032), (24, 0.048), (25, -0.027), (26, -0.001), (27, -0.024), (28, 0.03), (29, 0.027), (30, -0.023), (31, -0.005), (32, -0.013), (33, -0.001), (34, 0.017), (35, 0.061), (36, -0.028), (37, -0.023), (38, 0.03), (39, 0.019), (40, -0.027), (41, 0.006), (42, 0.008), (43, -0.003), (44, -0.006), (45, -0.019), (46, -0.012), (47, 0.002), (48, -0.029), (49, 0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97347772 <a title="1595-lsi-1" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>Introduction: Snabb Switch  - a toolkit for solving novel problems in networking. If you are building a new packet-processing network appliance then you can use Snabb Switch to get the job done more quickly.
 
Here's a great impassioned overview from  erichocean :
  Or, you could just avoid the OS altogether:  https://github.com/SnabbCo/snabbswitch    

Our current engineering target is 1 million writes/sec and > 10 million reads/sec on top of an architecture similar to that, on a single box, to our fully transactional, MVCC database (write do not block reads, and vice versa) that runs in the same process (a la SQLite), which we've also merged with our application code and our caching tier, so we're down to—literally—a single process for what would have been at least three separate tiers in a traditional setup.


The result is that we had to move to measuring request latency in microseconds exclusively. The architecture (without additional application-specific processing) supports a wire-to-wire mes</p><p>2 0.82577008 <a title="1595-lsi-2" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>Introduction: Our latest strategy is taken from a  great post by Paul Saab of Facebook , detailing how with changes Facebook has made to memcached they have:
  ...been able to scale memcached to handle 200,000 UDP requests per second with an average latency of 173 microseconds. The total throughput achieved is 300,000 UDP requests/s, but the latency at that request rate is too high to be useful in our system. This is an amazing increase from 50,000 UDP requests/s using the stock version of Linux and memcached.  
To scale Facebook has hundreds of thousands of TCP connections open to their memcached processes. First, this is still amazing. It's not so long ago you could have never done this. Optimizing connection use was always a priority because the OS simply couldn't handle large numbers of connections or large numbers of threads or large numbers of CPUs. To get to this point is a big accomplishment. Still, at that scale there are problems that are often solved.  Some of the problem Facebook faced a</p><p>3 0.79483473 <a title="1595-lsi-3" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>Introduction: Rick Reedin an upcoming talk in March titledThat's 'Billion' with a 'B':
Scaling to the next level at WhatsAppreveals some eye
poppingWhatsAppstats:What has hundreds of nodes, thousands of cores, hundreds
of terabytes of RAM, and hopes to serve the billions of smartphones that will
soon be a reality around the globe? The Erlang/FreeBSD-based server
infrastructure at WhatsApp. We've faced many challenges in meeting the ever-
growing demand for our messaging services, but as we continue to push the
envelope on size (>8000 cores) and speed (>70M Erlang messages per second) of
our serving system.But since we don't have that talk yet, let's take a look at
a talk Rick Reed gave two years ago on WhatsApp:Scaling to Millions of
Simultaneous Connections.Having built a high performance messaging bus in C++
while at Yahoo, Rick Reed is not new to the world of high scalability
architectures. The founders are also ex-Yahoo guys with not a little
experience scaling systems. So WhatsApp comes by thei</p><p>4 0.76989353 <a title="1595-lsi-4" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>Introduction: There’s some amount of debate whether Facebook  really  crossed over the one trillion page view per month threshold. While one report says it did,  another respected firm says it did not ; that its monthly page views are a mere 467 billion per month.


In the big scheme of things, the discrepancy is somewhat irrelevant, as neither show the  true  load on Facebook’s infrastructure – which is far more impressive a set of numbers than its externally measured “page view” metric.  Mashable reported in “ Facebook Surpasses 1 Trillion Pageviews per Month ” that the social networking giant saw “approximately 870 million unique visitors in June and 860 million in July” and followed up with some per visitor statistics, indicating “each visitor averaged approximately 1,160 page views in July and 40 per visit — enormous by any standard. Time spent on the site was around 25 minutes per user.”


From an architectural standpoint it’s not  just  about the page views. It’s about requests and responses,</p><p>5 0.76234901 <a title="1595-lsi-5" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>Introduction: Robert Johnson,   a director of engineering at Facebook, celebrated Facebook's monumental achievement of reaching 500 million users by sharing the  scaling principles that helped  reach that milestone. In case you weren't suitably impressed by the 500 million user number, Robert ratchets up the numbers game with these impressive figures:      
 
 1 million users per engineer 
 500 million active users 
 100 billion hits per day 
 50 billion photos 
 2 trillion objects cached, with hundreds of millions of requests per second 
 130TB of logs every day 
 
  
How did Facebook get to this point?
   
  People Matter Most . It's people who build and run systems. The best tools for scaling are an engineering and operations teams that can handle anything. 
  Scale Horizontally . Handling exponentially growing traffic requires spreading load arbitrarily across many machines. Using different databases for tables like accounts and profiles only doubles capacity. This approach hurts efficiency, but</p><p>6 0.74447221 <a title="1595-lsi-6" href="../high_scalability-2009/high_scalability-2009-02-16-Handle_1_Billion_Events_Per_Day_Using_a_Memory_Grid.html">513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</a></p>
<p>7 0.74310231 <a title="1595-lsi-7" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>8 0.73935884 <a title="1595-lsi-8" href="../high_scalability-2012/high_scalability-2012-04-06-Stuff_The_Internet_Says_On_Scalability_For_April_6%2C_2012.html">1223 high scalability-2012-04-06-Stuff The Internet Says On Scalability For April 6, 2012</a></p>
<p>9 0.71870792 <a title="1595-lsi-9" href="../high_scalability-2012/high_scalability-2012-12-14-Stuff_The_Internet_Says_On_Scalability_For_December_14%2C_2012.html">1372 high scalability-2012-12-14-Stuff The Internet Says On Scalability For December 14, 2012</a></p>
<p>10 0.71667051 <a title="1595-lsi-10" href="../high_scalability-2010/high_scalability-2010-12-31-Facebook_in_20_Minutes%3A_2.7M_Photos%2C_10.2M_Comments%2C_4.6M_Messages.html">966 high scalability-2010-12-31-Facebook in 20 Minutes: 2.7M Photos, 10.2M Comments, 4.6M Messages</a></p>
<p>11 0.70444739 <a title="1595-lsi-11" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>12 0.70419717 <a title="1595-lsi-12" href="../high_scalability-2011/high_scalability-2011-12-30-Stuff_The_Internet_Says_On_Scalability_For_December_30%2C_2011.html">1166 high scalability-2011-12-30-Stuff The Internet Says On Scalability For December 30, 2011</a></p>
<p>13 0.70377469 <a title="1595-lsi-13" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>14 0.70245069 <a title="1595-lsi-14" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>15 0.7023167 <a title="1595-lsi-15" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<p>16 0.69949889 <a title="1595-lsi-16" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>17 0.69896853 <a title="1595-lsi-17" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>18 0.69306624 <a title="1595-lsi-18" href="../high_scalability-2011/high_scalability-2011-12-09-Stuff_The_Internet_Says_On_Scalability_For_December_9%2C_2011.html">1154 high scalability-2011-12-09-Stuff The Internet Says On Scalability For December 9, 2011</a></p>
<p>19 0.68908429 <a title="1595-lsi-19" href="../high_scalability-2012/high_scalability-2012-08-24-Stuff_The_Internet_Says_On_Scalability_For_August_24%2C_2012.html">1311 high scalability-2012-08-24-Stuff The Internet Says On Scalability For August 24, 2012</a></p>
<p>20 0.68900621 <a title="1595-lsi-20" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.092), (2, 0.251), (29, 0.127), (45, 0.075), (56, 0.031), (60, 0.019), (61, 0.044), (64, 0.03), (79, 0.125), (85, 0.071), (94, 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92941904 <a title="1595-lda-1" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>Introduction: Snabb Switch  - a toolkit for solving novel problems in networking. If you are building a new packet-processing network appliance then you can use Snabb Switch to get the job done more quickly.
 
Here's a great impassioned overview from  erichocean :
  Or, you could just avoid the OS altogether:  https://github.com/SnabbCo/snabbswitch    

Our current engineering target is 1 million writes/sec and > 10 million reads/sec on top of an architecture similar to that, on a single box, to our fully transactional, MVCC database (write do not block reads, and vice versa) that runs in the same process (a la SQLite), which we've also merged with our application code and our caching tier, so we're down to—literally—a single process for what would have been at least three separate tiers in a traditional setup.


The result is that we had to move to measuring request latency in microseconds exclusively. The architecture (without additional application-specific processing) supports a wire-to-wire mes</p><p>2 0.91568142 <a title="1595-lda-2" href="../high_scalability-2010/high_scalability-2010-06-22-Exploring_the_software_behind_Facebook%2C_the_world%E2%80%99s_largest_site.html">845 high scalability-2010-06-22-Exploring the software behind Facebook, the world’s largest site</a></p>
<p>Introduction: Peter Alguacil at Pingdom wrote a HighScalability worthy article on Facebook's architecture:  Exploring the software behind Facebook, the world’s largest site . It covers the challenges Facebook faces, the software Facebook uses, and the techniques Facebook uses to keep on scaling. Definitely worth a look.</p><p>3 0.90513623 <a title="1595-lda-3" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>Introduction: It's HighScalability Time:
  
  100PB  : Facebook HDFS Cluster;  One Trillion  : Objects in S3 
 Quotable quotes:                           
 
  @mwinkle  : Listening to NASA big data challenges at ‪#hadoopSummit‬, the square kilometer array project will produce 700tb per second. TB. Per second. 
  @imrantech  : #hadoopsummit‬ @twitter - 400M tweets, 80-100TB per day 
  @r39132  : At Netflix talk at ‪#hadoopsummit‬ : 2 B hours streamed in Q4 2011, 75% of the 30M daily movie starts are sourced from recommendations 
  @nattybnatkins  : Run job. Identify bottleneck. Address bottleneck. Repeat. Sage wisdom from @tlipcon on optimizing MR jobs ‬ ‪#HadoopSummit‬ 
  @chiradeep  :  mainframe cost of operation - $5k per MIP per year ‪#hadoopsummit‬ 
  @MCanalytics  : #hadoopsummit‬ Yahoo metrics - 140pb on 42k nodes with 500 users on 360k Hadoop jobs for 100b events/day Holy smokes! 
  @M_Wein  : Domain expertise is the wave of the future: it's more about "Hadoop and Healthcare" than "Using Baye</p><p>4 0.90203637 <a title="1595-lda-4" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>Introduction: Skype's 220 millions users lost service for a stunning two days. The primary cause for Skype's nightmare (can you imagine the beeper storm that went off?) was a massive global roll-out of a  Window's patch  triggering the simultaneous reboot of millions of machines across the globe.  The secondary cause was a bug in Skype's software that prevented "self-healing" in the face of such attacks. The flood of log-in requests and a lack of "peer-to-peer resources" melted their system.
   
Who's fault is it? Is Skype to blame? Is Microsoft to blame? Or is the peer-to-peer model itself fundamentally flawed in some way?  Let's be real, how could Skype possibly test booting 220 million servers over a random configuration of resources? Answer: they can't. Yes, it's Skype's responsibility, but they are in a bit of a pickle on this one.  The boot scenario is one of the most basic and one of the most difficult scalability scenarios to plan for and test. You can't simulate the viciousness of real-life</p><p>5 0.87829316 <a title="1595-lda-5" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>Introduction: As it is said in the recent article  "Google: Taming the Long Latency Tail - When More Machines Equals Worse Results"  , latency variability has greater impact in larger scale clusters where a typical request is composed of multiple distributed/parallel requests. The overall response time dramatically decreases if latency of each request is not consistent and low. 
 
In dynamically scalable partitioned storage systems, whether it is a NoSQL database, filesystem or in-memory data grid, changes in the cluster (adding or removing a node) can lead to big data moves in the network to re-balance the cluster. Re-balancing will be needed for both primary and backup data on those nodes. If a node crashes for example, dead node’s data has to be re-owned (become primary) by other node(s) and also its backup has to be taken immediately to be fail-safe again. Shuffling MBs of data around has a negative effect in the cluster as it consumes your valuable resources such as network, CPU and RAM. It mig</p><p>6 0.87708795 <a title="1595-lda-6" href="../high_scalability-2009/high_scalability-2009-03-06-Cloud_Programming_Directly_Feeds_Cost_Allocation_Back_into_Software_Design.html">527 high scalability-2009-03-06-Cloud Programming Directly Feeds Cost Allocation Back into Software Design</a></p>
<p>7 0.87618017 <a title="1595-lda-7" href="../high_scalability-2008/high_scalability-2008-03-05-Oprah_is_the_Real_Social_Network.html">267 high scalability-2008-03-05-Oprah is the Real Social Network</a></p>
<p>8 0.87356448 <a title="1595-lda-8" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>9 0.8735584 <a title="1595-lda-9" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>10 0.87300676 <a title="1595-lda-10" href="../high_scalability-2013/high_scalability-2013-04-12-Stuff_The_Internet_Says_On_Scalability_For_April_12%2C_2013.html">1439 high scalability-2013-04-12-Stuff The Internet Says On Scalability For April 12, 2013</a></p>
<p>11 0.87280673 <a title="1595-lda-11" href="../high_scalability-2009/high_scalability-2009-04-04-Digg_Architecture.html">554 high scalability-2009-04-04-Digg Architecture</a></p>
<p>12 0.87279725 <a title="1595-lda-12" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<p>13 0.87201267 <a title="1595-lda-13" href="../high_scalability-2012/high_scalability-2012-12-28-Stuff_The_Internet_Says_On_Scalability_For_December_28%2C_2012.html">1378 high scalability-2012-12-28-Stuff The Internet Says On Scalability For December 28, 2012</a></p>
<p>14 0.8717286 <a title="1595-lda-14" href="../high_scalability-2013/high_scalability-2013-04-05-Stuff_The_Internet_Says_On_Scalability_For_April_5%2C_2013.html">1436 high scalability-2013-04-05-Stuff The Internet Says On Scalability For April 5, 2013</a></p>
<p>15 0.87156618 <a title="1595-lda-15" href="../high_scalability-2010/high_scalability-2010-11-09-Facebook_Uses_Non-Stored_Procedures_to_Update_Social_Graphs.html">936 high scalability-2010-11-09-Facebook Uses Non-Stored Procedures to Update Social Graphs</a></p>
<p>16 0.87153691 <a title="1595-lda-16" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>17 0.87043983 <a title="1595-lda-17" href="../high_scalability-2007/high_scalability-2007-07-24-Product%3A__Hibernate_Shards.html">24 high scalability-2007-07-24-Product:  Hibernate Shards</a></p>
<p>18 0.87003523 <a title="1595-lda-18" href="../high_scalability-2011/high_scalability-2011-02-22-Is_Node.js_Becoming_a_Part_of_the_Stack%3F_SimpleGeo_Says_Yes..html">993 high scalability-2011-02-22-Is Node.js Becoming a Part of the Stack? SimpleGeo Says Yes.</a></p>
<p>19 0.86944938 <a title="1595-lda-19" href="../high_scalability-2013/high_scalability-2013-04-26-Stuff_The_Internet_Says_On_Scalability_For_April_26%2C_2013.html">1447 high scalability-2013-04-26-Stuff The Internet Says On Scalability For April 26, 2013</a></p>
<p>20 0.86930227 <a title="1595-lda-20" href="../high_scalability-2013/high_scalability-2013-03-22-Stuff_The_Internet_Says_On_Scalability_For_March_22%2C_2013.html">1428 high scalability-2013-03-22-Stuff The Internet Says On Scalability For March 22, 2013</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
