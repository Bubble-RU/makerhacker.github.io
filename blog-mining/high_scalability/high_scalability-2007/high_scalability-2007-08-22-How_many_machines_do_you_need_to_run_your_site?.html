<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 high scalability-2007-08-22-How many machines do you need to run your site?</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-70" href="#">high_scalability-2007-70</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 high scalability-2007-08-22-How many machines do you need to run your site?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-70-html" href="http://highscalability.com//blog/2007/8/22/how-many-machines-do-you-need-to-run-your-site.html">html</a></p><p>Introduction: Amazingly   TechCrunch   runs their website on one web server and one database server, according to the fascinating survey   What the Webâ&euro;&trade;s most popular sites are running on   by   Pingdom  , a  provider of uptime and response time monitoring.      Early we learned   PlentyOfFish   catches and releases many millions of hits a day on just 1 web server and three database servers.   Google   runs a   Dalek   army full of servers.   YouSendIt  , a company making it easy to send and receive large files, has 24 web servers,  3 database servers, 170 storage servers, and a few miscellaneous servers.   Vimeo  , a video sharing company, has 100 servers for streaming video, 4 web servers, and 2 database servers.   Meebo  , an AJAX based instant messaging company, uses 40 servers to handle messaging, over 40 web servers,  and 10 servers for forums, jabber, testing, and so on.   FeedBurner  , a news feed management company, has 70 web servers, 15 database servers, and 10 miscellaneous servers. Now</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Amazingly   TechCrunch   runs their website on one web server and one database server, according to the fascinating survey   What the Webâ&euro;&trade;s most popular sites are running on   by   Pingdom  , a  provider of uptime and response time monitoring. [sent-1, score-0.567]
</p><p>2 Early we learned   PlentyOfFish   catches and releases many millions of hits a day on just 1 web server and three database servers. [sent-2, score-0.408]
</p><p>3 YouSendIt  , a company making it easy to send and receive large files, has 24 web servers,  3 database servers, 170 storage servers, and a few miscellaneous servers. [sent-4, score-0.626]
</p><p>4 Vimeo  , a video sharing company, has 100 servers for streaming video, 4 web servers, and 2 database servers. [sent-5, score-0.679]
</p><p>5 Meebo  , an AJAX based instant messaging company, uses 40 servers to handle messaging, over 40 web servers,  and 10 servers for forums, jabber, testing, and so on. [sent-6, score-0.797]
</p><p>6 FeedBurner  , a news feed management company, has 70 web servers, 15 database servers, and 10 miscellaneous servers. [sent-7, score-0.37]
</p><p>7 How many servers will you need and how can you trick yourself into using fewer? [sent-9, score-0.51]
</p><p>8 Find Someone Like You and Base Your Resource Estimates Off Them       We see quite a disparity in the number of servers needed for popular web sites. [sent-10, score-0.601]
</p><p>9 It ranges from just a few servers to many hundreds. [sent-11, score-0.41]
</p><p>10 The easiest approach to figuring out how many servers you'll need is to find a company similar to yours and look how many they need. [sent-13, score-0.887]
</p><p>11 Get Someone Else to Do it       Clearly content sites end up needing a lot of servers. [sent-20, score-0.356]
</p><p>12 This drops the expense of creating a large highly available storage infrastructure so much that it creates a whole new level of competition for content rich sites. [sent-26, score-0.346]
</p><p>13 If your web site is relatively simple blog then with mostly static content then you can get away with far fewer servers. [sent-38, score-0.51]
</p><p>14 And others sites that rely on social interaction, like   Google Talk  , may scale exponentially as users are added. [sent-44, score-0.451]
</p><p>15 Or can you get away with just beefing up the database server cache? [sent-50, score-0.386]
</p><p>16 Do you need servers for application specific tasks? [sent-51, score-0.401]
</p><p>17 GoogleTalk has to have servers for handling presence calculations. [sent-55, score-0.301]
</p><p>18 GigaVox needs servers to transcode podcasts into different formats and include fresh commercial content. [sent-57, score-0.47]
</p><p>19 If you are a calendar service you may need servers to calculate more complicated schedule availability schemes and to sync address books. [sent-58, score-0.48]
</p><p>20 So depending on your site, you may have to budget for many application related servers like these. [sent-59, score-0.489]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('servers', 0.301), ('company', 0.178), ('sites', 0.175), ('miscellaneous', 0.16), ('forums', 0.129), ('affordable', 0.121), ('site', 0.12), ('web', 0.117), ('exponentially', 0.116), ('blogs', 0.115), ('many', 0.109), ('away', 0.108), ('linearly', 0.107), ('needing', 0.105), ('need', 0.1), ('infrastructure', 0.098), ('gigavox', 0.096), ('beefing', 0.096), ('colorful', 0.096), ('laziness', 0.096), ('masterful', 0.096), ('competition', 0.094), ('youtube', 0.094), ('database', 0.093), ('popular', 0.093), ('someone', 0.091), ('video', 0.091), ('approach', 0.09), ('feedburner', 0.09), ('disparity', 0.09), ('googletalk', 0.09), ('jealous', 0.09), ('fewer', 0.089), ('server', 0.089), ('podcasts', 0.086), ('attachments', 0.086), ('plentyoffish', 0.086), ('transcode', 0.083), ('jabber', 0.083), ('scale', 0.081), ('nine', 0.08), ('may', 0.079), ('storage', 0.078), ('messaging', 0.078), ('profiled', 0.078), ('sharing', 0.077), ('multiply', 0.076), ('content', 0.076), ('fame', 0.074), ('commoditized', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="70-tfidf-1" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>Introduction: Amazingly   TechCrunch   runs their website on one web server and one database server, according to the fascinating survey   What the Webâ&euro;&trade;s most popular sites are running on   by   Pingdom  , a  provider of uptime and response time monitoring.      Early we learned   PlentyOfFish   catches and releases many millions of hits a day on just 1 web server and three database servers.   Google   runs a   Dalek   army full of servers.   YouSendIt  , a company making it easy to send and receive large files, has 24 web servers,  3 database servers, 170 storage servers, and a few miscellaneous servers.   Vimeo  , a video sharing company, has 100 servers for streaming video, 4 web servers, and 2 database servers.   Meebo  , an AJAX based instant messaging company, uses 40 servers to handle messaging, over 40 web servers,  and 10 servers for forums, jabber, testing, and so on.   FeedBurner  , a news feed management company, has 70 web servers, 15 database servers, and 10 miscellaneous servers. Now</p><p>2 0.18840717 <a title="70-tfidf-2" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>Introduction: Update 2:   YouTube Reaches One Billion Views Per Day .  That’s at least 11,574 views per second, 694,444 views per minute, and 41,666,667 views per hour.     Update:   YouTube: The Platform . YouTube adds a new rich set of APIs in order to become your video platform leader--all for free. Upload, edit, watch, search, and comment on video from your own site without visiting YouTube. Compose your site internally from APIs because you'll need to expose them later anyway.   YouTube grew incredibly fast, to over 100 million video views per day, with only a handful of people responsible for scaling the site. How did they manage to deliver all that video to all those users? And how have they evolved since being acquired by Google?
  Information Sources   
    Google Video  
   Platform   
  Apache 
  Python 
  Linux (SuSe) 
  MySQL  
  psyco, a dynamic python->C compiler 
  lighttpd for video instead of Apache  
   What's Inside?   The Stats   
  Supports the delivery of over 100 million vide</p><p>3 0.18608478 <a title="70-tfidf-3" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>Introduction: The future is live. The future is real-time. The future is now. That's the hype anyway. And as it has a habit of doing, the hype is slowly becoming reality. We are seeing live searches, live tweets, live location, live reality augmentation, live crab (fresh and local), and live event publishing. One of the most challenging of all live technologies is that of live video broadcasting. Imagine a world in which everyone becomes a  broadcaster  and a consumer of video streams, all in real-time (< 250 msec latency), all so you can talk and interact directly without feeling like you are in the middle of a time shift war. The resources and the engineering needed to make this happened must be substantial. How do you do that?
 
To find out I talked to Kyle Vogt, Justin.tv Founder and VP of Engineering. Justin.tv certainly has the numbers. Their 30 million unique monthly visitors even outshine YouTube in the video upload game, reportedly uploading nearly 30 hours per minute of video compared to Y</p><p>4 0.18589051 <a title="70-tfidf-4" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>Introduction: This is one of my favorite posts for a couple of reasons. I think it gives a lot of useful information in an interesting space. And Kyle Vogt was just a real pleasure to talk to. He was very helpful and forthcoming, which makes the whole experience better for everyone. 
 
 
 
The future is live. The future is real-time. The future is now. That's the hype anyway. And as it has a habit of doing, the hype is slowly becoming reality. We are seeing live searches, live tweets, live location, live reality augmentation, live crab (fresh and local), and live event publishing. One of the most challenging of all live technologies is that of live video broadcasting. Imagine a world in which everyone becomes a  broadcaster  and a consumer of video streams, all in real-time (< 250 msec latency), all so you can talk and interact directly without feeling like you are in the middle of a time shift war. The resources and the engineering needed to make this happened must be substantial. How do you do tha</p><p>5 0.17772816 <a title="70-tfidf-5" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>Introduction: One of the best projects I've ever worked on was creating a large scale web site publishing system that was almost entirely static. A large team of very talented creatives made the art work, writers penned the content, and designers generated templates. All assets were controlled in a database. Then all that was extracted, after applying many different filters, to a static site that was uploaded via ftp to dozens of web servers. It worked great. Reliable, fast, cheap, and simple. Updates were a bit of a pain as it required pushing a lot of files to a lot of servers, and that took time, but otherwise a solid system.
 
 Alas, this elegant system was replaced with a new fangled dynamic database based system. Content was pulled from a database using a dynamic language generated front-end. With a recent series of posts from Amazon's Werner Vogels, chronicling his experience of transforming his   All Things Distributed   blog into a static site using S3's ability to serve web pages, I get th</p><p>6 0.17756787 <a title="70-tfidf-6" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>7 0.17507051 <a title="70-tfidf-7" href="../high_scalability-2012/high_scalability-2012-06-05-Sponsored_Post%3A_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_Attribution_Modeling%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1257 high scalability-2012-06-05-Sponsored Post: Digital Ocean, NetDNA, Torbit, Velocity, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, Attribution Modeling, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>8 0.17193568 <a title="70-tfidf-8" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>9 0.17181218 <a title="70-tfidf-9" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>10 0.16956842 <a title="70-tfidf-10" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>11 0.16756578 <a title="70-tfidf-11" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>12 0.16711855 <a title="70-tfidf-12" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>13 0.16196159 <a title="70-tfidf-13" href="../high_scalability-2008/high_scalability-2008-02-25-Any_Suggestions_for_the_Architecture_Template%3F.html">259 high scalability-2008-02-25-Any Suggestions for the Architecture Template?</a></p>
<p>14 0.16196159 <a title="70-tfidf-14" href="../high_scalability-2008/high_scalability-2008-02-25-Architecture_Template_Advice_Needed.html">260 high scalability-2008-02-25-Architecture Template Advice Needed</a></p>
<p>15 0.15907681 <a title="70-tfidf-15" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>16 0.15638694 <a title="70-tfidf-16" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>17 0.15537733 <a title="70-tfidf-17" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>18 0.15371008 <a title="70-tfidf-18" href="../high_scalability-2013/high_scalability-2013-11-19-We_Finally_Cracked_the_10K_Problem_-_This_Time_for_Managing_Servers_with_2000x_Servers_Managed_Per_Sysadmin.html">1550 high scalability-2013-11-19-We Finally Cracked the 10K Problem - This Time for Managing Servers with 2000x Servers Managed Per Sysadmin</a></p>
<p>19 0.15328869 <a title="70-tfidf-19" href="../high_scalability-2007/high_scalability-2007-07-06-Start_Here.html">1 high scalability-2007-07-06-Start Here</a></p>
<p>20 0.15290497 <a title="70-tfidf-20" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.304), (1, 0.082), (2, 0.011), (3, -0.183), (4, -0.038), (5, -0.109), (6, -0.044), (7, -0.087), (8, -0.004), (9, 0.106), (10, -0.016), (11, -0.025), (12, -0.07), (13, 0.015), (14, 0.106), (15, 0.024), (16, 0.025), (17, 0.047), (18, -0.004), (19, 0.003), (20, -0.036), (21, 0.019), (22, 0.042), (23, -0.012), (24, 0.033), (25, -0.049), (26, 0.054), (27, 0.024), (28, 0.001), (29, 0.006), (30, 0.008), (31, 0.013), (32, 0.011), (33, -0.016), (34, -0.042), (35, -0.021), (36, -0.01), (37, -0.003), (38, 0.021), (39, 0.05), (40, 0.069), (41, -0.033), (42, 0.002), (43, -0.029), (44, 0.006), (45, -0.024), (46, 0.005), (47, -0.064), (48, 0.049), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97524124 <a title="70-lsi-1" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>Introduction: Amazingly   TechCrunch   runs their website on one web server and one database server, according to the fascinating survey   What the Webâ&euro;&trade;s most popular sites are running on   by   Pingdom  , a  provider of uptime and response time monitoring.      Early we learned   PlentyOfFish   catches and releases many millions of hits a day on just 1 web server and three database servers.   Google   runs a   Dalek   army full of servers.   YouSendIt  , a company making it easy to send and receive large files, has 24 web servers,  3 database servers, 170 storage servers, and a few miscellaneous servers.   Vimeo  , a video sharing company, has 100 servers for streaming video, 4 web servers, and 2 database servers.   Meebo  , an AJAX based instant messaging company, uses 40 servers to handle messaging, over 40 web servers,  and 10 servers for forums, jabber, testing, and so on.   FeedBurner  , a news feed management company, has 70 web servers, 15 database servers, and 10 miscellaneous servers. Now</p><p>2 0.85692304 <a title="70-lsi-2" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>Introduction: Slashdot effect  : overwhelming unprepared sites with an avalanche of reader's clicks after being mentioned on Slashdot. Sure, we now have the "Digg effect" and other hot new stars, but Slashdot was the original. And like many stars from generations past, Slashdot plays the elder statesman's role with with class, dignity, and restraint. Yet with millions and millions of users Slashdot is still box office gold and more than keeps up with the young'ins. And with age comes the wisdom of learning how to handle all those users. Just how does Slashdot scale and what can you learn by going old school?       Site: http://slashdot.org       Information Sources          Slashdot's Setup, Part 1- Hardware     Slashdot's Setup, Part 2- Software     History of Slashdot Part 3- Going Corporate     The History of Slashdot Part 4 - Yesterday, Today, Tomorrow     The Platform      MySQL   Linux (CentOS/RHEL)   Pound   Apache   Perl   Memcached   LVS    The Stats      Started building the system in 1999</p><p>3 0.81210101 <a title="70-lsi-3" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>Introduction: Our company offers a web service that is provided to users from several d  i  fferent hosting centers across the globe.       The content and functionality at each of the servers is almost exact  l  y the same, and we could have based them all in a single location.  However, we chose to distribute the servers geographica  l  ly to offer our users the best performance, regardless where they might be.     Up unt  i  l now, the only content on the servers that has had to be synchronized is the server software itse  l  f.  The features and functionality of our service are being updated regularly, so every week or two we push updates out to all the servers at basically the same time.  We use a relat  i  vely manual approach to do the updating, but it works fine.     Sometime soon, however, our synchronization needs are going to get a b  i  t more complex.     In particular, we'll soon start offering a feature at our site that wi  l  l involve a database with content that will change on an a</p><p>4 0.81103575 <a title="70-lsi-4" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI  seized multiple racks of computers  from DigitalOne, these  racks  host websites from many clients that just happened to be in the same racks as whomever they are investigating. Downed sites include Instapaper, Curbed Network, and  Pinboard . With the  density of servers  these days many 1000s of sites could easily have been effected.
 
Sites like Pinboard were victims by association, they did not inhale. This is an association sites have no control over. On a shared hosting service, you have no control over your fellow VM mates. In a cloud or a managed service, you have no control over which racks your servers are in. So like second hand smoke, you get the disease by random association. There's something inherently unfair about that.
 
A  comment by illumin8  shows just how Darth insidious this process can be:
  

A popular method used by hackers is to sign up for a virtual server with a stolen credit card. If t</p><p>5 0.80560851 <a title="70-lsi-5" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>Introduction: We've had a few articles on the  StackOverlflow Architecture  and  Stack Overflow Architecture Update - Now at 95 Million Page Views a Month . Time for another update. This time from a podcast. Every week or so Jeff, Joel and guests sit around and converse. The result is a  podcast . In a recent  podcast  they talked about some of their recent architecture issues, problems, and updates. And since I wrote this article before my vacation, they've also published a new architecture update article:  The Stack Exchange Architecture – 2011 Edition, Episode 1 .
 
My overall impression is they are in a comfortable place, adding new sites, adding new features, making a house a home.
 
Notable for their scale-up architecture, you might expect with their growth that they would slam into a wall. Not so. They've been able to scale-up the power of individual servers by adding more CPU and RAM. SSD has been added in some cases. Even their flagship StackOverflow product runs on a single server. New mac</p><p>6 0.79036379 <a title="70-lsi-6" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>7 0.78999686 <a title="70-lsi-7" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>8 0.78623724 <a title="70-lsi-8" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>9 0.78388757 <a title="70-lsi-9" href="../high_scalability-2009/high_scalability-2009-07-28-37signals_Architecture.html">663 high scalability-2009-07-28-37signals Architecture</a></p>
<p>10 0.78251344 <a title="70-lsi-10" href="../high_scalability-2007/high_scalability-2007-11-17-Can_How_Bees_Solve_their_Load_Balancing_Problems_Help_Build_More_Scalable_Websites%3F.html">158 high scalability-2007-11-17-Can How Bees Solve their Load Balancing Problems Help Build More Scalable Websites?</a></p>
<p>11 0.78114909 <a title="70-lsi-11" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>12 0.78101873 <a title="70-lsi-12" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>13 0.773166 <a title="70-lsi-13" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>14 0.77276123 <a title="70-lsi-14" href="../high_scalability-2010/high_scalability-2010-07-12-Creating_Scalable_Digital_Libraries.html">856 high scalability-2010-07-12-Creating Scalable Digital Libraries</a></p>
<p>15 0.77172673 <a title="70-lsi-15" href="../high_scalability-2011/high_scalability-2011-05-10-Viddler_Architecture_-_7_Million_Embeds_a_Day_and_1500_Req-Sec_Peak__.html">1037 high scalability-2011-05-10-Viddler Architecture - 7 Million Embeds a Day and 1500 Req-Sec Peak  </a></p>
<p>16 0.76953405 <a title="70-lsi-16" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>17 0.76729757 <a title="70-lsi-17" href="../high_scalability-2010/high_scalability-2010-12-29-Pinboard.in_Architecture_-_Pay_to_Play_to_Keep_a_System_Small__.html">965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </a></p>
<p>18 0.76347059 <a title="70-lsi-18" href="../high_scalability-2009/high_scalability-2009-08-05-Stack_Overflow_Architecture.html">671 high scalability-2009-08-05-Stack Overflow Architecture</a></p>
<p>19 0.76284552 <a title="70-lsi-19" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>20 0.76211107 <a title="70-lsi-20" href="../high_scalability-2009/high_scalability-2009-04-16-Serving_250M_quotes-day_at_CNBC.com_with_aiCache.html">573 high scalability-2009-04-16-Serving 250M quotes-day at CNBC.com with aiCache</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.2), (2, 0.197), (10, 0.036), (23, 0.01), (26, 0.02), (40, 0.03), (56, 0.022), (61, 0.125), (62, 0.062), (77, 0.024), (79, 0.123), (85, 0.023), (94, 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9726426 <a title="70-lda-1" href="../high_scalability-2011/high_scalability-2011-05-13-Stuff_The_Internet_Says_On_Scalability_For_May_13%2C_2011.html">1040 high scalability-2011-05-13-Stuff The Internet Says On Scalability For May 13, 2011</a></p>
<p>Introduction: Submitted for your reading pleasure on this beautiful blue sky, birds chirping Friday morning: 
  
 Unlocked Achievements:  Twitter: 900,000 Apps, 600,000 Developers And 13 Billion API Requests Per Day ;  Rackspace Now Has 70,000 Servers  
 Quotable Quotes for the speed of light Alex:            
 
  @LusciousPear  According to everyone I talk to, in aggregate, every single #nosql company is irrelevant, doing great, making tons of money, and broke. 
  XIX  Give a man a GO compiler and he eats for a day. Give a man a C compiler and he feeds upon php/python/jscript/lua/etc for the rest of his life. 
  @movingsideways  This tweet features a robust architecture and high scalability. 
  @serenestudios   Human touch trumps code 'scalability'? People matter more than code - we agree. 
  @SunGardFS  At current growth rates, server farms will consume more energy than all air travel by 2020 
  @marcloney  After the hype of NoSQL, are we just going to end up with a bunch of enterprise-class produ</p><p>2 0.97029394 <a title="70-lda-2" href="../high_scalability-2009/high_scalability-2009-02-05-Product%3A_HAProxy_-_The_Reliable%2C_High_Performance_TCP-HTTP_Load_Balancer.html">509 high scalability-2009-02-05-Product: HAProxy - The Reliable, High Performance TCP-HTTP Load Balancer</a></p>
<p>Introduction: Update:  Load Balancing in Amazon EC2 with HAProxy.  Grig Gheorghiu writes a nice post  on HAProxy functionality and configuration: Emulating virtual servers, Logging, SSL, Load balancing algorithms, Session persistence with cookies, Server health checks, etc.  Adapted From the website:    HAProxy  is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for web sites crawling under very high loads while needing persistence or Layer7 processing. Supporting tens of thousands of connections is clearly realistic with todays hardware. Its mode of operation makes its integration into existing architectures very easy and riskless, while still offering the possibility not to expose fragile web servers to the Net.     
 
 Currently, two major versions are supported : * version 1.1 - maintains critical sites online since 200 The most stable and reliable, has reached years of uptime. Receive</p><p>same-blog 3 0.96987695 <a title="70-lda-3" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>Introduction: Amazingly   TechCrunch   runs their website on one web server and one database server, according to the fascinating survey   What the Webâ&euro;&trade;s most popular sites are running on   by   Pingdom  , a  provider of uptime and response time monitoring.      Early we learned   PlentyOfFish   catches and releases many millions of hits a day on just 1 web server and three database servers.   Google   runs a   Dalek   army full of servers.   YouSendIt  , a company making it easy to send and receive large files, has 24 web servers,  3 database servers, 170 storage servers, and a few miscellaneous servers.   Vimeo  , a video sharing company, has 100 servers for streaming video, 4 web servers, and 2 database servers.   Meebo  , an AJAX based instant messaging company, uses 40 servers to handle messaging, over 40 web servers,  and 10 servers for forums, jabber, testing, and so on.   FeedBurner  , a news feed management company, has 70 web servers, 15 database servers, and 10 miscellaneous servers. Now</p><p>4 0.96821374 <a title="70-lda-4" href="../high_scalability-2011/high_scalability-2011-03-09-Productivity_vs._Control_tradeoffs_in_PaaS.html">1002 high scalability-2011-03-09-Productivity vs. Control tradeoffs in PaaS</a></p>
<p>Introduction: Gartner published recently an interesting paper:  Productivity vs. Control: Cloud Application Platforms Must Split to Win  . (The paper requires registration.)
 
The paper does a pretty good job covering the evolution that is taking place in the PaaS market toward a more open platform and compares between the two main categories: aPaaS (essentially a PaaS running as a service) and CEAP (Cloud Enabled Application Platform) which is the  *P* out of PaaS that gives you the platform to build your own PaaS in private or public cloud.
 
While I was reading through the paper I felt that something continued to bother me with this definition, even though I tend to agree with the overall observation. If I follow the logic of this paper than I have to give away productivity to gain control, hmm…  that’s a hard choice.
 
The issue seem to be with the way we define productivity. Read the full detailes  here</p><p>5 0.96694201 <a title="70-lda-5" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>Introduction: Aditya Agarwal, Director of Engineering at Facebook, gave an excellent  Scale at Facebook  talk that covers their architecture, but the talk is really more about how to scale an organization by preserving the best parts of its culture. The key take home of the talk is: 
  

You can get the code right, you can get the products right, but you need to get the culture right first. If you don't get the culture right then your company won't scale.

  
This leads into the four meta secrets of scaling at Facebook:
  
 Scaling takes Iteration 
 Don't Over Design 
 Choose the right tool for the job, but realize that your choice comes with overhead. 
 Get the culture right. Move Fast - break things. Huge Impact - small teams. Be bold - innovate. 
      Some Background    
  Facebook is big : 400 million active users; users spend an average of 20 minutes a day; 5 billion pieces of content (status updates, comments, likes, photo uploads, video uploads, chat messages, inbox messages, group events, f</p><p>6 0.96661597 <a title="70-lda-6" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>7 0.96641511 <a title="70-lda-7" href="../high_scalability-2007/high_scalability-2007-10-02-Secrets_to_Fotolog%27s_Scaling_Success.html">106 high scalability-2007-10-02-Secrets to Fotolog's Scaling Success</a></p>
<p>8 0.96603447 <a title="70-lda-8" href="../high_scalability-2009/high_scalability-2009-04-21-What_CDN_would_you_recommend%3F.html">576 high scalability-2009-04-21-What CDN would you recommend?</a></p>
<p>9 0.96511441 <a title="70-lda-9" href="../high_scalability-2012/high_scalability-2012-08-10-Stuff_The_Internet_Says_On_Scalability_For_August_10%2C_2012.html">1302 high scalability-2012-08-10-Stuff The Internet Says On Scalability For August 10, 2012</a></p>
<p>10 0.96506077 <a title="70-lda-10" href="../high_scalability-2009/high_scalability-2009-09-22-How_Ravelry_Scales_to_10_Million_Requests_Using_Rails.html">711 high scalability-2009-09-22-How Ravelry Scales to 10 Million Requests Using Rails</a></p>
<p>11 0.9644261 <a title="70-lda-11" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>12 0.96436876 <a title="70-lda-12" href="../high_scalability-2011/high_scalability-2011-08-05-Stuff_The_Internet_Says_On_Scalability_For_August_5%2C_2011.html">1093 high scalability-2011-08-05-Stuff The Internet Says On Scalability For August 5, 2011</a></p>
<p>13 0.96430349 <a title="70-lda-13" href="../high_scalability-2011/high_scalability-2011-04-22-Stuff_The_Internet_Says_On_Scalability_For_April_22%2C_2011.html">1028 high scalability-2011-04-22-Stuff The Internet Says On Scalability For April 22, 2011</a></p>
<p>14 0.96347886 <a title="70-lda-14" href="../high_scalability-2013/high_scalability-2013-12-06-Stuff_The_Internet_Says_On_Scalability_For_December_6th%2C_2013.html">1559 high scalability-2013-12-06-Stuff The Internet Says On Scalability For December 6th, 2013</a></p>
<p>15 0.96339893 <a title="70-lda-15" href="../high_scalability-2010/high_scalability-2010-10-28-Notes_from_A_NOSQL_Evening_in_Palo_Alto_.html">931 high scalability-2010-10-28-Notes from A NOSQL Evening in Palo Alto </a></p>
<p>16 0.9629913 <a title="70-lda-16" href="../high_scalability-2012/high_scalability-2012-07-23-State_of_the_CDN%3A_More_Traffic%2C_Stable_Prices%2C_More_Products%2C_Profits_-_Not_So_Much.html">1289 high scalability-2012-07-23-State of the CDN: More Traffic, Stable Prices, More Products, Profits - Not So Much</a></p>
<p>17 0.96277809 <a title="70-lda-17" href="../high_scalability-2012/high_scalability-2012-02-07-Hypertable_Routs_HBase_in_Performance_Test_--_HBase_Overwhelmed_by_Garbage_Collection.html">1189 high scalability-2012-02-07-Hypertable Routs HBase in Performance Test -- HBase Overwhelmed by Garbage Collection</a></p>
<p>18 0.96276081 <a title="70-lda-18" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>19 0.96180224 <a title="70-lda-19" href="../high_scalability-2013/high_scalability-2013-09-09-Need_Help_with_Database_Scalability%3F_Understand_I-O.html">1514 high scalability-2013-09-09-Need Help with Database Scalability? Understand I-O</a></p>
<p>20 0.96144003 <a title="70-lda-20" href="../high_scalability-2011/high_scalability-2011-05-10-Viddler_Architecture_-_7_Million_Embeds_a_Day_and_1500_Req-Sec_Peak__.html">1037 high scalability-2011-05-10-Viddler Architecture - 7 Million Embeds a Day and 1500 Req-Sec Peak  </a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
