<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 high scalability-2007-07-15-Lustre cluster file system</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-13" href="#">high_scalability-2007-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 high scalability-2007-07-15-Lustre cluster file system</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-13-html" href="http://highscalability.com//blog/2007/7/15/lustre-cluster-file-system.html">html</a></p><p>Introduction: Lustre速  is a scalable, secure, robust, highly-available cluster file system. It is designed, developed and maintained by Cluster File Systems, Inc.  The central goal is the development of a next-generation cluster file system which can serve clusters with 10,000's of nodes, provide petabytes of storage, and move 100's of GB/sec with state-of-the-art security and management infrastructure.  Lustre runs on many of the largest Linux clusters in the world, and is included by CFS's partners as a core component of their cluster offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1 supercomputers). Today's users have also demonstrated that Lustre scales down as well as it scales up, and runs in production on clusters as small as 4 and as large as 25,000 nodes.  The latest version of Lustre is always available from Cluster File Systems, Inc. Public Open Source releases of Lustre are available under the GNU General Public License. These releases are found here, and are used</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Lustre速  is a scalable, secure, robust, highly-available cluster file system. [sent-1, score-0.421]
</p><p>2 It is designed, developed and maintained by Cluster File Systems, Inc. [sent-2, score-0.181]
</p><p>3 The central goal is the development of a next-generation cluster file system which can serve clusters with 10,000's of nodes, provide petabytes of storage, and move 100's of GB/sec with state-of-the-art security and management infrastructure. [sent-3, score-1.213]
</p><p>4 Lustre runs on many of the largest Linux clusters in the world, and is included by CFS's partners as a core component of their cluster offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1 supercomputers). [sent-4, score-1.15]
</p><p>5 Today's users have also demonstrated that Lustre scales down as well as it scales up, and runs in production on clusters as small as 4 and as large as 25,000 nodes. [sent-5, score-0.882]
</p><p>6 The latest version of Lustre is always available from Cluster File Systems, Inc. [sent-6, score-0.257]
</p><p>7 Public Open Source releases of Lustre are available under the GNU General Public License. [sent-7, score-0.322]
</p><p>8 These releases are found here, and are used in production supercomputing environments worldwide. [sent-8, score-0.647]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lustre', 0.611), ('releases', 0.24), ('cluster', 0.232), ('clusters', 0.217), ('cfs', 0.192), ('file', 0.189), ('cray', 0.181), ('gnu', 0.181), ('supercomputing', 0.166), ('public', 0.145), ('supercomputers', 0.139), ('scales', 0.136), ('demonstrated', 0.126), ('runs', 0.125), ('partners', 0.121), ('included', 0.11), ('hp', 0.11), ('maintained', 0.109), ('production', 0.101), ('petabytes', 0.099), ('secure', 0.094), ('environments', 0.089), ('offering', 0.086), ('robust', 0.085), ('central', 0.084), ('available', 0.082), ('component', 0.077), ('examples', 0.077), ('latest', 0.074), ('developed', 0.072), ('serve', 0.071), ('largest', 0.069), ('goal', 0.067), ('security', 0.066), ('general', 0.063), ('include', 0.062), ('version', 0.06), ('linux', 0.059), ('today', 0.056), ('systems', 0.055), ('nodes', 0.053), ('move', 0.052), ('core', 0.051), ('found', 0.051), ('designed', 0.051), ('provide', 0.047), ('development', 0.045), ('management', 0.044), ('small', 0.041), ('always', 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="13-tfidf-1" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>Introduction: Lustre速  is a scalable, secure, robust, highly-available cluster file system. It is designed, developed and maintained by Cluster File Systems, Inc.  The central goal is the development of a next-generation cluster file system which can serve clusters with 10,000's of nodes, provide petabytes of storage, and move 100's of GB/sec with state-of-the-art security and management infrastructure.  Lustre runs on many of the largest Linux clusters in the world, and is included by CFS's partners as a core component of their cluster offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1 supercomputers). Today's users have also demonstrated that Lustre scales down as well as it scales up, and runs in production on clusters as small as 4 and as large as 25,000 nodes.  The latest version of Lustre is always available from Cluster File Systems, Inc. Public Open Source releases of Lustre are available under the GNU General Public License. These releases are found here, and are used</p><p>2 0.27178684 <a title="13-tfidf-2" href="../high_scalability-2008/high_scalability-2008-10-14-Implementing_the_Lustre_File_System_with_Sun_Storage%3A_High_Performance_Storage_for_High_Performance_Computing.html">411 high scalability-2008-10-14-Implementing the Lustre File System with Sun Storage: High Performance Storage for High Performance Computing</a></p>
<p>Introduction: Much of the focus of high performance computing (HPC) has centered on CPU performance. However, as computing requirements grow, HPC clusters are demanding higher rates of aggregate data throughput. Today's clusters feature larger numbers of nodes with increased compute speeds. The higher clock rates and operations per clock cycle create increased demand for local data on each node. In addition, InfiniBand and other high-speed, low-latency interconnects increase the data throughput available to each node.     Traditional shared file systems such as NFS have not been able to scale to meet this growing demand for data throughput on HPC clusters. Scalable cluster file systems that can provide parallel data access to hundreds of nodes and petabytes of storage are needed to provide the high data throughput required by large HPC applications, including manufacturing, electronic design, and research.     This paper describes an implementation of the Sun Lustre file system as a scalable storage</p><p>3 0.23524357 <a title="13-tfidf-3" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>Introduction: Todd had originally posted an entry on  collectl  here at  Collectl - Performance Data Collector . Collectl collects real-time data from a large number of subsystems like buddyinfo, cpu, disk, inodes, infiniband, lustre, memory, network, nfs, processes, quadrics, slabs, sockets and tcp, all using one tool and in one consistent format.
 
Since then a lot has happened.  It's now part of both Fedora and Debian distros, not to mention several others. There has also been a pretty good summary written up by  Joe Brockmeier . It's also pretty well documented (I like to think) on  sourceforge . There have also been a few blog postings by   Martin Bach   on his blog.
 
Anyhow, awhile back I released a new version of collectl-utils and gave a complete face-lift to one of the utilities, colmux, which is a collectl multiplexor.  This tool has the ability to run collectl on multiple systems, which in turn send all their output back to colmux.  Colmux then sorts the output on a user-specified column</p><p>4 0.15688354 <a title="13-tfidf-4" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>Introduction: From their  website :   There are a number of times in which you find yourself needing performance data. These can include benchmarking, monitoring a system's general heath or trying to determine what your system was doing at some time in the past. Sometimes you just want to know what the system is doing right now. Depending on what you're doing, you often end up using different tools, each designed to for that specific situation. Features include: 
 
 
 You are be able to run with non-integral sampling intervals.  
  Collectl  uses very little CPU. In fact it has been measured to use <0.1% when run as a daemon using the default sampling interval of 60 seconds for process and slab data and 10 seconds for everything else. 
 Brief, verbose, and plot formats are supported. 
 You can report aggregated performance numbers on many devices such as CPUs, Disks, interconnects such as Infiniband or Quadrics, Networks or even Lustre file systems. 
 Collectl will align its sampling on integral sec</p><p>5 0.15371439 <a title="13-tfidf-5" href="../high_scalability-2008/high_scalability-2008-10-15-Tokyo_Tech_Tsubame_Grid_Storage_Implementation.html">420 high scalability-2008-10-15-Tokyo Tech Tsubame Grid Storage Implementation</a></p>
<p>Introduction: This Sun BluePrint article describes the storage architecture of the Tokyo Institute of Technology TSUBAME grid. The Tokyo Institute of Technology is of the world's leading technical institutes, and recently created the fastest supercomputer in Asia, and one of the largest supercomputers outside of the United States. By deploying Sun Fire x64 servers and data servers in a grid architecture, Tokyo Tech built a cost-effective and flexible supercomputer consisting of hundreds of systems, thousands of processors, terabytes of memory and a petabyte of storage that supports users running common off-the-shelf applications. This is the second of a three-article series. It describes the steps to install and configuring the Lustre file system within the storage architecture.</p><p>6 0.12025353 <a title="13-tfidf-6" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>7 0.11520797 <a title="13-tfidf-7" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>8 0.11327661 <a title="13-tfidf-8" href="../high_scalability-2007/high_scalability-2007-09-27-Product%3A_Ganglia_Monitoring_System.html">101 high scalability-2007-09-27-Product: Ganglia Monitoring System</a></p>
<p>9 0.098795086 <a title="13-tfidf-9" href="../high_scalability-2011/high_scalability-2011-07-07-Myth%3A_Google_Uses_Server_Farms_So_You_Should_Too_-_Resurrection_of_the_Big-Ass_Machines.html">1075 high scalability-2011-07-07-Myth: Google Uses Server Farms So You Should Too - Resurrection of the Big-Ass Machines</a></p>
<p>10 0.095782928 <a title="13-tfidf-10" href="../high_scalability-2008/high_scalability-2008-02-27-Product%3A_System_Imager_-_Automate_Deployment_and_Installs.html">263 high scalability-2008-02-27-Product: System Imager - Automate Deployment and Installs</a></p>
<p>11 0.084816858 <a title="13-tfidf-11" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>12 0.081122756 <a title="13-tfidf-12" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>13 0.080583476 <a title="13-tfidf-13" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>14 0.080086142 <a title="13-tfidf-14" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>15 0.077832446 <a title="13-tfidf-15" href="../high_scalability-2014/high_scalability-2014-05-02-Stuff_The_Internet_Says_On_Scalability_For_May_2nd%2C_2014.html">1642 high scalability-2014-05-02-Stuff The Internet Says On Scalability For May 2nd, 2014</a></p>
<p>16 0.075421102 <a title="13-tfidf-16" href="../high_scalability-2007/high_scalability-2007-11-20-Product%3A_SmartFrog_a_Distributed_Configuration_and_Deployment_Framework.html">161 high scalability-2007-11-20-Product: SmartFrog a Distributed Configuration and Deployment Framework</a></p>
<p>17 0.07533212 <a title="13-tfidf-17" href="../high_scalability-2008/high_scalability-2008-02-07-clusteradmin.blogspot.com_-_blog_about_building_and_administering_clusters.html">243 high scalability-2008-02-07-clusteradmin.blogspot.com - blog about building and administering clusters</a></p>
<p>18 0.073814407 <a title="13-tfidf-18" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>19 0.072887354 <a title="13-tfidf-19" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>20 0.072632387 <a title="13-tfidf-20" href="../high_scalability-2011/high_scalability-2011-05-17-Facebook%3A_An_Example_Canonical_Architecture_for_Scaling_Billions_of_Messages.html">1042 high scalability-2011-05-17-Facebook: An Example Canonical Architecture for Scaling Billions of Messages</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.097), (1, 0.027), (2, 0.018), (3, 0.007), (4, -0.013), (5, 0.021), (6, 0.073), (7, -0.056), (8, -0.011), (9, 0.096), (10, 0.012), (11, 0.006), (12, 0.076), (13, -0.019), (14, 0.039), (15, 0.045), (16, -0.025), (17, 0.011), (18, -0.058), (19, 0.029), (20, 0.063), (21, -0.007), (22, 0.005), (23, 0.022), (24, -0.047), (25, 0.018), (26, 0.036), (27, -0.023), (28, -0.072), (29, -0.013), (30, -0.031), (31, -0.001), (32, 0.008), (33, -0.009), (34, 0.003), (35, 0.056), (36, -0.024), (37, -0.094), (38, -0.016), (39, 0.02), (40, -0.032), (41, -0.052), (42, -0.025), (43, 0.059), (44, -0.051), (45, 0.148), (46, -0.049), (47, -0.018), (48, -0.002), (49, 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97618699 <a title="13-lsi-1" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>Introduction: Lustre速  is a scalable, secure, robust, highly-available cluster file system. It is designed, developed and maintained by Cluster File Systems, Inc.  The central goal is the development of a next-generation cluster file system which can serve clusters with 10,000's of nodes, provide petabytes of storage, and move 100's of GB/sec with state-of-the-art security and management infrastructure.  Lustre runs on many of the largest Linux clusters in the world, and is included by CFS's partners as a core component of their cluster offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1 supercomputers). Today's users have also demonstrated that Lustre scales down as well as it scales up, and runs in production on clusters as small as 4 and as large as 25,000 nodes.  The latest version of Lustre is always available from Cluster File Systems, Inc. Public Open Source releases of Lustre are available under the GNU General Public License. These releases are found here, and are used</p><p>2 0.71617705 <a title="13-lsi-2" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_FAI_-_Fully_Automatic_Installation.html">272 high scalability-2008-03-08-Product: FAI - Fully Automatic Installation</a></p>
<p>Introduction: From their website:    FAI  is an automated installation tool to install or deploy Debian GNU/Linux and other distributions on a bunch of different hosts or a Cluster. It's more flexible than other tools like kickstart for Red Hat, autoyast and alice for SuSE or Jumpstart for SUN Solaris. FAI can also be used for configuration management of a running system.  You can take one or more virgin PCs, turn on the power and after a few minutes Linux is installed, configured and running on all your machines, without any interaction necessary. FAI it's a scalable method for installing and updating all your computers unattended with little effort involved. It's a centralized management system for your Linux deployment.  
   
  FAI's target group are system administrators who have to install Linux onto one or even hundreds of computers. It's not only a tool for doing a Cluster installation but a general purpose installation tool. It can be used for installing a Beowulf cluster, a rendering farm,</p><p>3 0.68335235 <a title="13-lsi-3" href="../high_scalability-2007/high_scalability-2007-10-07-Product%3A_Wackamole.html">114 high scalability-2007-10-07-Product: Wackamole</a></p>
<p>Introduction: Wackamole  is an application that helps with making a cluster highly available. It manages a bunch of virtual IPs, that should be available to the outside world at all times. Wackamole ensures that a single machine within a cluster is listening on each virtual IP address that Wackamole manages.
 
If it discovers that particular machines within the cluster are not alive, it will almost immediately ensure that other machines acquire these public IPs. At no time will more than one machine listen on any virtual IP. Wackamole also works toward achieving a balanced distribution of number IPs on the machine within the cluster it manages.   There is no other software like Wackamole. Wackamole is quite unique in that it operates in a completely peer-to-peer mode within the cluster. Other products that provide the same high-availability guarantees use a "VIP" method.  Wackamole is an application that runs as root in a cluster to make it highly available. It uses the membership notifications prov</p><p>4 0.6827237 <a title="13-lsi-4" href="../high_scalability-2008/high_scalability-2008-10-14-Implementing_the_Lustre_File_System_with_Sun_Storage%3A_High_Performance_Storage_for_High_Performance_Computing.html">411 high scalability-2008-10-14-Implementing the Lustre File System with Sun Storage: High Performance Storage for High Performance Computing</a></p>
<p>Introduction: Much of the focus of high performance computing (HPC) has centered on CPU performance. However, as computing requirements grow, HPC clusters are demanding higher rates of aggregate data throughput. Today's clusters feature larger numbers of nodes with increased compute speeds. The higher clock rates and operations per clock cycle create increased demand for local data on each node. In addition, InfiniBand and other high-speed, low-latency interconnects increase the data throughput available to each node.     Traditional shared file systems such as NFS have not been able to scale to meet this growing demand for data throughput on HPC clusters. Scalable cluster file systems that can provide parallel data access to hundreds of nodes and petabytes of storage are needed to provide the high data throughput required by large HPC applications, including manufacturing, electronic design, and research.     This paper describes an implementation of the Sun Lustre file system as a scalable storage</p><p>5 0.6714257 <a title="13-lsi-5" href="../high_scalability-2008/high_scalability-2008-02-27-Product%3A_System_Imager_-_Automate_Deployment_and_Installs.html">263 high scalability-2008-02-27-Product: System Imager - Automate Deployment and Installs</a></p>
<p>Introduction: From their website:    SystemImager  is software that makes the installation of Linux to masses of similar machines relatively easy. It makes software distribution, configuration, and operating system updates easy, and can also be used for content distribution.  SystemImager makes it easy to do automated installs (clones), software distribution, content or data distribution, configuration changes, and operating system updates to your network of Linux machines. You can even update from one Linux release version to another!  
   
  It can also be used to ensure safe production deployments. By saving your current production image before updating to your new production image, you have a highly reliable contingency mechanism. If the new production enviroment is found to be flawed, simply roll-back to the last production image with a simple update command!  Some typical environments include: Internet server farms, database server farms, high performance clusters, computer labs, and corporate</p><p>6 0.66486651 <a title="13-lsi-6" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>7 0.62539548 <a title="13-lsi-7" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>8 0.62280202 <a title="13-lsi-8" href="../high_scalability-2007/high_scalability-2007-07-25-Paper%3A_Designing_Disaster_Tolerant_High_Availability_Clusters.html">25 high scalability-2007-07-25-Paper: Designing Disaster Tolerant High Availability Clusters</a></p>
<p>9 0.61467791 <a title="13-lsi-9" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>10 0.57517731 <a title="13-lsi-10" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>11 0.56538022 <a title="13-lsi-11" href="../high_scalability-2008/high_scalability-2008-05-25-Product%3A_Condor__-_Compute_Intensive_Workload_Management.html">326 high scalability-2008-05-25-Product: Condor  - Compute Intensive Workload Management</a></p>
<p>12 0.55981261 <a title="13-lsi-12" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>13 0.55946469 <a title="13-lsi-13" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>14 0.55682707 <a title="13-lsi-14" href="../high_scalability-2007/high_scalability-2007-09-27-Product%3A_Ganglia_Monitoring_System.html">101 high scalability-2007-09-27-Product: Ganglia Monitoring System</a></p>
<p>15 0.554456 <a title="13-lsi-15" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>16 0.55051225 <a title="13-lsi-16" href="../high_scalability-2009/high_scalability-2009-01-08-file_synchronization_solutions.html">488 high scalability-2009-01-08-file synchronization solutions</a></p>
<p>17 0.54548383 <a title="13-lsi-17" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>18 0.54464263 <a title="13-lsi-18" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>19 0.54113179 <a title="13-lsi-19" href="../high_scalability-2008/high_scalability-2008-10-29-CTL_-_Distributed_Control_Dispatching_Framework_.html">433 high scalability-2008-10-29-CTL - Distributed Control Dispatching Framework </a></p>
<p>20 0.54044747 <a title="13-lsi-20" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.12), (2, 0.208), (30, 0.03), (75, 0.271), (79, 0.136), (94, 0.097)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92032146 <a title="13-lda-1" href="../high_scalability-2012/high_scalability-2012-09-11-How_big_is_a_Petabyte%2C_Exabyte%2C_Zettabyte%2C_or_a_Yottabyte%3F.html">1320 high scalability-2012-09-11-How big is a Petabyte, Exabyte, Zettabyte, or a Yottabyte?</a></p>
<p>Introduction: This is an intuitive look at large data sizes By Julian Bunn in  Globally Interconnected Object Databases .
  Bytes(8 bits)   
 0.1 bytes:  A binary decision  
 1 byte:  A single character  
 10 bytes:  A single word  
 100 bytes:  A telegram  OR  A punched card  
   Kilobyte (1000 bytes)   
 1 Kilobyte:  A very short story  
 2 Kilobytes: A Typewritten page 
 10 Kilobytes:  An encyclopaedic page  OR  A deck of punched cards  
 50 Kilobytes: A compressed document image page 
 100 Kilobytes:  A low-resolution photograph  
 200 Kilobytes: A box of punched cards 
 500 Kilobytes: A very heavy box of punched cards 
   Megabyte (1 000 000 bytes)   
 1 Megabyte:  A small novel  OR  A 3.5 inch floppy disk  
 2 Megabytes: A high resolution photograph 
 5 Megabytes:  The complete works of Shakespeare  OR 30 seconds of TV-quality video 
 10 Megabytes: A minute of high-fidelity sound OR A digital chest X-ray 
 20 Megabytes:  A box of floppy disks  
 50 Megabytes: A digital mammogram 
 100 Megabyte</p><p>same-blog 2 0.8585645 <a title="13-lda-2" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>Introduction: Lustre速  is a scalable, secure, robust, highly-available cluster file system. It is designed, developed and maintained by Cluster File Systems, Inc.  The central goal is the development of a next-generation cluster file system which can serve clusters with 10,000's of nodes, provide petabytes of storage, and move 100's of GB/sec with state-of-the-art security and management infrastructure.  Lustre runs on many of the largest Linux clusters in the world, and is included by CFS's partners as a core component of their cluster offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1 supercomputers). Today's users have also demonstrated that Lustre scales down as well as it scales up, and runs in production on clusters as small as 4 and as large as 25,000 nodes.  The latest version of Lustre is always available from Cluster File Systems, Inc. Public Open Source releases of Lustre are available under the GNU General Public License. These releases are found here, and are used</p><p>3 0.83116275 <a title="13-lda-3" href="../high_scalability-2010/high_scalability-2010-07-24-4_New_Podcasts_for_Scalable_Summertime_Reading.html">864 high scalability-2010-07-24-4 New Podcasts for Scalable Summertime Reading</a></p>
<p>Introduction: It's trendy today to say "I don't read blogs anymore, I just  let the  random chance of my social network guide me to new and interesting  content." #fail. While someone says this I imagine them flicking their  hair back in a "I can't be bothered with true understanding" disdain. And  where does random chance get its content? From people like these. So:  support your local blog! 
 
If you would like to be a part of random chance, here are a few new podcasts/blogs/vidcasts that you may not know about and that I've found interesting:
  
  DevOps Cafe . With this  new video series where John and Damon visit high  performing companies and record an insider's tour of the tools and  processes those companies are using to solve their DevOps problems , DevOps is a profession that finally seems to be realizing their own value. In the first episode John Paul Ramirez takes the crew on a tour of Shopzilla's application lifecycle metrics and dashboard. The second episode feature John Allspaw, VP of</p><p>4 0.77617317 <a title="13-lda-4" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>Introduction: The Peregrine falcon is a bird of prey, famous for its  high speed diving   attacks , feeding primarily on much slower Hadoops. Wait, sorry, it is Kevin Burton of Spinn3r's new  Peregrine project -- a  new FAST modern map reduce framework optimized for iterative and pipelined map reduce jobs -- that feeds on Hadoops.
 
If you don't know Kevin, he does a lot of excellent technical work that he's kind enough to share it on  his blog . Only he hasn't been blogging much lately, he's been heads down working on Peregrine. Now that Peregrine has been released, here's a short email interview with Kevin on why you might want to take up  falconry , the ancient sport of MapReduce.
  What does Spinn3r do that Peregrine is important to you?  
Ideally it was designed to execute pagerank but many iterative applications that we deploy and WANT to deploy (k-means) would be horribly inefficient under Hadoop as it doesn't have any support for merging and joining IO between tasks.  It also doesn't support</p><p>5 0.77132303 <a title="13-lda-5" href="../high_scalability-2010/high_scalability-2010-03-09-Sponsored_Post%3A_Job_Openings_-_Squarespace.html">791 high scalability-2010-03-09-Sponsored Post: Job Openings - Squarespace</a></p>
<p>Introduction: Squarespace Looking for Full-time Scaling Expert  
Interested in helping a cutting-edge, high-growth startup scale? Squarespace, which was profiled here last year in  Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month  and also  hosts this blog , is currently in the market for a crack scalability engineer to help build out its cloud infrastructure. Squarespace is very excited about finding a full-time scaling expert.
 
Interested applicants should go to  http://www.squarespace.com/jobs-software-engineer  for more information.
 
ďťż
 
If you would like to advertise your critical, hard to fill job opeinings on HighScalability, please  contact us  and we'll get it setup for you.</p><p>6 0.76378495 <a title="13-lda-6" href="../high_scalability-2013/high_scalability-2013-11-22-Stuff_The_Internet_Says_On_Scalability_For_November_22th%2C_2013.html">1552 high scalability-2013-11-22-Stuff The Internet Says On Scalability For November 22th, 2013</a></p>
<p>7 0.75000226 <a title="13-lda-7" href="../high_scalability-2013/high_scalability-2013-11-07-Paper%3A_Tempest%3A_Scalable_Time-Critical_Web_Services_Platform.html">1544 high scalability-2013-11-07-Paper: Tempest: Scalable Time-Critical Web Services Platform</a></p>
<p>8 0.72831047 <a title="13-lda-8" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<p>9 0.72090948 <a title="13-lda-9" href="../high_scalability-2008/high_scalability-2008-04-30-Rather_small_site_architecture..html">312 high scalability-2008-04-30-Rather small site architecture.</a></p>
<p>10 0.71916789 <a title="13-lda-10" href="../high_scalability-2010/high_scalability-2010-02-23-Sponsored_Post%3A_Job_Openings_-_Squarespace.html">781 high scalability-2010-02-23-Sponsored Post: Job Openings - Squarespace</a></p>
<p>11 0.71576929 <a title="13-lda-11" href="../high_scalability-2012/high_scalability-2012-08-22-Cloud_Deployment%3A_It%E2%80%99s_All_About_Cloud_Automation.html">1309 high scalability-2012-08-22-Cloud Deployment: It’s All About Cloud Automation</a></p>
<p>12 0.71081704 <a title="13-lda-12" href="../high_scalability-2008/high_scalability-2008-04-18-Scaling_Mania_at_MySQL_Conference_2008.html">303 high scalability-2008-04-18-Scaling Mania at MySQL Conference 2008</a></p>
<p>13 0.71068627 <a title="13-lda-13" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>14 0.70882607 <a title="13-lda-14" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>15 0.70517159 <a title="13-lda-15" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<p>16 0.70372903 <a title="13-lda-16" href="../high_scalability-2007/high_scalability-2007-07-16-Blog%3A_MySQL_Performance_Blog_-_Everything_about_MySQL_Performance._.html">15 high scalability-2007-07-16-Blog: MySQL Performance Blog - Everything about MySQL Performance. </a></p>
<p>17 0.70161557 <a title="13-lda-17" href="../high_scalability-2008/high_scalability-2008-03-04-Manage_Downtime_Risk_by_Connecting_Multiple_Data_Centers_into_a_Secure_Virtual_LAN.html">266 high scalability-2008-03-04-Manage Downtime Risk by Connecting Multiple Data Centers into a Secure Virtual LAN</a></p>
<p>18 0.7012912 <a title="13-lda-18" href="../high_scalability-2010/high_scalability-2010-07-22-How_can_we_spark_the_movement_of_research_out_of_the_Ivory_Tower_and_into_production%3F.html">863 high scalability-2010-07-22-How can we spark the movement of research out of the Ivory Tower and into production?</a></p>
<p>19 0.70091128 <a title="13-lda-19" href="../high_scalability-2014/high_scalability-2014-03-14-Stuff_The_Internet_Says_On_Scalability_For_March_14th%2C_2014.html">1612 high scalability-2014-03-14-Stuff The Internet Says On Scalability For March 14th, 2014</a></p>
<p>20 0.70042193 <a title="13-lda-20" href="../high_scalability-2009/high_scalability-2009-06-30-Hot_New_Trend%3A_Linking_Clouds_Through_Cheap_IP_VPNs_Instead_of_Private_Lines_.html">645 high scalability-2009-06-30-Hot New Trend: Linking Clouds Through Cheap IP VPNs Instead of Private Lines </a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
