<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-103" href="#">high_scalability-2007-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-103-html" href="http://highscalability.com//blog/2007/9/28/kosmos-file-system-kfs-is-a-new-high-end-google-file-system.html">html</a></p><p>Introduction: There's a new clustered file system on the spindle:   Kosmos File System (KFS)  .  Thanks to Rich Skrenta for turning me on to KFS and I think his blog   post   says it all.  KFS is an open source project written in C++ by search startup   Kosmix  . The team members have a good   pedigree   so there's a better than average chance this software will be worth considering.     After you stop trying to turn KFS into "Kentucky Fried File System" in your mind,  take a look at KFS' intriguing feature set:           Incremental scalability: New chunkserver nodes can be added as storage needs increase; the system automatically adapts to the new nodes.  Availability: Replication is used to provide availability due to chunk server failures. Typically, files are replicated 3-way.   Per file degree of replication: The degree of replication is configurable on a per file basis, with a max. limit of 64.   Re-replication: Whenever the degree of replication for a file drops below the configured amount (</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There's a new clustered file system on the spindle:   Kosmos File System (KFS)  . [sent-1, score-0.18]
</p><p>2 After you stop trying to turn KFS into "Kentucky Fried File System" in your mind,  take a look at KFS' intriguing feature set:           Incremental scalability: New chunkserver nodes can be added as storage needs increase; the system automatically adapts to the new nodes. [sent-5, score-0.365]
</p><p>3 Availability: Replication is used to provide availability due to chunk server failures. [sent-6, score-0.112]
</p><p>4 Per file degree of replication: The degree of replication is configurable on a per file basis, with a max. [sent-8, score-0.52]
</p><p>5 Re-replication: Whenever the degree of replication for a file drops below the configured amount (such as, due to an extended chunkserver outage), the metaserver forces the block to be re-replicated on the remaining chunk servers. [sent-10, score-0.632]
</p><p>6 Re-balancing: Periodically, the meta-server may rebalance the chunks amongst chunkservers. [sent-12, score-0.116]
</p><p>7 This is done to help with balancing disk space utilization amongst nodes. [sent-13, score-0.114]
</p><p>8 Checksum verification is done on each read; whenever there is a checksum mismatch, re-replication is used to recover the corrupted chunk. [sent-15, score-0.286]
</p><p>9 When an application creates a file, the filename becomes part of the filesystem namespace. [sent-17, score-0.122]
</p><p>10 Periodically, the cache is flushed and data is pushed out to the chunkservers. [sent-19, score-0.132]
</p><p>11 Also, applications can force data to be flushed to the chunkservers. [sent-20, score-0.132]
</p><p>12 In either case, once data is flushed to the server, it is available for reading. [sent-21, score-0.132]
</p><p>13 Leases: KFS client library uses caching to improve performance. [sent-22, score-0.216]
</p><p>14 Client side fail-over: The client library is resilient to chunksever failures. [sent-25, score-0.216]
</p><p>15 During reads, if the client library determines that the chunkserver it is communicating with is unreachable, the client library will fail-over to another chunkserver and continue the read. [sent-26, score-0.886]
</p><p>16 Language support: KFS client library can be accessed from C++, Java, and Python. [sent-28, score-0.216]
</p><p>17 FUSE support on Linux: By mounting KFS via FUSE, this support allows existing linux utilities (such as, ls) to interface with KFS. [sent-29, score-0.227]
</p><p>18 This allows users to navigate the filesystem tree using utilities such as, cp, ls, mkdir, rmdir, rm, mv. [sent-31, score-0.193]
</p><p>19 Deploy scripts: To simplify launching KFS servers, a set of scripts to (1) install KFS binaries on a set of nodes, (2) start/stop KFS servers on a set of nodes are also provided. [sent-33, score-0.319]
</p><p>20 What we are still missing though is a Bigtable like database on top of the file system for scaling structured data. [sent-35, score-0.18]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kfs', 0.814), ('chunkserver', 0.227), ('file', 0.14), ('flushed', 0.132), ('library', 0.116), ('ls', 0.102), ('client', 0.1), ('checksum', 0.095), ('degree', 0.087), ('utilities', 0.082), ('versioning', 0.079), ('amongst', 0.07), ('chunk', 0.07), ('periodically', 0.068), ('replication', 0.066), ('filesystem', 0.065), ('whenever', 0.063), ('tens', 0.059), ('architectureyou', 0.057), ('filename', 0.057), ('kentucky', 0.057), ('rmdir', 0.057), ('set', 0.056), ('blocks', 0.055), ('scripts', 0.055), ('favorably', 0.053), ('fried', 0.053), ('mkdir', 0.053), ('mounting', 0.053), ('rm', 0.053), ('unreachable', 0.053), ('fuse', 0.051), ('adapts', 0.051), ('leases', 0.051), ('binaries', 0.049), ('rich', 0.048), ('cp', 0.047), ('spindle', 0.047), ('nodes', 0.047), ('mismatch', 0.046), ('navigate', 0.046), ('rebalance', 0.046), ('support', 0.046), ('done', 0.044), ('corrupted', 0.042), ('verification', 0.042), ('due', 0.042), ('bag', 0.041), ('system', 0.04), ('overwhelming', 0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="103-tfidf-1" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:   Kosmos File System (KFS)  .  Thanks to Rich Skrenta for turning me on to KFS and I think his blog   post   says it all.  KFS is an open source project written in C++ by search startup   Kosmix  . The team members have a good   pedigree   so there's a better than average chance this software will be worth considering.     After you stop trying to turn KFS into "Kentucky Fried File System" in your mind,  take a look at KFS' intriguing feature set:           Incremental scalability: New chunkserver nodes can be added as storage needs increase; the system automatically adapts to the new nodes.  Availability: Replication is used to provide availability due to chunk server failures. Typically, files are replicated 3-way.   Per file degree of replication: The degree of replication is configurable on a per file basis, with a max. limit of 64.   Re-replication: Whenever the degree of replication for a file drops below the configured amount (</p><p>2 0.091173276 <a title="103-tfidf-2" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:   Sorting 1 PB with MapReduce . PB is not peanut-butter-and-jelly misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.  It took six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000 computers  and the results were replicated thrice on 48,000 disks.  Update:   Greg Linden  points to a new Google article  MapReduce: simplified data processing on large clusters . Some interesting stats: 100k MapReduce jobs are executed each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  Google is the King of scalability.  Everyone knows Google for their large,  sophisticated, and fast searching, but they don't just shine in search. Their platform approach to building scalable applications allows them to roll out internet scale applications at an alarmingly high competition crushing rate. Their goal is always to build</p><p>3 0.088253312 <a title="103-tfidf-3" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranate  is a novel distributed file system built over distributed tabular storage that acts an awful lot like a NoSQL system. It's targeted at increasing the performance of tiny object access in order to support applications like online photo and micro-blog services, which require high concurrency, high throughput, and low latency.   Their tests seem to indicate it works:
  

We have demonstrate that file system over tabular storage performs well for highly concurrent access. In our test cluster, we observed  linearly  increased more than  100,000  aggregate read and write requests served per second (  RPS  ).   

  
Rather than sitting atop the file system like almost every other K-V store, Pomegranate is baked into file system. The idea is that the file system API is common to every platform so it wouldn't require a separate API to use. Every application could use it out of the box.
 
The features of Pomegranate are:
  
 It handles billions of small files efficiently, even in on</p><p>4 0.080895253 <a title="103-tfidf-4" href="../high_scalability-2007/high_scalability-2007-08-28-Google_Utilities_%3A_An_online_google_guide%2Ctools_and_Utilities..html">75 high scalability-2007-08-28-Google Utilities : An online google guide,tools and Utilities.</a></p>
<p>Introduction: An online google guide,tools and Utilities to use with the Google search engine.Free google tools and utilities.</p><p>5 0.072768807 <a title="103-tfidf-5" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>Introduction: This is the third guest post  (  part 1  ,   part 2  ) of a series by Greg Lindahl, CTO of blekko, the spam free search engine. Previously, Greg was Founder and Distinguished Engineer at PathScale, at which he was the architect of the InfiniPath low-latency InfiniBand HCA, used to build tightly-coupled supercomputing clusters. 
 
blekko's home-grown NoSQL database was designed from the start to support a web-scale search engine, with 1,000s of servers and petabytes of disk. Data replication is a very important part of keeping the database up and serving queries.  Like many NoSQL database authors, we decided to keep R=3 copies of each piece of data in the database, and not use RAID to improve reliability. The key goal we were shooting for was a database which degrades gracefully when there are many small failures over time, without needing human intervention.
  Why don't we like RAID for big NoSQL databases?  
Most big storage systems use RAID levels like 3, 4, 5, or 10 to improve relia</p><p>6 0.060893528 <a title="103-tfidf-6" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>7 0.060332693 <a title="103-tfidf-7" href="../high_scalability-2014/high_scalability-2014-03-05-10_Things_You_Should_Know_About_Running_MongoDB_at_Scale.html">1606 high scalability-2014-03-05-10 Things You Should Know About Running MongoDB at Scale</a></p>
<p>8 0.058650773 <a title="103-tfidf-8" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>9 0.058221906 <a title="103-tfidf-9" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>10 0.058218677 <a title="103-tfidf-10" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>11 0.058004726 <a title="103-tfidf-11" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>12 0.057940878 <a title="103-tfidf-12" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>13 0.05579488 <a title="103-tfidf-13" href="../high_scalability-2010/high_scalability-2010-07-12-Creating_Scalable_Digital_Libraries.html">856 high scalability-2010-07-12-Creating Scalable Digital Libraries</a></p>
<p>14 0.055329792 <a title="103-tfidf-14" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>15 0.052822176 <a title="103-tfidf-15" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>16 0.052723732 <a title="103-tfidf-16" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>17 0.05247343 <a title="103-tfidf-17" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>18 0.051912721 <a title="103-tfidf-18" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>19 0.051648214 <a title="103-tfidf-19" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>20 0.051641423 <a title="103-tfidf-20" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.093), (1, 0.044), (2, -0.016), (3, -0.031), (4, -0.0), (5, 0.038), (6, 0.05), (7, -0.018), (8, -0.01), (9, 0.017), (10, 0.01), (11, -0.02), (12, 0.004), (13, -0.032), (14, 0.026), (15, 0.024), (16, -0.025), (17, 0.01), (18, -0.006), (19, -0.005), (20, 0.018), (21, 0.011), (22, 0.006), (23, 0.018), (24, -0.01), (25, -0.006), (26, 0.054), (27, -0.015), (28, -0.06), (29, -0.014), (30, -0.026), (31, -0.026), (32, 0.016), (33, -0.023), (34, -0.015), (35, 0.014), (36, 0.018), (37, 0.014), (38, 0.005), (39, -0.008), (40, -0.001), (41, -0.05), (42, 0.009), (43, 0.025), (44, -0.036), (45, -0.011), (46, -0.025), (47, 0.002), (48, 0.007), (49, 0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96473688 <a title="103-lsi-1" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:   Kosmos File System (KFS)  .  Thanks to Rich Skrenta for turning me on to KFS and I think his blog   post   says it all.  KFS is an open source project written in C++ by search startup   Kosmix  . The team members have a good   pedigree   so there's a better than average chance this software will be worth considering.     After you stop trying to turn KFS into "Kentucky Fried File System" in your mind,  take a look at KFS' intriguing feature set:           Incremental scalability: New chunkserver nodes can be added as storage needs increase; the system automatically adapts to the new nodes.  Availability: Replication is used to provide availability due to chunk server failures. Typically, files are replicated 3-way.   Per file degree of replication: The degree of replication is configurable on a per file basis, with a max. limit of 64.   Re-replication: Whenever the degree of replication for a file drops below the configured amount (</p><p>2 0.87429506 <a title="103-lsi-2" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranate  is a novel distributed file system built over distributed tabular storage that acts an awful lot like a NoSQL system. It's targeted at increasing the performance of tiny object access in order to support applications like online photo and micro-blog services, which require high concurrency, high throughput, and low latency.   Their tests seem to indicate it works:
  

We have demonstrate that file system over tabular storage performs well for highly concurrent access. In our test cluster, we observed  linearly  increased more than  100,000  aggregate read and write requests served per second (  RPS  ).   

  
Rather than sitting atop the file system like almost every other K-V store, Pomegranate is baked into file system. The idea is that the file system API is common to every platform so it wouldn't require a separate API to use. Every application could use it out of the box.
 
The features of Pomegranate are:
  
 It handles billions of small files efficiently, even in on</p><p>3 0.80920839 <a title="103-lsi-3" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:    GlusterFS  is a clustered file-system capable of scaling to several peta-bytes. It aggregates various storage bricks over Infiniband RDMA or TCP/IP interconnect into one large parallel network file system. Storage bricks can be made of any commodity hardware such as x86-64 server with SATA-II RAID and Infiniband HBA).  Cluster file systems are still not mature for enterprise market. They are too complex to deploy and maintain though they are extremely scalable and cheap. Can be entirely built out of commodity OS and hardware. GlusterFS hopes to solves this problem.   GlusterFS achieved  35 GBps read throughput . The GlusterFS Aggregated I/O Benchmark was performed on 64 bricks clustered storage system over 10 Gbps Infiniband interconnect. A cluster of 220 clients pounded the storage system with multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB block size. GlusterFS was configured with unify translator and round-robin scheduler</p><p>4 0.78905773 <a title="103-lsi-4" href="../high_scalability-2007/high_scalability-2007-10-21-Paper%3A_Standardizing_Storage_Clusters_%28with_pNFS%29.html">128 high scalability-2007-10-21-Paper: Standardizing Storage Clusters (with pNFS)</a></p>
<p>Introduction: pNFS (parallel NFS) is the next generation of NFS and its main claim to fame is that it's clustered, which "enables clients to directly access file data spread over multiple storage servers in parallel. As a result, each client can leverage the full aggregate bandwidth of a clustered storage service at the granularity of an individual file." About pNFS  StorageMojo  says:  pNFS is going to commoditize parallel data access. In 5 years we wonâ&euro;&trade;t know how we got along without it . Something to watch.</p><p>5 0.78454423 <a title="103-lsi-5" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>Introduction: Tachyon   ( github ) is interesting new filesystem brought to by the folks at the  UC Berkeley AMP Lab :
     
 
 Tachyon is a fault tolerant distributed ﬁle system enabling reliable file sharing at memory-speed across cluster frameworks, such as Spark and MapReduce.It offers up to 300 times higher throughput than HDFS, by leveraging lineage information and using memory aggressively. Tachyon caches working set files in memory, and enables different jobs/queries and frameworks to access cached files at memory speed. Thus, Tachyon avoids going to disk to load datasets that is frequently read. 
 
 It has a Java-like File API, native support for raw tables, a pluggable file system, and it works with Hadoop with no modifications. 
   
  
 It might work well for streaming media too as you wouldn't have to wait for the complete file to hit the disk before rendering. 
  
  
  Discuss on Hacker News</p><p>6 0.7778939 <a title="103-lsi-6" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>7 0.77565843 <a title="103-lsi-7" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>8 0.77078694 <a title="103-lsi-8" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>9 0.76969928 <a title="103-lsi-9" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>10 0.76749831 <a title="103-lsi-10" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>11 0.76316422 <a title="103-lsi-11" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>12 0.75684428 <a title="103-lsi-12" href="../high_scalability-2007/high_scalability-2007-11-06-Product%3A_ChironFS.html">143 high scalability-2007-11-06-Product: ChironFS</a></p>
<p>13 0.74529612 <a title="103-lsi-13" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>14 0.74477482 <a title="103-lsi-14" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>15 0.72940439 <a title="103-lsi-15" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>16 0.71648294 <a title="103-lsi-16" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>17 0.70470428 <a title="103-lsi-17" href="../high_scalability-2007/high_scalability-2007-08-01-Product%3A_MogileFS.html">53 high scalability-2007-08-01-Product: MogileFS</a></p>
<p>18 0.70186561 <a title="103-lsi-18" href="../high_scalability-2008/high_scalability-2008-02-27-Product%3A_System_Imager_-_Automate_Deployment_and_Installs.html">263 high scalability-2008-02-27-Product: System Imager - Automate Deployment and Installs</a></p>
<p>19 0.69847888 <a title="103-lsi-19" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>20 0.69675243 <a title="103-lsi-20" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.082), (2, 0.221), (10, 0.036), (37, 0.295), (47, 0.019), (61, 0.054), (76, 0.015), (79, 0.122), (85, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91999328 <a title="103-lda-1" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Since  The Big List Of Articles On The Amazon Outage  was published we've a had few updates that people might not have seen. Amazon of course released their  Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region . Netlix shared their  Lessons Learned from the AWS Outage  as did Heroku ( How Heroku Survived the Amazon Outage ), Smug Mug ( How SmugMug survived the Amazonpocalypse ), and SimpleGeo ( How SimpleGeo Stayed Up During the AWS Downtime ). 
 
The curious thing from my perspective is the general lack of response to Amazon's explanation. I expected more discussion. There's been almost none that I've seen. My guess is very few people understand what Amazon was talking about enough to comment whereas almost everyone feels qualified to talk about the event itself.
 
 Lesson for crisis handlers : deep dive post-mortems that are timely, long, honestish, and highly technical are the most effective means of staunching the downward spiral of media attention.</p><p>2 0.89160168 <a title="103-lda-2" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Please see  The Updated Big List Of Articles On The Amazon Outage  for a new improved list. 
 
So many great articles have been written on the Amazon Outage. Some aim at being helpful, some chastise developers for being so stupid, some chastise Amazon for being so incompetent, some talk about the pain they and their companies have experienced, and some even predict the downfall of the cloud. Still others say we have seen a sea change in future of the cloud, a prediction that's hard to disagree with, though the shape of the change remains...cloudy.
 
I'll try to keep this list update as more information comes out. There will be a lot for developers to consider going forward. If there's a resource you think should be added, just let me know.
  Amazon's Explanation of What Happened   
  Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region  
  Hackers News thread on AWS Service Disruption Post Mortem   
  Quite Funny Commentary on the Summary  
   Experiences f</p><p>3 0.86429739 <a title="103-lda-3" href="../high_scalability-2010/high_scalability-2010-09-01-Scale-out_vs_Scale-up.html">891 high scalability-2010-09-01-Scale-out vs Scale-up</a></p>
<p>Introduction: In this post I'll cover the difference between multi-core concurrency that is often referred to as Scale-Up and distributed computing that is often referred to as Scale-Out mode.Â 
 
 more ..</p><p>same-blog 4 0.82146138 <a title="103-lda-4" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:   Kosmos File System (KFS)  .  Thanks to Rich Skrenta for turning me on to KFS and I think his blog   post   says it all.  KFS is an open source project written in C++ by search startup   Kosmix  . The team members have a good   pedigree   so there's a better than average chance this software will be worth considering.     After you stop trying to turn KFS into "Kentucky Fried File System" in your mind,  take a look at KFS' intriguing feature set:           Incremental scalability: New chunkserver nodes can be added as storage needs increase; the system automatically adapts to the new nodes.  Availability: Replication is used to provide availability due to chunk server failures. Typically, files are replicated 3-way.   Per file degree of replication: The degree of replication is configurable on a per file basis, with a max. limit of 64.   Re-replication: Whenever the degree of replication for a file drops below the configured amount (</p><p>5 0.80591977 <a title="103-lda-5" href="../high_scalability-2012/high_scalability-2012-12-31-Designing_for_Resiliency_will_be_so_2013.html">1379 high scalability-2012-12-31-Designing for Resiliency will be so 2013</a></p>
<p>Introduction: A big part of engineering for a quality experience is bringing in the  long tail . An improbable severe failure can ruin your experience of a site, even if your average experience is quite good. That's where building for  resilience  comes in. Resiliency used to be outside the realm of possibility for the common system. It was simply too complex and too expensive.
 
An evolution has been underway, making 2013 possibly the first time resiliency is truly on the table as a standard part of system architectures. We are getting the clouds, we are getting the tools, and prices are almost low enough.
 
Even Netflix,  real leaders  in the resiliency architecture game, took some heat for relying completely on Amazon's ELB and not having a backup load balancing system, leading to a prolonged  Christmas Eve failure . Adrian Cockcroft, Cloud Architect at Netflix,  said they've investigated  creating their own load balancing service, but that "we try not to invest in undifferentiated heavy lifting.</p><p>6 0.80487871 <a title="103-lda-6" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>7 0.80204576 <a title="103-lda-7" href="../high_scalability-2010/high_scalability-2010-12-29-Pinboard.in_Architecture_-_Pay_to_Play_to_Keep_a_System_Small__.html">965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </a></p>
<p>8 0.79678917 <a title="103-lda-8" href="../high_scalability-2008/high_scalability-2008-04-29-Strategy%3A_Sample_to_Reduce_Data_Set.html">311 high scalability-2008-04-29-Strategy: Sample to Reduce Data Set</a></p>
<p>9 0.77912492 <a title="103-lda-9" href="../high_scalability-2007/high_scalability-2007-10-07-Paper%3A_Architecture_of_a_Highly_Scalable_NIO-Based_Server.html">113 high scalability-2007-10-07-Paper: Architecture of a Highly Scalable NIO-Based Server</a></p>
<p>10 0.77337265 <a title="103-lda-10" href="../high_scalability-2013/high_scalability-2013-03-04-7_Life_Saving_Scalability_Defenses_Against_Load_Monster_Attacks.html">1415 high scalability-2013-03-04-7 Life Saving Scalability Defenses Against Load Monster Attacks</a></p>
<p>11 0.75194001 <a title="103-lda-11" href="../high_scalability-2009/high_scalability-2009-06-05-SSL_RPC_API_Scalability.html">620 high scalability-2009-06-05-SSL RPC API Scalability</a></p>
<p>12 0.74192971 <a title="103-lda-12" href="../high_scalability-2008/high_scalability-2008-05-03-Product%3A_nginx.html">314 high scalability-2008-05-03-Product: nginx</a></p>
<p>13 0.74025476 <a title="103-lda-13" href="../high_scalability-2012/high_scalability-2012-12-03-Resiliency_is_the_New_Normal_-_A_Deep_Look_at_What_It_Means_and_How_to_Build_It.html">1366 high scalability-2012-12-03-Resiliency is the New Normal - A Deep Look at What It Means and How to Build It</a></p>
<p>14 0.71794778 <a title="103-lda-14" href="../high_scalability-2013/high_scalability-2013-04-23-Facebook_Secrets_of_Web_Performance.html">1444 high scalability-2013-04-23-Facebook Secrets of Web Performance</a></p>
<p>15 0.71070838 <a title="103-lda-15" href="../high_scalability-2011/high_scalability-2011-08-29-The_Three_Ages_of_Google_-_Batch%2C_Warehouse%2C_Instant.html">1107 high scalability-2011-08-29-The Three Ages of Google - Batch, Warehouse, Instant</a></p>
<p>16 0.70785493 <a title="103-lda-16" href="../high_scalability-2013/high_scalability-2013-03-06-Low_Level_Scalability_Solutions_-_The_Aggregation_Collection.html">1418 high scalability-2013-03-06-Low Level Scalability Solutions - The Aggregation Collection</a></p>
<p>17 0.70344067 <a title="103-lda-17" href="../high_scalability-2008/high_scalability-2008-03-17-Microsoft%27s_New_Database_Cloud_Ready_to_Rumble_with_Amazon.html">279 high scalability-2008-03-17-Microsoft's New Database Cloud Ready to Rumble with Amazon</a></p>
<p>18 0.6842351 <a title="103-lda-18" href="../high_scalability-2008/high_scalability-2008-12-01-Deploying_MySQL_Database_in_Solaris_Cluster_Environments.html">454 high scalability-2008-12-01-Deploying MySQL Database in Solaris Cluster Environments</a></p>
<p>19 0.68064451 <a title="103-lda-19" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>20 0.67750466 <a title="103-lda-20" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
