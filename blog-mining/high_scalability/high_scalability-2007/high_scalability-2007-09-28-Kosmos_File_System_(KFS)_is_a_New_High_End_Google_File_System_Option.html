<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-103" href="#">high_scalability-2007-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-103-html" href="http://highscalability.com//blog/2007/9/28/kosmos-file-system-kfs-is-a-new-high-end-google-file-system.html">html</a></p><p>Introduction: There's a new clustered file system on the spindle:Kosmos File System (KFS).
Thanks to Rich Skrenta for turning me on to KFS and I think his blogpostsays
it all. KFS is an open source project written in C++ by search startupKosmix.
The team members have a goodpedigreeso there's a better than average chance
this software will be worth considering.After you stop trying to turn KFS into
"Kentucky Fried File System" in your mind, take a look at KFS' intriguing
feature set:breakIncremental scalability: New chunkserver nodes can be added
as storage needs increase; the system automatically adapts to the new
nodes.Availability: Replication is used to provide availability due to chunk
server failures. Typically, files are replicated 3-way.Per file degree of
replication: The degree of replication is configurable on a per file basis,
with a max. limit of 64.Re-replication: Whenever the degree of replication for
a file drops below the configured amount (such as, due to an extended
chunkserver outa</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kfs', 0.814), ('chunkserver', 0.227), ('file', 0.14), ('flushed', 0.132), ('library', 0.116), ('ls', 0.102), ('client', 0.1), ('checksum', 0.095), ('degree', 0.087), ('utilities', 0.082), ('versioning', 0.079), ('amongst', 0.07), ('chunk', 0.07), ('periodically', 0.068), ('replication', 0.066), ('filesystem', 0.065), ('whenever', 0.063), ('tens', 0.059), ('architectureyou', 0.057), ('filename', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="103-tfidf-1" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:Kosmos File System (KFS).
Thanks to Rich Skrenta for turning me on to KFS and I think his blogpostsays
it all. KFS is an open source project written in C++ by search startupKosmix.
The team members have a goodpedigreeso there's a better than average chance
this software will be worth considering.After you stop trying to turn KFS into
"Kentucky Fried File System" in your mind, take a look at KFS' intriguing
feature set:breakIncremental scalability: New chunkserver nodes can be added
as storage needs increase; the system automatically adapts to the new
nodes.Availability: Replication is used to provide availability due to chunk
server failures. Typically, files are replicated 3-way.Per file degree of
replication: The degree of replication is configurable on a per file basis,
with a max. limit of 64.Re-replication: Whenever the degree of replication for
a file drops below the configured amount (such as, due to an extended
chunkserver outa</p><p>2 0.091173276 <a title="103-tfidf-2" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:Sorting 1 PB with MapReduce. PB is not peanut-butter-and-jelly
misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.It took
six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000
computersand the results were replicated thrice on 48,000 disks.Update:Greg
Lindenpoints to a new Google articleMapReduce: simplified data processing on
large clusters. Some interesting stats: 100k MapReduce jobs are executed each
day; more than 20 petabytes of data are processed per day; more than 10k
MapReduce programs have been implemented; machines are dual processor with
gigabit ethernet and 4-8 GB of memory.Google is the King of scalability.
Everyone knows Google for their large, sophisticated, and fast searching, but
they don't just shine in search. Their platform approach to building scalable
applications allows them to roll out internet scale applications at an
alarmingly high competition crushing rate. Their goal is always to build a
higher performing h</p><p>3 0.088253312 <a title="103-tfidf-3" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>4 0.080895253 <a title="103-tfidf-4" href="../high_scalability-2007/high_scalability-2007-08-28-Google_Utilities_%3A_An_online_google_guide%2Ctools_and_Utilities..html">75 high scalability-2007-08-28-Google Utilities : An online google guide,tools and Utilities.</a></p>
<p>Introduction: An online google guide,tools and Utilities to use with the Google search
engine.Free google tools and utilities.</p><p>5 0.072768807 <a title="103-tfidf-5" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>Introduction: This is the third guest post (part 1,part 2) of a series by Greg Lindahl, CTO
of blekko, the spam free search engine. Previously, Greg was Founder and
Distinguished Engineer at PathScale, at which he was the architect of the
InfiniPath low-latency InfiniBand HCA, used to build tightly-coupled
supercomputing clusters.blekko's home-grown NoSQL database was designed from
the start to support a web-scale search engine, with 1,000s of servers and
petabytes of disk. Data replication is a very important part of keeping the
database up and serving queries. Like many NoSQL database authors, we decided
to keep R=3 copies of each piece of data in the database, and not use RAID to
improve reliability. The key goal we were shooting for was a database which
degrades gracefully when there are many small failures over time, without
needing human intervention.Why don't we like RAID for big NoSQL databases?Most
big storage systems use RAID levels like 3, 4, 5, or 10 to improve
reliability. A conservativ</p><p>6 0.060893528 <a title="103-tfidf-6" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>7 0.060332693 <a title="103-tfidf-7" href="../high_scalability-2014/high_scalability-2014-03-05-10_Things_You_Should_Know_About_Running_MongoDB_at_Scale.html">1606 high scalability-2014-03-05-10 Things You Should Know About Running MongoDB at Scale</a></p>
<p>8 0.058650773 <a title="103-tfidf-8" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>9 0.058221906 <a title="103-tfidf-9" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>10 0.058218677 <a title="103-tfidf-10" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>11 0.058004726 <a title="103-tfidf-11" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>12 0.057940878 <a title="103-tfidf-12" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>13 0.05579488 <a title="103-tfidf-13" href="../high_scalability-2010/high_scalability-2010-07-12-Creating_Scalable_Digital_Libraries.html">856 high scalability-2010-07-12-Creating Scalable Digital Libraries</a></p>
<p>14 0.055329792 <a title="103-tfidf-14" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>15 0.052822176 <a title="103-tfidf-15" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>16 0.052723732 <a title="103-tfidf-16" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>17 0.05247343 <a title="103-tfidf-17" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>18 0.051912721 <a title="103-tfidf-18" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>19 0.051648214 <a title="103-tfidf-19" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>20 0.051641423 <a title="103-tfidf-20" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.093), (1, 0.044), (2, -0.016), (3, -0.031), (4, -0.0), (5, 0.038), (6, 0.05), (7, -0.018), (8, -0.01), (9, 0.017), (10, 0.01), (11, -0.02), (12, 0.004), (13, -0.032), (14, 0.026), (15, 0.024), (16, -0.025), (17, 0.01), (18, -0.006), (19, -0.005), (20, 0.018), (21, 0.011), (22, 0.006), (23, 0.018), (24, -0.01), (25, -0.006), (26, 0.054), (27, -0.015), (28, -0.06), (29, -0.014), (30, -0.026), (31, -0.026), (32, 0.016), (33, -0.023), (34, -0.015), (35, 0.014), (36, 0.018), (37, 0.014), (38, 0.005), (39, -0.008), (40, -0.001), (41, -0.05), (42, 0.009), (43, 0.025), (44, -0.036), (45, -0.011), (46, -0.025), (47, 0.002), (48, 0.007), (49, 0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96473688 <a title="103-lsi-1" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:Kosmos File System (KFS).
Thanks to Rich Skrenta for turning me on to KFS and I think his blogpostsays
it all. KFS is an open source project written in C++ by search startupKosmix.
The team members have a goodpedigreeso there's a better than average chance
this software will be worth considering.After you stop trying to turn KFS into
"Kentucky Fried File System" in your mind, take a look at KFS' intriguing
feature set:breakIncremental scalability: New chunkserver nodes can be added
as storage needs increase; the system automatically adapts to the new
nodes.Availability: Replication is used to provide availability due to chunk
server failures. Typically, files are replicated 3-way.Per file degree of
replication: The degree of replication is configurable on a per file basis,
with a max. limit of 64.Re-replication: Whenever the degree of replication for
a file drops below the configured amount (such as, due to an extended
chunkserver outa</p><p>2 0.87429506 <a title="103-lsi-2" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>3 0.80920839 <a title="103-lsi-3" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:GlusterFSis a clustered file-system capable of
scaling to several peta-bytes. It aggregates various storage bricks over
Infiniband RDMA or TCP/IP interconnect into one large parallel network file
system. Storage bricks can be made of any commodity hardware such as x86-64
server with SATA-II RAID and Infiniband HBA).Cluster file systems are still
not mature for enterprise market. They are too complex to deploy and maintain
though they are extremely scalable and cheap. Can be entirely built out of
commodity OS and hardware. GlusterFS hopes to solves this problem.GlusterFS
achieved35 GBps read throughput. The GlusterFS Aggregated I/O Benchmark was
performed on 64 bricks clustered storage system over 10 Gbps Infiniband
interconnect. A cluster of 220 clients pounded the storage system with
multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB
block size. GlusterFS was configured with unify translator and round-robin
scheduler.The advantage</p><p>4 0.78905773 <a title="103-lsi-4" href="../high_scalability-2007/high_scalability-2007-10-21-Paper%3A_Standardizing_Storage_Clusters_%28with_pNFS%29.html">128 high scalability-2007-10-21-Paper: Standardizing Storage Clusters (with pNFS)</a></p>
<p>Introduction: pNFS (parallel NFS) is the next generation of NFS and its main claim to fame
is that it's clustered, which "enables clients to directly access file data
spread over multiple storage servers in parallel. As a result, each client can
leverage the full aggregate bandwidth of a clustered storage service at the
granularity of an individual file." About pNFSStorageMojosays:pNFS is going to
commoditize parallel data access. In 5 years we won't know how we got along
without it. Something to watch.</p><p>5 0.78454423 <a title="103-lsi-5" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>Introduction: Tachyon  (github) is interesting new filesystem brought to by the folks at
theUC Berkeley AMP Lab:Tachyon is a fault tolerant distributed ďŹ le system
enabling reliable file sharing at memory-speed across cluster frameworks, such
as Spark and MapReduce.It offers up to 300 times higher throughput than HDFS,
by leveraging lineage information and using memory aggressively. Tachyon
caches working set files in memory, and enables different jobs/queries and
frameworks to access cached files at memory speed. Thus, Tachyon avoids going
to disk to load datasets that is frequently read.It has a Java-like File API,
native support for raw tables, a pluggable file system, and it works with
Hadoop with no modifications. It might work well for streaming media too as
you wouldn't have to wait for the complete file to hit the disk before
rendering.Discuss on Hacker News</p><p>6 0.7778939 <a title="103-lsi-6" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>7 0.77565843 <a title="103-lsi-7" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>8 0.77078694 <a title="103-lsi-8" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>9 0.76969928 <a title="103-lsi-9" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>10 0.76749831 <a title="103-lsi-10" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>11 0.76316422 <a title="103-lsi-11" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>12 0.75684428 <a title="103-lsi-12" href="../high_scalability-2007/high_scalability-2007-11-06-Product%3A_ChironFS.html">143 high scalability-2007-11-06-Product: ChironFS</a></p>
<p>13 0.74529612 <a title="103-lsi-13" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>14 0.74477482 <a title="103-lsi-14" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>15 0.72940439 <a title="103-lsi-15" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>16 0.71648294 <a title="103-lsi-16" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>17 0.70470428 <a title="103-lsi-17" href="../high_scalability-2007/high_scalability-2007-08-01-Product%3A_MogileFS.html">53 high scalability-2007-08-01-Product: MogileFS</a></p>
<p>18 0.70186561 <a title="103-lsi-18" href="../high_scalability-2008/high_scalability-2008-02-27-Product%3A_System_Imager_-_Automate_Deployment_and_Installs.html">263 high scalability-2008-02-27-Product: System Imager - Automate Deployment and Installs</a></p>
<p>19 0.69847888 <a title="103-lsi-19" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>20 0.69675243 <a title="103-lsi-20" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.082), (2, 0.221), (10, 0.036), (37, 0.295), (47, 0.019), (61, 0.054), (76, 0.015), (79, 0.122), (85, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91999328 <a title="103-lda-1" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: SinceThe Big List Of Articles On The Amazon Outage was published we've a had
few updates that people might not have seen. Amazon of course released their
Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East
Region. Netlix shared theirLessons Learned from the AWS Outage as did Heroku
(How Heroku Survived the Amazon Outage), Smug Mug (How SmugMug survived the
Amazonpocalypse), and SimpleGeo (How SimpleGeo Stayed Up During the AWS
Downtime). The curious thing from my perspective is the general lack of
response to Amazon's explanation. I expected more discussion. There's been
almost none that I've seen. My guess is very few people understand what Amazon
was talking about enough to comment whereas almost everyone feels qualified to
talk about the event itself.Lesson for crisis handlers: deep dive post-mortems
that are timely, long, honestish, and highly technical are the most effective
means of staunching the downward spiral of media attention. Amazon's
Explanation of</p><p>2 0.89160168 <a title="103-lda-2" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Please seeThe Updated Big List Of Articles On The Amazon Outage for a new
improved list.So many great articles have been written on the Amazon Outage.
Some aim at being helpful, some chastise developers for being so stupid, some
chastise Amazon for being so incompetent, some talk about the pain they and
their companies have experienced, and some even predict the downfall of the
cloud. Still others say we have seen a sea change in future of the cloud, a
prediction that's hard to disagree with, though the shape of the change
remains...cloudy.I'll try to keep this list update as more information comes
out. There will be a lot for developers to consider going forward. If there's
a resource you think should be added, just let me know.Amazon's Explanation of
What HappenedSummary of the Amazon EC2 and Amazon RDS Service Disruption in
the US East RegionHackers News thread on AWS Service Disruption Post Mortem
Quite Funny Commentary on the SummaryExperiences from Specific Companies, Both
Good a</p><p>3 0.86429739 <a title="103-lda-3" href="../high_scalability-2010/high_scalability-2010-09-01-Scale-out_vs_Scale-up.html">891 high scalability-2010-09-01-Scale-out vs Scale-up</a></p>
<p>Introduction: In this post I'll cover the difference between multi-core concurrency that is
often referred to as Scale-Up and distributed computing that is often referred
to as Scale-Out mode. more..</p><p>same-blog 4 0.82146138 <a title="103-lda-4" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:Kosmos File System (KFS).
Thanks to Rich Skrenta for turning me on to KFS and I think his blogpostsays
it all. KFS is an open source project written in C++ by search startupKosmix.
The team members have a goodpedigreeso there's a better than average chance
this software will be worth considering.After you stop trying to turn KFS into
"Kentucky Fried File System" in your mind, take a look at KFS' intriguing
feature set:breakIncremental scalability: New chunkserver nodes can be added
as storage needs increase; the system automatically adapts to the new
nodes.Availability: Replication is used to provide availability due to chunk
server failures. Typically, files are replicated 3-way.Per file degree of
replication: The degree of replication is configurable on a per file basis,
with a max. limit of 64.Re-replication: Whenever the degree of replication for
a file drops below the configured amount (such as, due to an extended
chunkserver outa</p><p>5 0.80591977 <a title="103-lda-5" href="../high_scalability-2012/high_scalability-2012-12-31-Designing_for_Resiliency_will_be_so_2013.html">1379 high scalability-2012-12-31-Designing for Resiliency will be so 2013</a></p>
<p>Introduction: A big part of engineering for a quality experience is bringing in thelong
tail. An improbable severe failure can ruin your experience of a site, even if
your average experience is quite good. That's where building for resilience
comes in. Resiliency used to be outside the realm of possibility for the
common system. It was simply too complex and too expensive.An evolution has
been underway, making 2013 possibly the first time resiliency is truly on the
table as a standard part of system architectures. We are getting the clouds,
we are getting the tools, and prices are almost low enough.Even Netflix,real
leadersin the resiliency architecture game, took some heat for relying
completely on Amazon's ELB and not having a backup load balancing system,
leading to a prolonged Christmas Eve failure. Adrian Cockcroft, Cloud
Architect at Netflix, said they've investigated creating their own load
balancing service, but that "we try not to invest in undifferentiated heavy
lifting."So resiliency is s</p><p>6 0.80487871 <a title="103-lda-6" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>7 0.80204576 <a title="103-lda-7" href="../high_scalability-2010/high_scalability-2010-12-29-Pinboard.in_Architecture_-_Pay_to_Play_to_Keep_a_System_Small__.html">965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </a></p>
<p>8 0.79678917 <a title="103-lda-8" href="../high_scalability-2008/high_scalability-2008-04-29-Strategy%3A_Sample_to_Reduce_Data_Set.html">311 high scalability-2008-04-29-Strategy: Sample to Reduce Data Set</a></p>
<p>9 0.77912492 <a title="103-lda-9" href="../high_scalability-2007/high_scalability-2007-10-07-Paper%3A_Architecture_of_a_Highly_Scalable_NIO-Based_Server.html">113 high scalability-2007-10-07-Paper: Architecture of a Highly Scalable NIO-Based Server</a></p>
<p>10 0.77337265 <a title="103-lda-10" href="../high_scalability-2013/high_scalability-2013-03-04-7_Life_Saving_Scalability_Defenses_Against_Load_Monster_Attacks.html">1415 high scalability-2013-03-04-7 Life Saving Scalability Defenses Against Load Monster Attacks</a></p>
<p>11 0.75194001 <a title="103-lda-11" href="../high_scalability-2009/high_scalability-2009-06-05-SSL_RPC_API_Scalability.html">620 high scalability-2009-06-05-SSL RPC API Scalability</a></p>
<p>12 0.74192971 <a title="103-lda-12" href="../high_scalability-2008/high_scalability-2008-05-03-Product%3A_nginx.html">314 high scalability-2008-05-03-Product: nginx</a></p>
<p>13 0.74025476 <a title="103-lda-13" href="../high_scalability-2012/high_scalability-2012-12-03-Resiliency_is_the_New_Normal_-_A_Deep_Look_at_What_It_Means_and_How_to_Build_It.html">1366 high scalability-2012-12-03-Resiliency is the New Normal - A Deep Look at What It Means and How to Build It</a></p>
<p>14 0.71794778 <a title="103-lda-14" href="../high_scalability-2013/high_scalability-2013-04-23-Facebook_Secrets_of_Web_Performance.html">1444 high scalability-2013-04-23-Facebook Secrets of Web Performance</a></p>
<p>15 0.71070838 <a title="103-lda-15" href="../high_scalability-2011/high_scalability-2011-08-29-The_Three_Ages_of_Google_-_Batch%2C_Warehouse%2C_Instant.html">1107 high scalability-2011-08-29-The Three Ages of Google - Batch, Warehouse, Instant</a></p>
<p>16 0.70785493 <a title="103-lda-16" href="../high_scalability-2013/high_scalability-2013-03-06-Low_Level_Scalability_Solutions_-_The_Aggregation_Collection.html">1418 high scalability-2013-03-06-Low Level Scalability Solutions - The Aggregation Collection</a></p>
<p>17 0.70344067 <a title="103-lda-17" href="../high_scalability-2008/high_scalability-2008-03-17-Microsoft%27s_New_Database_Cloud_Ready_to_Rumble_with_Amazon.html">279 high scalability-2008-03-17-Microsoft's New Database Cloud Ready to Rumble with Amazon</a></p>
<p>18 0.6842351 <a title="103-lda-18" href="../high_scalability-2008/high_scalability-2008-12-01-Deploying_MySQL_Database_in_Solaris_Cluster_Environments.html">454 high scalability-2008-12-01-Deploying MySQL Database in Solaris Cluster Environments</a></p>
<p>19 0.68064451 <a title="103-lda-19" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>20 0.67750466 <a title="103-lda-20" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
