<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-112" href="#">high_scalability-2007-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-112-html" href="http://highscalability.com//blog/2007/10/4/you-can-now-store-all-your-stuff-on-your-own-google-like-fil.html">html</a></p><p>Introduction: Update :   Parascale’s CTO on what’s different about Parascale .   Let's say you have gigglebytes of data to store and you aren't sure you want to use a  CDN . Amazon's  S3  doesn't excite you. And you aren't quite ready to join the  grid  nation. You want to keep it all in house. Wouldn't it be nice to have something like the  Google File System  you could use to create a unified file system out of all your disks sitting on all your nodes?
 
According to Robin Harris, a.k.a  StorageMojo  (a great blog BTW), you can now have your own GFS:  Parascale launches Google-like storage software .  Parascale  calls their softwate a Virtual Storage Network (VSN). It "aggregates disks across commodity Linux x86 servers to deliver petabyte-scale file storage. With features such as automated, transparent file replication and file migration, Parascale eliminates storage hotspots and delivers massive read/write bandwidth." Why should you care?  I don't know about you, but the "storage problem" is one</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Wouldn't it be nice to have something like the  Google File System  you could use to create a unified file system out of all your disks sitting on all your nodes? [sent-6, score-0.519]
</p><p>2 a  StorageMojo  (a great blog BTW), you can now have your own GFS:  Parascale launches Google-like storage software . [sent-9, score-0.209]
</p><p>3 It "aggregates disks across commodity Linux x86 servers to deliver petabyte-scale file storage. [sent-11, score-0.461]
</p><p>4 With features such as automated, transparent file replication and file migration, Parascale eliminates storage hotspots and delivers massive read/write bandwidth. [sent-12, score-0.571]
</p><p>5 So I like the simplicity of buying commodity nodes with just a bunch of disks attached. [sent-27, score-0.292]
</p><p>6 But the question has always been how do you turn all those disks into a unified storage system without writing a ton of software on top? [sent-28, score-0.56]
</p><p>7 NAS appliances rely on costly low-volume boxes that are closed and don't scale. [sent-30, score-0.177]
</p><p>8 GFS has been deployed in production clusters of over 5,000 servers, proving the scalability of the architecture. [sent-31, score-0.082]
</p><p>9 Fast, reliable, low-cost and massively scalable storage powers the growth of new applications like Web 2. [sent-32, score-0.145]
</p><p>10 Parascale is the first of a new generation of software-only storage solutions. [sent-34, score-0.145]
</p><p>11 Harris says why this is a good thing:     I like software-based systems because hardware is a commodity. [sent-36, score-0.151]
</p><p>12 When you create custom hardware you also create low-volume, high-cost components whose economics go from bad to worse. [sent-37, score-0.207]
</p><p>13 But data is getting cooler and the requirement for specialized high-performance hardware is shrinking relative to the market. [sent-39, score-0.25]
</p><p>14 Appliances can add a lot of value, but they are also a way of monetizing you. [sent-41, score-0.088]
</p><p>15 A software system on commodity hardware has the potential to give good value. [sent-42, score-0.273]
</p><p>16 You are leasing the software per year, per disk spindle. [sent-46, score-0.156]
</p><p>17 I sounds like it could be horribly expensive or really reasonable. [sent-49, score-0.088]
</p><p>18 Another thing that bothers me is that you can't run a database on top of their file system. [sent-51, score-0.257]
</p><p>19 This means I need an entire separate storage system for my database. [sent-52, score-0.145]
</p><p>20 Related Articles     LiveJournal  created an open source distributed file system called MogileFS that builders may find useful. [sent-55, score-0.24]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parascale', 0.54), ('harris', 0.224), ('vsn', 0.216), ('nas', 0.216), ('gfs', 0.206), ('file', 0.169), ('disks', 0.168), ('storage', 0.145), ('commodity', 0.124), ('unified', 0.121), ('appliances', 0.118), ('systemyou', 0.098), ('leasing', 0.092), ('btw', 0.092), ('iscsi', 0.092), ('hotspots', 0.088), ('bothers', 0.088), ('horribly', 0.088), ('monetizing', 0.088), ('hardware', 0.085), ('shrinking', 0.085), ('proving', 0.082), ('san', 0.081), ('mogilefs', 0.08), ('sas', 0.08), ('cooler', 0.08), ('definite', 0.08), ('thegoogle', 0.08), ('scsi', 0.076), ('defense', 0.076), ('aggregates', 0.075), ('conflicting', 0.072), ('builders', 0.071), ('alpha', 0.071), ('frustrating', 0.07), ('opinions', 0.07), ('announces', 0.067), ('says', 0.066), ('pricing', 0.066), ('appliance', 0.065), ('sata', 0.065), ('software', 0.064), ('robin', 0.062), ('confusing', 0.062), ('beta', 0.062), ('ton', 0.062), ('resolution', 0.061), ('create', 0.061), ('mess', 0.06), ('costly', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="112-tfidf-1" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>Introduction: Update :   Parascale’s CTO on what’s different about Parascale .   Let's say you have gigglebytes of data to store and you aren't sure you want to use a  CDN . Amazon's  S3  doesn't excite you. And you aren't quite ready to join the  grid  nation. You want to keep it all in house. Wouldn't it be nice to have something like the  Google File System  you could use to create a unified file system out of all your disks sitting on all your nodes?
 
According to Robin Harris, a.k.a  StorageMojo  (a great blog BTW), you can now have your own GFS:  Parascale launches Google-like storage software .  Parascale  calls their softwate a Virtual Storage Network (VSN). It "aggregates disks across commodity Linux x86 servers to deliver petabyte-scale file storage. With features such as automated, transparent file replication and file migration, Parascale eliminates storage hotspots and delivers massive read/write bandwidth." Why should you care?  I don't know about you, but the "storage problem" is one</p><p>2 0.16634963 <a title="112-tfidf-2" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:   Sorting 1 PB with MapReduce . PB is not peanut-butter-and-jelly misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.  It took six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000 computers  and the results were replicated thrice on 48,000 disks.  Update:   Greg Linden  points to a new Google article  MapReduce: simplified data processing on large clusters . Some interesting stats: 100k MapReduce jobs are executed each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  Google is the King of scalability.  Everyone knows Google for their large,  sophisticated, and fast searching, but they don't just shine in search. Their platform approach to building scalable applications allows them to roll out internet scale applications at an alarmingly high competition crushing rate. Their goal is always to build</p><p>3 0.15359208 <a title="112-tfidf-3" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data ( arround 44 gigs right now ).     For some time I have been using rsync to keep all the content equal on all servers, but the amount of data has been growing, and rsync takes a few too much time to "compare" all data from source to destination, and create a lot of I/O.     I have been taking a look at MogileFS, it seems a good and reliable option, but as the fuse module is not finished, we should have to rewrite all our apps, and its not an option atm.     Any ideas?     I just want a "real time, non resource-hungry" solution alternative for rsync. If I get more features on the way, then they are welcome :)     Why I prefer to use a Distributed File System instead of using NAS + NFS?     - I need 2 NAS, if I dont want a point of failure, and NAS hard is expensive.   - Non-shared hardware, all server has their own local disks.   - As files are replicated, I can save a lot of money, RAID is not a MUST.     Thn</p><p>4 0.13301542 <a title="112-tfidf-4" href="../high_scalability-2009/high_scalability-2009-02-19-Heavy_upload_server_scalability.html">516 high scalability-2009-02-19-Heavy upload server scalability</a></p>
<p>Introduction: Hi,     We are running a backup solution that uploads every night the files our clients worked on during the day (Cabonite-like).   We have currently about 10GB of data per night, via http PUT requests (1 per file), and the files are written as-is on a NAS.   Our architecture is basically compound of a load balancer (hardware, sticky sessions), 5 servers (Tomcat under RHEL4/5, ) and a NAS (nfs 3).     Since our number of clients is rising, (as is our system load) how would you recommend we could scale our infrastructure? hardware and software? Should we go towards NAS sharding, more servers, NIO on tomcat...?       Thanks for your inputs!</p><p>5 0.12035613 <a title="112-tfidf-5" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>Introduction: “ Data is everywhere, never be at a single location. Not scalable, not maintainable. ”  –Alex Szalay    While Galileo played life and death doctrinal games over the mysteries revealed by the telescope, another revolution went unnoticed, the microscope gave up mystery after mystery and nobody yet understood how subversive would be what it revealed. For the first time these new tools of perceptual augmentation allowed humans to peek behind the veil of appearance. A new new eye driving human invention and discovery for hundreds of years.     Data is another    material    that hides, revealing itself only when we look at different scales and investigate its underlying patterns. If the universe is truly    made of information   , then we are looking into truly primal stuff. A new eye is needed for Data and an ambitious project called    Data-scope    aims to be the lens.     A detailed    paper    on the Data-Scope tells more about what it is:  
  The Data-Scope is a new scientific instrum</p><p>6 0.1164172 <a title="112-tfidf-6" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>7 0.11619018 <a title="112-tfidf-7" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>8 0.10471509 <a title="112-tfidf-8" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>9 0.10294846 <a title="112-tfidf-9" href="../high_scalability-2008/high_scalability-2008-01-28-Howto_setup_GFS-GNBD.html">227 high scalability-2008-01-28-Howto setup GFS-GNBD</a></p>
<p>10 0.10172988 <a title="112-tfidf-10" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>11 0.10118516 <a title="112-tfidf-11" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_GridLayer._Utility_computing_for_online_application.html">42 high scalability-2007-07-30-Product: GridLayer. Utility computing for online application</a></p>
<p>12 0.10023803 <a title="112-tfidf-12" href="../high_scalability-2008/high_scalability-2008-02-12-Product%3A_rPath_-_Creating_and_Managing_Virtual_Appliances.html">245 high scalability-2008-02-12-Product: rPath - Creating and Managing Virtual Appliances</a></p>
<p>13 0.099528641 <a title="112-tfidf-13" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>14 0.098079652 <a title="112-tfidf-14" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>15 0.097953945 <a title="112-tfidf-15" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>16 0.097318396 <a title="112-tfidf-16" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>17 0.096637614 <a title="112-tfidf-17" href="../high_scalability-2011/high_scalability-2011-09-13-Must_see%3A_5_Steps_to_Scaling_MongoDB_%28Or_Any_DB%29_in_8_Minutes.html">1114 high scalability-2011-09-13-Must see: 5 Steps to Scaling MongoDB (Or Any DB) in 8 Minutes</a></p>
<p>18 0.093178317 <a title="112-tfidf-18" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>19 0.092623524 <a title="112-tfidf-19" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>20 0.089644395 <a title="112-tfidf-20" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, 0.059), (2, 0.002), (3, -0.007), (4, -0.039), (5, 0.003), (6, 0.018), (7, -0.035), (8, 0.017), (9, 0.037), (10, -0.0), (11, -0.059), (12, 0.003), (13, -0.004), (14, 0.056), (15, 0.056), (16, -0.019), (17, 0.042), (18, -0.065), (19, 0.046), (20, 0.026), (21, 0.037), (22, -0.009), (23, -0.002), (24, -0.017), (25, -0.035), (26, 0.101), (27, -0.03), (28, -0.093), (29, 0.038), (30, -0.047), (31, 0.011), (32, 0.03), (33, -0.012), (34, -0.065), (35, -0.003), (36, -0.027), (37, 0.021), (38, 0.009), (39, -0.017), (40, -0.039), (41, -0.08), (42, -0.044), (43, 0.026), (44, -0.018), (45, 0.024), (46, -0.012), (47, -0.012), (48, -0.002), (49, 0.006)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96337515 <a title="112-lsi-1" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>Introduction: Update :   Parascale’s CTO on what’s different about Parascale .   Let's say you have gigglebytes of data to store and you aren't sure you want to use a  CDN . Amazon's  S3  doesn't excite you. And you aren't quite ready to join the  grid  nation. You want to keep it all in house. Wouldn't it be nice to have something like the  Google File System  you could use to create a unified file system out of all your disks sitting on all your nodes?
 
According to Robin Harris, a.k.a  StorageMojo  (a great blog BTW), you can now have your own GFS:  Parascale launches Google-like storage software .  Parascale  calls their softwate a Virtual Storage Network (VSN). It "aggregates disks across commodity Linux x86 servers to deliver petabyte-scale file storage. With features such as automated, transparent file replication and file migration, Parascale eliminates storage hotspots and delivers massive read/write bandwidth." Why should you care?  I don't know about you, but the "storage problem" is one</p><p>2 0.85188234 <a title="112-lsi-2" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>Introduction: DataDirect Networks (www.ddn.com) is searching for beta testers for our exciting new object-based clustered storage system.  Does this sound like you?     * Need to store millions to hundreds of billions of files   * Want to use one big file system but can't because no single file system scales big enough   * Running out of inodes   * Have to constantly tweak file systems to perform better   * Need to replicate content to more than one data center across geographies   * Have thumbnail images or other small files that wreak havoc on your file and storage systems   * Constantly tweaking and engineering around performance and scalability limits   * No storage system delivers enough IOPS to serve your content   * Spend time load balancing the storage environment   * Want a single, simple way to manage all this data     If this sounds like you, please contact me at jgoldstein@ddn.com.  DataDirect Networks is a 10-year old, well-established storage systems company specializing in Extreme Sto</p><p>3 0.84560734 <a title="112-lsi-3" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:    GlusterFS  is a clustered file-system capable of scaling to several peta-bytes. It aggregates various storage bricks over Infiniband RDMA or TCP/IP interconnect into one large parallel network file system. Storage bricks can be made of any commodity hardware such as x86-64 server with SATA-II RAID and Infiniband HBA).  Cluster file systems are still not mature for enterprise market. They are too complex to deploy and maintain though they are extremely scalable and cheap. Can be entirely built out of commodity OS and hardware. GlusterFS hopes to solves this problem.   GlusterFS achieved  35 GBps read throughput . The GlusterFS Aggregated I/O Benchmark was performed on 64 bricks clustered storage system over 10 Gbps Infiniband interconnect. A cluster of 220 clients pounded the storage system with multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB block size. GlusterFS was configured with unify translator and round-robin scheduler</p><p>4 0.83977324 <a title="112-lsi-4" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data ( arround 44 gigs right now ).     For some time I have been using rsync to keep all the content equal on all servers, but the amount of data has been growing, and rsync takes a few too much time to "compare" all data from source to destination, and create a lot of I/O.     I have been taking a look at MogileFS, it seems a good and reliable option, but as the fuse module is not finished, we should have to rewrite all our apps, and its not an option atm.     Any ideas?     I just want a "real time, non resource-hungry" solution alternative for rsync. If I get more features on the way, then they are welcome :)     Why I prefer to use a Distributed File System instead of using NAS + NFS?     - I need 2 NAS, if I dont want a point of failure, and NAS hard is expensive.   - Non-shared hardware, all server has their own local disks.   - As files are replicated, I can save a lot of money, RAID is not a MUST.     Thn</p><p>5 0.79386133 <a title="112-lsi-5" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:   Kosmos File System (KFS)  .  Thanks to Rich Skrenta for turning me on to KFS and I think his blog   post   says it all.  KFS is an open source project written in C++ by search startup   Kosmix  . The team members have a good   pedigree   so there's a better than average chance this software will be worth considering.     After you stop trying to turn KFS into "Kentucky Fried File System" in your mind,  take a look at KFS' intriguing feature set:           Incremental scalability: New chunkserver nodes can be added as storage needs increase; the system automatically adapts to the new nodes.  Availability: Replication is used to provide availability due to chunk server failures. Typically, files are replicated 3-way.   Per file degree of replication: The degree of replication is configurable on a per file basis, with a max. limit of 64.   Re-replication: Whenever the degree of replication for a file drops below the configured amount (</p><p>6 0.77681261 <a title="112-lsi-6" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>7 0.7735604 <a title="112-lsi-7" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>8 0.76518774 <a title="112-lsi-8" href="../high_scalability-2007/high_scalability-2007-10-21-Paper%3A_Standardizing_Storage_Clusters_%28with_pNFS%29.html">128 high scalability-2007-10-21-Paper: Standardizing Storage Clusters (with pNFS)</a></p>
<p>9 0.76232916 <a title="112-lsi-9" href="../high_scalability-2011/high_scalability-2011-12-23-Funny%3A_A_Cautionary_Tale_About_Storage_and_Backup.html">1162 high scalability-2011-12-23-Funny: A Cautionary Tale About Storage and Backup</a></p>
<p>10 0.76076549 <a title="112-lsi-10" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>11 0.75115973 <a title="112-lsi-11" href="../high_scalability-2007/high_scalability-2007-11-06-Product%3A_ChironFS.html">143 high scalability-2007-11-06-Product: ChironFS</a></p>
<p>12 0.73702091 <a title="112-lsi-12" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>13 0.73601907 <a title="112-lsi-13" href="../high_scalability-2011/high_scalability-2011-12-23-Stuff_The_Internet_Says_On_Scalability_For_December_23%2C_2011.html">1163 high scalability-2011-12-23-Stuff The Internet Says On Scalability For December 23, 2011</a></p>
<p>14 0.73127931 <a title="112-lsi-14" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>15 0.73097575 <a title="112-lsi-15" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>16 0.71657026 <a title="112-lsi-16" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>17 0.71458894 <a title="112-lsi-17" href="../high_scalability-2007/high_scalability-2007-07-15-Isilon_Clustred_Storage_System.html">12 high scalability-2007-07-15-Isilon Clustred Storage System</a></p>
<p>18 0.71004182 <a title="112-lsi-18" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>19 0.70991272 <a title="112-lsi-19" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>20 0.69336307 <a title="112-lsi-20" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.123), (2, 0.166), (10, 0.065), (13, 0.24), (38, 0.028), (56, 0.012), (61, 0.08), (77, 0.011), (79, 0.121), (85, 0.02), (94, 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.86518461 <a title="112-lda-1" href="../high_scalability-2007/high_scalability-2007-07-27-Product%3A_Munin_Monitoriting_Tool.html">34 high scalability-2007-07-27-Product: Munin Monitoriting Tool</a></p>
<p>Introduction: Munin  the monitoring tool surveys all your computers and remembers what it saw. It presents all the information in graphs through a web interface. Its emphasis is on plug and play capabilities. After completing a installation a high number of monitoring plugins will be playing with no more effort.  Using Munin you can easily monitor the performance of your computers, networks, SANs, applications, weather measurements and whatever comes to mind. It makes it easy to determine "what's different today" when a performance problem crops up. It makes it easy to see how you're doing capacity-wise on any resources.</p><p>2 0.84750164 <a title="112-lda-2" href="../high_scalability-2012/high_scalability-2012-07-12-4_Strategies_for_Punching_Down_Traffic_Spikes.html">1282 high scalability-2012-07-12-4 Strategies for Punching Down Traffic Spikes</a></p>
<p>Introduction: Travis Reeder in  Spikability - An Application's Ability to Handle Unknown and/or Inconsistent Load  gives four good ways of handling spikey loads:
  
  Have more resources than you'll ever need . Estimate the maximum traffic you'll need and keep that many servers running. Downside is you are paying for capacity you aren't using. 
  Disable features during high loads . Reduce load by disabling features or substituting in lighter weight features. Downside is users to have access to features. 
  Auto scaling . Launch new servers in response to load. Downsides are it's complicated to setup and slow to respond. Random spikes will cause cycling of instances going up and down. 
  Use message queues . Queues soak up work requests during traffic spikes. More servers can be started to process work from the queue. Resources aren't wasted and features are disabled. Downside is increased latency.</p><p>3 0.83849287 <a title="112-lda-3" href="../high_scalability-2010/high_scalability-2010-01-11-Have_We_Reached_the_End_of_Scaling%3F.html">758 high scalability-2010-01-11-Have We Reached the End of Scaling?</a></p>
<p>Introduction: This is an excerpt from my article  Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud.  
 
Have we reached the end of scaling? That's what I asked myself one day after noticing a bunch of "The End of" headlines. We've reached  The End of History  because the Western liberal democracy is the "end point of humanity's sociocultural evolution and the final form of human government."  We've reached  The End of Science  because of the "fact that there aren't going to be any obvious, cataclysmic revolutions." We've even reached  The End of Theory  because all answers can be found in the continuous stream of data we're collecting. And doesn't always seem like we're at  The End of the World ?
 
Motivated by the prospect of everything ending, I began to wonder: have we really reached The End of Scaling?
 
For a while I thought this might be true. The reason I thought the End of Scaling might be near is because of the slow down of potential articles at m</p><p>4 0.83789414 <a title="112-lda-4" href="../high_scalability-2008/high_scalability-2008-12-29-100%25_on_Amazon_Web_Services%3A_Soocial.com_-_a_lesson_of_porting_your_service_to_Amazon.html">477 high scalability-2008-12-29-100% on Amazon Web Services: Soocial.com - a lesson of porting your service to Amazon</a></p>
<p>Introduction: Simone Brunozzi, technology evangelist for Amazon Web Services in Europe, describes how Soocial.com was fully ported to Amazon web services.         ----------------   This period of the year I decided to dedicate some time to better understand how our customers use AWS, therefore I spent some online time with Stefan Fountain and the nice guys at Soocial.com, a "one address book solution to contact management", and I would like to share with you some details of their IT infrastructure, which now runs 100% on Amazon Web Services!    In the last few months, they've been working hard to cope with tens of thousands of users and to get ready to easily scale to millions. To make this possible, they decided to move ALL their architecture to Amazon Web Services. Despite the fact that they were quite happy with their previous hosting provider, Amazon proved to be the way to go.    -----------------           Read the rest of the article here .</p><p>5 0.82066047 <a title="112-lda-5" href="../high_scalability-2014/high_scalability-2014-01-10-Stuff_The_Internet_Says_On_Scalability_For_January_10th%2C_2014.html">1576 high scalability-2014-01-10-Stuff The Internet Says On Scalability For January 10th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:
    Run  pneumatic tubes  alongside optical fiber cables and we unite the digital and material worlds.   
  1 billion : searches on DuckDuckGo in 2013 
 Quotable Quotes:                                         
 
  pg : We [Hacker News] currently get just over 200k uniques and just under 2m page views on weekdays (less on weekends).                                        
 
  rtm : New server: one Xeon E5-2690 chip, 2.9 GHz, 8 cores total, 32 GB RAM. 
 
 
  Kyle Vanhemert : The graph shows the site’s [Reddit] beginnings in the primordial muck of porn and programming. 
  Drake Baer : But it's not about knowing the most people possible. Instead of being about size, a successful network is about shape. 
  @computionist : Basically when you cache early to scale you've admitted you have no idea what you're doing. 
  Norvig's Law : Any technology that surpasses 50% penetration will never double again.   
  mbell : Keep in mind that a single modern physical ser</p><p>6 0.81944776 <a title="112-lda-6" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>7 0.81793785 <a title="112-lda-7" href="../high_scalability-2008/high_scalability-2008-09-23-The_7_Stages_of_Scaling_Web_Apps.html">391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</a></p>
<p>same-blog 8 0.81705987 <a title="112-lda-8" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>9 0.77439737 <a title="112-lda-9" href="../high_scalability-2009/high_scalability-2009-07-29-Strategy%3A_Devirtualize_for_More_Vroom.html">664 high scalability-2009-07-29-Strategy: Devirtualize for More Vroom</a></p>
<p>10 0.77196169 <a title="112-lda-10" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>11 0.75684291 <a title="112-lda-11" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>12 0.7458908 <a title="112-lda-12" href="../high_scalability-2009/high_scalability-2009-06-10-Hive_-_A_Petabyte_Scale_Data_Warehouse_using_Hadoop.html">624 high scalability-2009-06-10-Hive - A Petabyte Scale Data Warehouse using Hadoop</a></p>
<p>13 0.74541962 <a title="112-lda-13" href="../high_scalability-2013/high_scalability-2013-04-25-Paper%3A_Making_reliable_distributed_systems_in_the_presence_of_software_errors.html">1446 high scalability-2013-04-25-Paper: Making reliable distributed systems in the presence of software errors</a></p>
<p>14 0.73227614 <a title="112-lda-14" href="../high_scalability-2011/high_scalability-2011-05-15-Building_a_Database_remote_availability_site.html">1041 high scalability-2011-05-15-Building a Database remote availability site</a></p>
<p>15 0.73121339 <a title="112-lda-15" href="../high_scalability-2014/high_scalability-2014-04-04-Stuff_The_Internet_Says_On_Scalability_For_April_4th%2C_2014.html">1626 high scalability-2014-04-04-Stuff The Internet Says On Scalability For April 4th, 2014</a></p>
<p>16 0.73003548 <a title="112-lda-16" href="../high_scalability-2010/high_scalability-2010-02-10-ElasticSearch_-_Open_Source%2C_Distributed%2C_RESTful_Search_Engine.html">775 high scalability-2010-02-10-ElasticSearch - Open Source, Distributed, RESTful Search Engine</a></p>
<p>17 0.72940224 <a title="112-lda-17" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>18 0.72938228 <a title="112-lda-18" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>19 0.72925043 <a title="112-lda-19" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>20 0.72853249 <a title="112-lda-20" href="../high_scalability-2007/high_scalability-2007-10-23-Hire_Facebook%2C_Ning%2C_and_Salesforce_to_Scale_for_You.html">129 high scalability-2007-10-23-Hire Facebook, Ning, and Salesforce to Scale for You</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
