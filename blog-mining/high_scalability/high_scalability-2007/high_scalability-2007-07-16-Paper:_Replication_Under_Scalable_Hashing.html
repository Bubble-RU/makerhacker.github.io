<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-19" href="#">high_scalability-2007-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-19-html" href="http://highscalability.com//blog/2007/7/16/paper-replication-under-scalable-hashing.html">html</a></p><p>Introduction: Replication Under Scalable Hashing: A Family of Algorithms for
ScalableDecentralized Data DistributionFrom the abstract:Typical algorithms
for decentralized data distributionwork best in a system that is fully built
before it first used;adding or removing components results in either
extensivereorganization of data or load imbalance in the system.We have
developed a family of decentralized algorithms,RUSH (Replication Under
Scalable Hashing), thatmaps replicated objects to a scalable collection of
storageservers or disks. RUSH algorithms distribute objects toservers
according to user-specified server weighting. Whileall RUSH variants support
addition of servers to the system,different variants have different
characteristics withrespect to lookup time in petabyte-scale systems,
performancewith mirroring (as opposed to redundancy codes),and storage server
removal. All RUSH variants redistributeas few objects as possible when new
servers areadded or existing servers are removed, and all v</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We have developed a family of decentralized algorithms,RUSH (Replication Under Scalable Hashing), thatmaps replicated objects to a scalable collection of storageservers or disks. [sent-2, score-1.037]
</p><p>2 RUSH algorithms distribute objects toservers according to user-specified server weighting. [sent-3, score-0.684]
</p><p>3 Whileall RUSH variants support addition of servers to the system,different variants have different characteristics withrespect to lookup time in petabyte-scale systems, performancewith mirroring (as opposed to redundancy codes),and storage server removal. [sent-4, score-1.69]
</p><p>4 All RUSH variants redistributeas few objects as possible when new servers areadded or existing servers are removed, and all variantsguarantee that no two replicas of a particular object areever placed on the same server. [sent-5, score-1.407]
</p><p>5 Because there is no centraldirectory, clients can compute data locations in parallel,allowing thousands of clients to access objects on thousandsof servers simultaneously. [sent-6, score-1.148]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rush', 0.435), ('variants', 0.427), ('objects', 0.266), ('decentralized', 0.229), ('family', 0.218), ('hashing', 0.207), ('algorithms', 0.192), ('thousandsof', 0.187), ('imbalance', 0.176), ('mirroring', 0.142), ('clients', 0.14), ('opposed', 0.135), ('object', 0.135), ('codes', 0.127), ('replication', 0.109), ('servers', 0.107), ('placed', 0.103), ('scalable', 0.102), ('removing', 0.102), ('removed', 0.1), ('lookup', 0.098), ('characteristics', 0.096), ('replicas', 0.096), ('locations', 0.094), ('abstract', 0.091), ('according', 0.089), ('redundancy', 0.087), ('distribute', 0.087), ('device', 0.079), ('replicated', 0.078), ('collection', 0.074), ('developed', 0.07), ('either', 0.067), ('typical', 0.065), ('particular', 0.064), ('storage', 0.061), ('addition', 0.06), ('compute', 0.06), ('fully', 0.058), ('adding', 0.058), ('thousands', 0.058), ('existing', 0.058), ('components', 0.057), ('data', 0.056), ('results', 0.054), ('server', 0.05), ('possible', 0.044), ('built', 0.042), ('access', 0.04), ('best', 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="19-tfidf-1" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>Introduction: Replication Under Scalable Hashing: A Family of Algorithms for
ScalableDecentralized Data DistributionFrom the abstract:Typical algorithms
for decentralized data distributionwork best in a system that is fully built
before it first used;adding or removing components results in either
extensivereorganization of data or load imbalance in the system.We have
developed a family of decentralized algorithms,RUSH (Replication Under
Scalable Hashing), thatmaps replicated objects to a scalable collection of
storageservers or disks. RUSH algorithms distribute objects toservers
according to user-specified server weighting. Whileall RUSH variants support
addition of servers to the system,different variants have different
characteristics withrespect to lookup time in petabyte-scale systems,
performancewith mirroring (as opposed to redundancy codes),and storage server
removal. All RUSH variants redistributeas few objects as possible when new
servers areadded or existing servers are removed, and all v</p><p>2 0.18595898 <a title="19-tfidf-2" href="../high_scalability-2011/high_scalability-2011-01-14-Stuff_The_Internet_Says_On_Scalability_For_January_14%2C_2011.html">973 high scalability-2011-01-14-Stuff The Internet Says On Scalability For January 14, 2011</a></p>
<p>Introduction: Submitted for your reading pleasure...On the new yearTwitter set a recordwith
6,939 Tweets Per Second (TPS). Cool video visualizing New Year's Eve Tweet
data across the world. Marko Rodriguez in Memoirs of a Graph Addict: Despair
to Redemption tells a stirring tale of how graph programming saved the world
from certain destruction by realizing Aritstotle's dream of an eudaimonia-
driven society. Could a relational database do that? The tools of the
revolution can be found at tinkerprop.com, which describes a databases
agnostic stack for working with property graphs, they include Blueprints - a
property graph model interface; Pipes - a dataflow netowork using process
grapphs; Gremlin - a graph based programming language; Rexster - a RESTful
graph shell.The never never ending battle of good versus evil has nothing on
programmers arguing about bracket policies or sync vs async programming
models. In this node.js thread, I love async, but I can't code like this, the
battle continues. In the</p><p>3 0.15151589 <a title="19-tfidf-3" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>Introduction: Consistent hashing is one of those ideas that really puts the science in
computer science and reminds us why all those really smart people spend years
slaving over algorithms. Consistent hashing is "a scheme that provides hash
table functionality in a way that the addition or removal of one slot does not
significantly change the mapping of keys to slots" and was originally a way of
distributing requests among a changing population of web servers. My first
reaction to the idea was "wow, that's really smart" and I sadly realized I
would never come up with something so elegant. I then immediately saw
applications for it everywhere. And consistent hashing is used everywhere:
distributed hash tables, overlay networks, P2P, IM, caching, and CDNs. Here's
the abstract from the original paper and after the abstract are some links to
a few very good articles with accessible explanations of consistent hashing
and its applications in the real world.breakAbstract:We describe a family of
caching pro</p><p>4 0.12974919 <a title="19-tfidf-4" href="../high_scalability-2010/high_scalability-2010-09-02-Distributed_Hashing_Algorithms_by_Example%3A_Consistent_Hashing.html">892 high scalability-2010-09-02-Distributed Hashing Algorithms by Example: Consistent Hashing</a></p>
<p>Introduction: Consistent Hashing is a specific implementation of hashing that is well suited
for many of today's web-scale load balancing problems. Specifically, it can be
seen in use in various caching solutions like Memcached and is applicable to
NoSQL solutions as well. Consistent Hashing is used particularly because it
provides a solution for the typical "hashcode mod n" method of distributing
keys across a series of servers. It does this by allowing servers to be added
or removed without significantly upsetting the distribution of keys, nor does
it require that all keys be rehashed to accommodate the change in the number
of servers.ďťżYou can read the full storehere.</p><p>5 0.12842001 <a title="19-tfidf-5" href="../high_scalability-2007/high_scalability-2007-10-18-another_approach_to_replication.html">125 high scalability-2007-10-18-another approach to replication</a></p>
<p>Introduction: File replication based on erasure codes can reduce total replicas size 2 times
and more.</p><p>6 0.12310652 <a title="19-tfidf-6" href="../high_scalability-2008/high_scalability-2008-08-29-Product%3A_ScaleOut_StateServer_is_Memcached_on_Steroids.html">373 high scalability-2008-08-29-Product: ScaleOut StateServer is Memcached on Steroids</a></p>
<p>7 0.11482569 <a title="19-tfidf-7" href="../high_scalability-2009/high_scalability-2009-03-17-IBM_WebSphere_eXtreme_Scale_%28IMDG%29.html">542 high scalability-2009-03-17-IBM WebSphere eXtreme Scale (IMDG)</a></p>
<p>8 0.10573222 <a title="19-tfidf-8" href="../high_scalability-2010/high_scalability-2010-05-26-End-To-End_Performance_Study_of_Cloud_Services.html">831 high scalability-2010-05-26-End-To-End Performance Study of Cloud Services</a></p>
<p>9 0.090702131 <a title="19-tfidf-9" href="../high_scalability-2012/high_scalability-2012-02-24-Stuff_The_Internet_Says_On_Scalability_For_February_24%2C_2012.html">1198 high scalability-2012-02-24-Stuff The Internet Says On Scalability For February 24, 2012</a></p>
<p>10 0.085939139 <a title="19-tfidf-10" href="../high_scalability-2008/high_scalability-2008-08-14-Product%3A_Terracotta_-_Open_Source_Network-Attached_Memory.html">364 high scalability-2008-08-14-Product: Terracotta - Open Source Network-Attached Memory</a></p>
<p>11 0.085733779 <a title="19-tfidf-11" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>12 0.08379437 <a title="19-tfidf-12" href="../high_scalability-2010/high_scalability-2010-02-25-Paper%3A_High_Performance_Scalable_Data_Stores_.html">784 high scalability-2010-02-25-Paper: High Performance Scalable Data Stores </a></p>
<p>13 0.080486886 <a title="19-tfidf-13" href="../high_scalability-2014/high_scalability-2014-04-07-Google_Finds%3A_Centralized_Control%2C_Distributed_Data_Architectures_Work_Better_than_Fully_Decentralized_Architectures.html">1627 high scalability-2014-04-07-Google Finds: Centralized Control, Distributed Data Architectures Work Better than Fully Decentralized Architectures</a></p>
<p>14 0.079406552 <a title="19-tfidf-14" href="../high_scalability-2013/high_scalability-2013-06-27-Paper%3A_XORing_Elephants%3A_Novel_Erasure_Codes_for_Big_Data.html">1483 high scalability-2013-06-27-Paper: XORing Elephants: Novel Erasure Codes for Big Data</a></p>
<p>15 0.076937392 <a title="19-tfidf-15" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>16 0.076216698 <a title="19-tfidf-16" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>17 0.070600569 <a title="19-tfidf-17" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<p>18 0.070397094 <a title="19-tfidf-18" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>19 0.069349721 <a title="19-tfidf-19" href="../high_scalability-2014/high_scalability-2014-02-19-Planetary-Scale_Computing_Architectures_for_Electronic_Trading_and_How_Algorithms_Shape_Our_World.html">1599 high scalability-2014-02-19-Planetary-Scale Computing Architectures for Electronic Trading and How Algorithms Shape Our World</a></p>
<p>20 0.06908913 <a title="19-tfidf-20" href="../high_scalability-2013/high_scalability-2013-11-19-We_Finally_Cracked_the_10K_Problem_-_This_Time_for_Managing_Servers_with_2000x_Servers_Managed_Per_Sysadmin.html">1550 high scalability-2013-11-19-We Finally Cracked the 10K Problem - This Time for Managing Servers with 2000x Servers Managed Per Sysadmin</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.101), (1, 0.045), (2, 0.016), (3, -0.031), (4, -0.023), (5, 0.076), (6, 0.049), (7, -0.04), (8, -0.056), (9, 0.025), (10, 0.008), (11, -0.004), (12, -0.023), (13, -0.003), (14, 0.005), (15, 0.035), (16, 0.01), (17, 0.019), (18, 0.02), (19, -0.014), (20, -0.015), (21, 0.049), (22, -0.015), (23, -0.018), (24, -0.036), (25, -0.055), (26, 0.043), (27, 0.01), (28, 0.007), (29, -0.008), (30, -0.021), (31, -0.01), (32, -0.007), (33, -0.016), (34, -0.038), (35, -0.031), (36, 0.049), (37, -0.021), (38, 0.006), (39, -0.006), (40, 0.039), (41, -0.007), (42, 0.029), (43, 0.004), (44, -0.067), (45, 0.0), (46, -0.014), (47, 0.027), (48, 0.048), (49, 0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91426748 <a title="19-lsi-1" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>Introduction: Replication Under Scalable Hashing: A Family of Algorithms for
ScalableDecentralized Data DistributionFrom the abstract:Typical algorithms
for decentralized data distributionwork best in a system that is fully built
before it first used;adding or removing components results in either
extensivereorganization of data or load imbalance in the system.We have
developed a family of decentralized algorithms,RUSH (Replication Under
Scalable Hashing), thatmaps replicated objects to a scalable collection of
storageservers or disks. RUSH algorithms distribute objects toservers
according to user-specified server weighting. Whileall RUSH variants support
addition of servers to the system,different variants have different
characteristics withrespect to lookup time in petabyte-scale systems,
performancewith mirroring (as opposed to redundancy codes),and storage server
removal. All RUSH variants redistributeas few objects as possible when new
servers areadded or existing servers are removed, and all v</p><p>2 0.72996163 <a title="19-lsi-2" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>Introduction: Consistent hashing is one of those ideas that really puts the science in
computer science and reminds us why all those really smart people spend years
slaving over algorithms. Consistent hashing is "a scheme that provides hash
table functionality in a way that the addition or removal of one slot does not
significantly change the mapping of keys to slots" and was originally a way of
distributing requests among a changing population of web servers. My first
reaction to the idea was "wow, that's really smart" and I sadly realized I
would never come up with something so elegant. I then immediately saw
applications for it everywhere. And consistent hashing is used everywhere:
distributed hash tables, overlay networks, P2P, IM, caching, and CDNs. Here's
the abstract from the original paper and after the abstract are some links to
a few very good articles with accessible explanations of consistent hashing
and its applications in the real world.breakAbstract:We describe a family of
caching pro</p><p>3 0.71334356 <a title="19-lsi-3" href="../high_scalability-2008/high_scalability-2008-08-29-Product%3A_ScaleOut_StateServer_is_Memcached_on_Steroids.html">373 high scalability-2008-08-29-Product: ScaleOut StateServer is Memcached on Steroids</a></p>
<p>Introduction: ScaleOut StateServeris an in-memory distributed cache across a server farm or
compute grid. Unlike middleware vendors, StateServer is aims at being a very
good data cache, it doesn't try to handle job scheduling as well.StateServer
is what you might get when you take Memcached and merge in all the value added
distributed caching features you've ever dreamed of. True, Memcached is free
and ScaleOut StateServer is very far from free, but for those looking a for a
satisfying out-of-the-box experience, StateServer may be just the caching
solution you are looking for. Yes, "solution" is one of those "oh my God I'm
going to pay through the nose" indicator words, but it really applies here.
Memcached is a framework whereas StateServer has already prepackaged most
features you would need to add through your own programming efforts.Why use a
distributed cache? Because it combines the holly quadrinity of computing:
better performance, linear scalability, high availability, and fast
application d</p><p>4 0.71236402 <a title="19-lsi-4" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>5 0.70865864 <a title="19-lsi-5" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>Introduction: We've seen a lot ofNoSQLaction lately built around distributed hash tables.
Btrees are getting jealous. Btrees, once the king of the database world, want
their throne back.Paul Buchheitsurfaced a paper:A practical scalable
distributed B-treeby Marcos K. Aguilera and Wojciech Golab, that might help
spark a revolution.From the Abstract:We propose a new algorithm for a
practical, fault tolerant, and scalable B-tree distributed over a set of
servers. Our algorithm supports practical features not present in prior work:
transactions that allow atomic execution of multiple operations over multiple
B-trees, online migration of B-tree nodes between servers, and dynamic
addition and removal of servers. Moreover, our algorithm is conceptually
simple: we use transactions to manipulate B-tree nodes so that clients need
not use complicated concurrency and locking protocols used in prior work. To
execute these transactions quickly, we rely on three techniques: (1) We use
optimistic concurrency contro</p><p>6 0.70571357 <a title="19-lsi-6" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>7 0.67090279 <a title="19-lsi-7" href="../high_scalability-2010/high_scalability-2010-09-02-Distributed_Hashing_Algorithms_by_Example%3A_Consistent_Hashing.html">892 high scalability-2010-09-02-Distributed Hashing Algorithms by Example: Consistent Hashing</a></p>
<p>8 0.65840679 <a title="19-lsi-8" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>9 0.65038007 <a title="19-lsi-9" href="../high_scalability-2009/high_scalability-2009-09-07-Product%3A_Infinispan_-_Open_Source_Data_Grid.html">696 high scalability-2009-09-07-Product: Infinispan - Open Source Data Grid</a></p>
<p>10 0.64676964 <a title="19-lsi-10" href="../high_scalability-2009/high_scalability-2009-03-17-IBM_WebSphere_eXtreme_Scale_%28IMDG%29.html">542 high scalability-2009-03-17-IBM WebSphere eXtreme Scale (IMDG)</a></p>
<p>11 0.64512289 <a title="19-lsi-11" href="../high_scalability-2012/high_scalability-2012-04-30-Masstree_-_Much_Faster_than_MongoDB%2C_VoltDB%2C_Redis%2C_and_Competitive_with_Memcached.html">1236 high scalability-2012-04-30-Masstree - Much Faster than MongoDB, VoltDB, Redis, and Competitive with Memcached</a></p>
<p>12 0.63810718 <a title="19-lsi-12" href="../high_scalability-2014/high_scalability-2014-05-15-Paper%3A_SwiftCloud%3A_Fault-Tolerant_Geo-Replication_Integrated_all_the_Way_to_the_Client_Machine.html">1648 high scalability-2014-05-15-Paper: SwiftCloud: Fault-Tolerant Geo-Replication Integrated all the Way to the Client Machine</a></p>
<p>13 0.63698518 <a title="19-lsi-13" href="../high_scalability-2009/high_scalability-2009-03-06-Product%3A_Lightcloud_-_Key-Value_Database.html">528 high scalability-2009-03-06-Product: Lightcloud - Key-Value Database</a></p>
<p>14 0.63309449 <a title="19-lsi-14" href="../high_scalability-2011/high_scalability-2011-01-27-Comet_-_An_Example_of_the_New_Key-Code_Databases.html">979 high scalability-2011-01-27-Comet - An Example of the New Key-Code Databases</a></p>
<p>15 0.63287985 <a title="19-lsi-15" href="../high_scalability-2007/high_scalability-2007-10-18-another_approach_to_replication.html">125 high scalability-2007-10-18-another approach to replication</a></p>
<p>16 0.62875694 <a title="19-lsi-16" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>17 0.62870198 <a title="19-lsi-17" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>18 0.62505889 <a title="19-lsi-18" href="../high_scalability-2011/high_scalability-2011-11-07-10_Core_Architecture_Pattern_Variations_for_Achieving_Scalability.html">1138 high scalability-2011-11-07-10 Core Architecture Pattern Variations for Achieving Scalability</a></p>
<p>19 0.61957651 <a title="19-lsi-19" href="../high_scalability-2007/high_scalability-2007-10-14-Product%3A_The_Spread_Toolkit.html">122 high scalability-2007-10-14-Product: The Spread Toolkit</a></p>
<p>20 0.6186378 <a title="19-lsi-20" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.095), (2, 0.191), (10, 0.051), (40, 0.118), (79, 0.085), (81, 0.226), (94, 0.1)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.85208338 <a title="19-lda-1" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>Introduction: Replication Under Scalable Hashing: A Family of Algorithms for
ScalableDecentralized Data DistributionFrom the abstract:Typical algorithms
for decentralized data distributionwork best in a system that is fully built
before it first used;adding or removing components results in either
extensivereorganization of data or load imbalance in the system.We have
developed a family of decentralized algorithms,RUSH (Replication Under
Scalable Hashing), thatmaps replicated objects to a scalable collection of
storageservers or disks. RUSH algorithms distribute objects toservers
according to user-specified server weighting. Whileall RUSH variants support
addition of servers to the system,different variants have different
characteristics withrespect to lookup time in petabyte-scale systems,
performancewith mirroring (as opposed to redundancy codes),and storage server
removal. All RUSH variants redistributeas few objects as possible when new
servers areadded or existing servers are removed, and all v</p><p>2 0.810215 <a title="19-lda-2" href="../high_scalability-2009/high_scalability-2009-03-16-Cisco_and_Sun_to_Compete_for_Unified_Computing%3F.html">540 high scalability-2009-03-16-Cisco and Sun to Compete for Unified Computing?</a></p>
<p>Introduction: A recentInfoWorld articleclaims that "With Cisco expected to enter the blade
market and Sun expected to offer networking equipment, things could get
interesting awfully fast." How does this effect your infrastructure strategy
and decisions?Would you consider to build scalable web applications on the
Cisco Unified Computing System? Or would you consider to build a router out of
a server with the use of OpenSolaris and Project Crossbow as the article
suggests? Will any of these initiatives change the way we build scalable web
infrastructure or are these just attempts to sale these systems?What do you
think?</p><p>3 0.7317124 <a title="19-lda-3" href="../high_scalability-2010/high_scalability-2010-02-06-GEO-aware_traffic_load_balancing_and_caching_at_CNBC.com.html">773 high scalability-2010-02-06-GEO-aware traffic load balancing and caching at CNBC.com</a></p>
<p>Introduction: CNBC, like many large web sites, relied  on a CDN for content delivery.
Recently, we started looking  to see if we could improve this model.  Our
criteria was:- improve response time- have better control over traffic (real
time reporting, change management and alerting)- better utilize internal
datacenters and their infrastructure- shield users from any troubles at the
origin infrastructure- cost outAfter researching the market, we turned to two
vendors: Dyn (Dynamic Network Services) andaiScaler. We' have had   about a
year worth of experience withaiScaler (search for "CNBC" to see my previous
post ), but Dyn was a new vendor for us.  We started building our relationship
at Velocity conference in the summer of 2009.Dyn has recently started offering
a geo-aware DNS load balancing solution, using Anycast and the distributed
nature of their DNS presence to enable a key component of what we were trying
to achieve: steer users to geographically closest origin point. The traffic
balancing r</p><p>4 0.72926813 <a title="19-lda-4" href="../high_scalability-2013/high_scalability-2013-06-06-Paper%3A_Memory_Barriers%3A_a_Hardware_View_for_Software_Hackers.html">1471 high scalability-2013-06-06-Paper: Memory Barriers: a Hardware View for Software Hackers</a></p>
<p>Introduction: It's not often you get so enthusiastic a recommendation for a paper as Sergio
Bossa gives Memory Barriers: a Hardware View for Software Hackers: If you only
want to read one piece about CPUs architecture, cache coherency and memory
barriers, make it this one.It is a clear and well written article. It even has
a quiz. What's it about?So what possessed CPU designers to cause them to
inﬂict memory barriers on poor unsuspecting SMP software designers?In short,
because reordering memory references allows much better performance, and so
memory barriers are needed to force ordering in things like synchronization
primitives whose correct operation depends on ordered memory
references.Getting a more detailed answer to this question requires a good
understanding of how CPU caches work, and especially what is required to make
caches really work well. The following sections:present the structure of a
cache,describe how cache-coherency protocols ensure that CPUs agree on the
value of each location</p><p>5 0.7265197 <a title="19-lda-5" href="../high_scalability-2013/high_scalability-2013-03-07-It%27s_a_VM_Wasteland_-_A_Near_Optimal_Packing_of_VMs_to_Machines_Reduces_TCO_by_22%25.html">1419 high scalability-2013-03-07-It's a VM Wasteland - A Near Optimal Packing of VMs to Machines Reduces TCO by 22%</a></p>
<p>Introduction: In Algorithm Design for Performance Aware VM Consolidation we learn some
shocking facts (gambling in Casablanca?):Average server utilization in many
data centers is low, estimated between 5% and 15%. This is wasteful because an
idle server often consumes more than 50% of peak power.Surely that's just for
old style datacenters? Nope. In Google data centers, workloads that are
consolidated use only 50% of the processor cores. Every other processor core
is left unused simply to ensure that performance does not degrade.It's a VM
wasteland. The goal is to reduce waste by packing VMs onto machines without
hurting performance or wasting resources. The idea is to select VMs that
interfere the least with each other and places them together on the same
server.It's a NP-Complete problem, but this paper describes a practical method
that performs provably close to the optimal. Interestingly they can optimize
for performance or power efficiency, so you can use different algorithms for
different work</p><p>6 0.72118139 <a title="19-lda-6" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>7 0.71711648 <a title="19-lda-7" href="../high_scalability-2012/high_scalability-2012-12-21-Stuff_The_Internet_Says_On_Scalability_For_December_21%2C_2012.html">1375 high scalability-2012-12-21-Stuff The Internet Says On Scalability For December 21, 2012</a></p>
<p>8 0.71157855 <a title="19-lda-8" href="../high_scalability-2008/high_scalability-2008-05-27-Should_Twitter_be_an_All-You-Can-Eat_Buffet_or_a_Vending_Machine%3F.html">330 high scalability-2008-05-27-Should Twitter be an All-You-Can-Eat Buffet or a Vending Machine?</a></p>
<p>9 0.71146584 <a title="19-lda-9" href="../high_scalability-2010/high_scalability-2010-02-15-The_Amazing_Collective_Compute_Power_of_the_Ambient_Cloud.html">778 high scalability-2010-02-15-The Amazing Collective Compute Power of the Ambient Cloud</a></p>
<p>10 0.71030372 <a title="19-lda-10" href="../high_scalability-2013/high_scalability-2013-07-17-How_do_you_create_a_100th_Monkey_software_development_culture%3F.html">1492 high scalability-2013-07-17-How do you create a 100th Monkey software development culture?</a></p>
<p>11 0.70834756 <a title="19-lda-11" href="../high_scalability-2009/high_scalability-2009-01-04-Alternative_Memcache_Usage%3A_A_Highly_Scalable%2C_Highly_Available%2C_In-Memory_Shard_Index.html">482 high scalability-2009-01-04-Alternative Memcache Usage: A Highly Scalable, Highly Available, In-Memory Shard Index</a></p>
<p>12 0.70555848 <a title="19-lda-12" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<p>13 0.70143199 <a title="19-lda-13" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>14 0.70027411 <a title="19-lda-14" href="../high_scalability-2010/high_scalability-2010-02-01-What_Will_Kill_the_Cloud%3F.html">768 high scalability-2010-02-01-What Will Kill the Cloud?</a></p>
<p>15 0.69283772 <a title="19-lda-15" href="../high_scalability-2012/high_scalability-2012-01-13-Stuff_The_Internet_Says_On_Scalability_For_January_13%2C_2012.html">1174 high scalability-2012-01-13-Stuff The Internet Says On Scalability For January 13, 2012</a></p>
<p>16 0.68987238 <a title="19-lda-16" href="../high_scalability-2011/high_scalability-2011-09-26-17_Techniques_Used_to_Scale_Turntable.fm_and_Labmeeting_to_Millions_of_Users.html">1124 high scalability-2011-09-26-17 Techniques Used to Scale Turntable.fm and Labmeeting to Millions of Users</a></p>
<p>17 0.6892634 <a title="19-lda-17" href="../high_scalability-2012/high_scalability-2012-04-06-Stuff_The_Internet_Says_On_Scalability_For_April_6%2C_2012.html">1223 high scalability-2012-04-06-Stuff The Internet Says On Scalability For April 6, 2012</a></p>
<p>18 0.68867081 <a title="19-lda-18" href="../high_scalability-2010/high_scalability-2010-01-04-11_Strategies_to_Rock_Your_Startup%E2%80%99s_Scalability_in_2010.html">757 high scalability-2010-01-04-11 Strategies to Rock Your Startup’s Scalability in 2010</a></p>
<p>19 0.68835801 <a title="19-lda-19" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>20 0.68760437 <a title="19-lda-20" href="../high_scalability-2013/high_scalability-2013-05-29-Amazon%3A_Creating_a_Customer_Utopia_One_Culture_Hack_at_a_Time.html">1466 high scalability-2013-05-29-Amazon: Creating a Customer Utopia One Culture Hack at a Time</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
