<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 high scalability-2007-11-21-n-phase commit for FS writes, reads stay local</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-163" href="#">high_scalability-2007-163</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>163 high scalability-2007-11-21-n-phase commit for FS writes, reads stay local</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-163-html" href="http://highscalability.com//blog/2007/11/22/n-phase-commit-for-fs-writes-reads-stay-local.html">html</a></p><p>Introduction: I am try  i  ng to f  i  nd a L  i  nux FS that wi  l  l a  l  low me to rep  l  icate a  l  l wr  i  tes synchronous  l  y to n nodes   i  n a web server c  l  uster, wh  i  le keep  i  ng a  l  l reads local. It shou  l  d not require specialized hardware.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I am try  i  ng to f  i  nd a L  i  nux FS that wi  l  l a  l  low me to rep  l  icate a  l  l wr  i  tes synchronous  l  y to n nodes   i  n a web server c  l  uster, wh  i  le keep  i  ng a  l  l reads local. [sent-1, score-0.359]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fs', 0.55), ('synchronously', 0.429), ('replicate', 0.312), ('specialized', 0.28), ('keeping', 0.22), ('reads', 0.202), ('require', 0.193), ('trying', 0.191), ('allow', 0.188), ('linux', 0.188), ('writes', 0.186), ('cluster', 0.148), ('hardware', 0.133), ('find', 0.131), ('server', 0.082), ('web', 0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="163-tfidf-1" href="../high_scalability-2007/high_scalability-2007-11-21-n-phase_commit_for_FS_writes%2C_reads_stay_local.html">163 high scalability-2007-11-21-n-phase commit for FS writes, reads stay local</a></p>
<p>Introduction: I am try  i  ng to f  i  nd a L  i  nux FS that wi  l  l a  l  low me to rep  l  icate a  l  l wr  i  tes synchronous  l  y to n nodes   i  n a web server c  l  uster, wh  i  le keep  i  ng a  l  l reads local. It shou  l  d not require specialized hardware.</p><p>2 0.1303753 <a title="163-tfidf-2" href="../high_scalability-2007/high_scalability-2007-07-25-Product%3A_NetApp_MetroCluster_Software.html">28 high scalability-2007-07-25-Product: NetApp MetroCluster Software</a></p>
<p>Introduction: NetApp MetroCluster Software  Cost-effective is an integrated high-availability storage cluster and site failover capability.  NetApp MetroCluster is an integrated high-availability and disaster recovery solution that can reduce system complexity and simplify management while ensuring greater return on investment. MetroCluster uses clustered server technology to replicate data synchronously between sites located miles apart, eliminating data loss in case of a disruption. Simple and powerful recovery process minimizes downtime, with little or no user action required.   At one company I worked at they used the NetApp snap mirror feature to replicate data across long distances to multiple datacenters. They had a very fast backbone and it worked well. The issue with NetApp is always one of cost, but if you can afford it, it's a good option.</p><p>3 0.12542784 <a title="163-tfidf-3" href="../high_scalability-2007/high_scalability-2007-12-10-1_Master%2C_N_Slaves.html">178 high scalability-2007-12-10-1 Master, N Slaves</a></p>
<p>Introduction: Hello all,     Reading the site you can note that "1 Master for writes, N Slaves for reads" scheme is used offen.     How is this implemented? Who decides where writes and reads go? Something in application level or specific database proxies, like Slony-I?     Thanks.</p><p>4 0.09713912 <a title="163-tfidf-4" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_DRBD_-_Distributed_Replicated_Block_Device.html">271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</a></p>
<p>Introduction: From their website:   DRBD  is a block device which is designed to build high availability clusters. This is done by mirroring a whole block device via (a dedicated) network. You could see it as a network raid-1.  DRBD takes over the data, writes it to the local disk and sends it to the other host. On the other host, it takes it to the disk there.  
   
  The other components needed are a cluster membership service, which is supposed to be heartbeat, and some kind of application that works on top of a block device.  Examples: A filesystem & fsck. A journaling FS. A database with recovery capabilities.  Each device (DRBD provides more than one of these devices) has a state, which can be 'primary' or 'secondary'. On the node with the primary device the application is supposed to run and to access the device (/dev/drbdX). Every write is sent to the local 'lower level block device' and to the node with the device in 'secondary' state. The secondary device simply writes the data to its lowe</p><p>5 0.093384437 <a title="163-tfidf-5" href="../high_scalability-2007/high_scalability-2007-07-25-Product%3A_3_PAR_REMOTE_COPY.html">27 high scalability-2007-07-25-Product: 3 PAR REMOTE COPY</a></p>
<p>Introduction: 3PAR Remote Copy  is a uniquely simple and efficient replication technology that allows customers to protect and share any application data affordably. Built upon 3PAR Thin Copy technology, Remote Copy lowers the total cost of storage by addressing the cost and complexity of remote replication.  Common Uses of 3PAR Remote Copy:  Affordable Disaster Recovery: Mirror data cost-effectively across town or across the world.  Centralized Archive: Replicate data from multiple 3PAR InServs located in multiple data centers to a centralized data archive location.  Resilient Pod Architecture: Mutually replicate tier 1 or 2 data to tier 3 capacity between two InServs (application pods).  Remote Data Access: Replicate data to a remote location for sharing of data with remote users.</p><p>6 0.084622249 <a title="163-tfidf-6" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>7 0.072821341 <a title="163-tfidf-7" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>8 0.071870312 <a title="163-tfidf-8" href="../high_scalability-2011/high_scalability-2011-07-07-Myth%3A_Google_Uses_Server_Farms_So_You_Should_Too_-_Resurrection_of_the_Big-Ass_Machines.html">1075 high scalability-2011-07-07-Myth: Google Uses Server Farms So You Should Too - Resurrection of the Big-Ass Machines</a></p>
<p>9 0.070865035 <a title="163-tfidf-9" href="../high_scalability-2009/high_scalability-2009-04-04-Digg_Architecture.html">554 high scalability-2009-04-04-Digg Architecture</a></p>
<p>10 0.070631184 <a title="163-tfidf-10" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>11 0.069724649 <a title="163-tfidf-11" href="../high_scalability-2009/high_scalability-2009-01-16-Database_Sharding_for_startups.html">492 high scalability-2009-01-16-Database Sharding for startups</a></p>
<p>12 0.068356 <a title="163-tfidf-12" href="../high_scalability-2014/high_scalability-2014-02-12-Paper%3A_Network_Stack_Specialization_for_Performance_.html">1594 high scalability-2014-02-12-Paper: Network Stack Specialization for Performance </a></p>
<p>13 0.067082524 <a title="163-tfidf-13" href="../high_scalability-2010/high_scalability-2010-04-06-Strategy%3A_Make_it_Really_Fast_vs_Do_the_Work_Up_Front.html">805 high scalability-2010-04-06-Strategy: Make it Really Fast vs Do the Work Up Front</a></p>
<p>14 0.066829652 <a title="163-tfidf-14" href="../high_scalability-2012/high_scalability-2012-01-19-Is_it_time_to_get_rid_of_the_Linux_OS_model_in_the_cloud%3F.html">1177 high scalability-2012-01-19-Is it time to get rid of the Linux OS model in the cloud?</a></p>
<p>15 0.064090811 <a title="163-tfidf-15" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>16 0.064036958 <a title="163-tfidf-16" href="../high_scalability-2009/high_scalability-2009-08-11-13_Scalability_Best_Practices.html">679 high scalability-2009-08-11-13 Scalability Best Practices</a></p>
<p>17 0.063853867 <a title="163-tfidf-17" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>18 0.061860874 <a title="163-tfidf-18" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<p>19 0.0608414 <a title="163-tfidf-19" href="../high_scalability-2008/high_scalability-2008-09-05-Product%3A_Tungsten_Replicator.html">380 high scalability-2008-09-05-Product: Tungsten Replicator</a></p>
<p>20 0.060792901 <a title="163-tfidf-20" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.062), (1, 0.036), (2, -0.015), (3, -0.03), (4, -0.007), (5, 0.031), (6, 0.029), (7, -0.026), (8, -0.012), (9, 0.01), (10, -0.009), (11, -0.004), (12, -0.013), (13, -0.009), (14, 0.031), (15, 0.02), (16, -0.006), (17, 0.029), (18, -0.011), (19, 0.019), (20, 0.014), (21, 0.035), (22, -0.055), (23, -0.027), (24, 0.005), (25, 0.001), (26, -0.006), (27, -0.031), (28, -0.031), (29, -0.018), (30, 0.025), (31, 0.004), (32, 0.021), (33, -0.008), (34, -0.001), (35, 0.022), (36, -0.024), (37, -0.005), (38, 0.002), (39, -0.032), (40, -0.036), (41, -0.032), (42, -0.068), (43, 0.019), (44, 0.049), (45, -0.001), (46, 0.029), (47, -0.03), (48, 0.018), (49, 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92755294 <a title="163-lsi-1" href="../high_scalability-2007/high_scalability-2007-11-21-n-phase_commit_for_FS_writes%2C_reads_stay_local.html">163 high scalability-2007-11-21-n-phase commit for FS writes, reads stay local</a></p>
<p>Introduction: I am try  i  ng to f  i  nd a L  i  nux FS that wi  l  l a  l  low me to rep  l  icate a  l  l wr  i  tes synchronous  l  y to n nodes   i  n a web server c  l  uster, wh  i  le keep  i  ng a  l  l reads local. It shou  l  d not require specialized hardware.</p><p>2 0.69188797 <a title="163-lsi-2" href="../high_scalability-2007/high_scalability-2007-12-10-1_Master%2C_N_Slaves.html">178 high scalability-2007-12-10-1 Master, N Slaves</a></p>
<p>Introduction: Hello all,     Reading the site you can note that "1 Master for writes, N Slaves for reads" scheme is used offen.     How is this implemented? Who decides where writes and reads go? Something in application level or specific database proxies, like Slony-I?     Thanks.</p><p>3 0.61861235 <a title="163-lsi-3" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>Introduction: This is the third guest post  (  part 1  ,   part 2  ) of a series by Greg Lindahl, CTO of blekko, the spam free search engine. Previously, Greg was Founder and Distinguished Engineer at PathScale, at which he was the architect of the InfiniPath low-latency InfiniBand HCA, used to build tightly-coupled supercomputing clusters. 
 
blekko's home-grown NoSQL database was designed from the start to support a web-scale search engine, with 1,000s of servers and petabytes of disk. Data replication is a very important part of keeping the database up and serving queries.  Like many NoSQL database authors, we decided to keep R=3 copies of each piece of data in the database, and not use RAID to improve reliability. The key goal we were shooting for was a database which degrades gracefully when there are many small failures over time, without needing human intervention.
  Why don't we like RAID for big NoSQL databases?  
Most big storage systems use RAID levels like 3, 4, 5, or 10 to improve relia</p><p>4 0.58776355 <a title="163-lsi-4" href="../high_scalability-2007/high_scalability-2007-11-06-Product%3A_ChironFS.html">143 high scalability-2007-11-06-Product: ChironFS</a></p>
<p>Introduction: If you are trying to create highly available file systems, especially across data centers,  then ChironFS is one potential solution. It's relatively new, so there aren't lots of experience reports, but it looks worth considering. What is  ChironFS  and how does it work?
 
Adapted from the ChironFS website:   The Chiron Filesystem is a Fuse based filesystem that frees you from single points of failure. It's main purpose is to guarantee filesystem availability using replication. But it isn't a RAID implementation. RAID replicates DEVICES not FILESYSTEMS.  Why not just use RAID over some network block device? Because it is a block device and if one server mounts that device in RW mode, no other server will be able to mount it in RW mode. Any real network may have many servers and offer a variety of services. Keeping everything running can become a real nightmare!</p><p>5 0.57690507 <a title="163-lsi-5" href="../high_scalability-2009/high_scalability-2009-04-04-Digg_Architecture.html">554 high scalability-2009-04-04-Digg Architecture</a></p>
<p>Introduction: Update 4: :  Introducing Digg’s IDDB Infrastructure  by Joe Stump.  IDDB is a way to partition both indexes (e.g. integer sequences and unique character indexes) and actual tables across multiple storage servers (MySQL and MemcacheDB are currently supported with more to follow).   Update 3: :  Scaling Digg and Other Web Applications .  Update 2: :  How Digg Works  and  How Digg Really Works  (wear ear plugs). Brought  to you straight from Digg's blog. A very succinct explanation of the major elements of the Digg architecture while tracing a request through the system. I've updated this profile with the new information.  Update:   Digg now receives 230 million plus page views per month and 26 million unique visitors - traffic that necessitated major internal upgrades .  Traffic generated by Digg's over 22 million famously info-hungry users and 230 million page views can crash an unsuspecting website head-on into its CPU, memory, and bandwidth limits. How does Digg handle billions of req</p><p>6 0.55995256 <a title="163-lsi-6" href="../high_scalability-2010/high_scalability-2010-03-23-Digg%3A_4000%25_Performance_Increase_by_Sorting_in_PHP_Rather_than_MySQL.html">799 high scalability-2010-03-23-Digg: 4000% Performance Increase by Sorting in PHP Rather than MySQL</a></p>
<p>7 0.55708259 <a title="163-lsi-7" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_DRBD_-_Distributed_Replicated_Block_Device.html">271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</a></p>
<p>8 0.55528033 <a title="163-lsi-8" href="../high_scalability-2008/high_scalability-2008-04-29-High_performance_file_server.html">310 high scalability-2008-04-29-High performance file server</a></p>
<p>9 0.55501592 <a title="163-lsi-9" href="../high_scalability-2008/high_scalability-2008-09-10-Shard_servers_--_go_big_or_small%3F.html">383 high scalability-2008-09-10-Shard servers -- go big or small?</a></p>
<p>10 0.55361319 <a title="163-lsi-10" href="../high_scalability-2007/high_scalability-2007-11-16-Product%3A_lbpool_-_Load_Balancing_JDBC_Pool.html">157 high scalability-2007-11-16-Product: lbpool - Load Balancing JDBC Pool</a></p>
<p>11 0.54658234 <a title="163-lsi-11" href="../high_scalability-2012/high_scalability-2012-06-20-Ask_HighScalability%3A_How_do_I_organize_millions_of_images%3F.html">1268 high scalability-2012-06-20-Ask HighScalability: How do I organize millions of images?</a></p>
<p>12 0.54319441 <a title="163-lsi-12" href="../high_scalability-2009/high_scalability-2009-02-19-Heavy_upload_server_scalability.html">516 high scalability-2009-02-19-Heavy upload server scalability</a></p>
<p>13 0.542584 <a title="163-lsi-13" href="../high_scalability-2014/high_scalability-2014-01-15-Vedis_-_An_Embedded_Implementation_of_Redis_Supporting_Terabyte_Sized_Databases.html">1580 high scalability-2014-01-15-Vedis - An Embedded Implementation of Redis Supporting Terabyte Sized Databases</a></p>
<p>14 0.53638434 <a title="163-lsi-14" href="../high_scalability-2008/high_scalability-2008-02-18-How_to_deal_with_an_I-O_bottleneck_to_disk%3F.html">251 high scalability-2008-02-18-How to deal with an I-O bottleneck to disk?</a></p>
<p>15 0.53298414 <a title="163-lsi-15" href="../high_scalability-2008/high_scalability-2008-09-05-Product%3A_Tungsten_Replicator.html">380 high scalability-2008-09-05-Product: Tungsten Replicator</a></p>
<p>16 0.52808219 <a title="163-lsi-16" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<p>17 0.52720481 <a title="163-lsi-17" href="../high_scalability-2007/high_scalability-2007-11-02-How_WordPress.com_Tracks_300_Servers_Handling_10_Million_Pageviews.html">140 high scalability-2007-11-02-How WordPress.com Tracks 300 Servers Handling 10 Million Pageviews</a></p>
<p>18 0.52613735 <a title="163-lsi-18" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<p>19 0.52477366 <a title="163-lsi-19" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>20 0.52325571 <a title="163-lsi-20" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.187), (47, 0.473), (61, 0.126)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.82177818 <a title="163-lda-1" href="../high_scalability-2007/high_scalability-2007-11-21-n-phase_commit_for_FS_writes%2C_reads_stay_local.html">163 high scalability-2007-11-21-n-phase commit for FS writes, reads stay local</a></p>
<p>Introduction: I am try  i  ng to f  i  nd a L  i  nux FS that wi  l  l a  l  low me to rep  l  icate a  l  l wr  i  tes synchronous  l  y to n nodes   i  n a web server c  l  uster, wh  i  le keep  i  ng a  l  l reads local. It shou  l  d not require specialized hardware.</p><p>2 0.74838912 <a title="163-lda-2" href="../high_scalability-2007/high_scalability-2007-08-03-Scaling_IMAP_and_POP3.html">57 high scalability-2007-08-03-Scaling IMAP and POP3</a></p>
<p>Introduction: Just thought I'd drop a brief suggestion to anyone building a large mail system. Our solution for scaling mail pickup was to develop a sharded architecture whereby accounts are spread across a cluster of servers, each with imap/pop3 capability. Then we use a cluster of reverse proxies (Perdition) speaking to the backend imap/pop3 servers . The benefit of this approach is you can use simply use round-robin or HA loadbalancing on the perdition servers that end users connect to (e.g. admins can easily move accounts around on the backend storage servers without affecting end users). Perdition manages routing users to the appropriate backend servers and has MySQL support. What we also liked about this approach was that it had no dependency on a distributed or networked filesystem, so less chance of corruption or data consistency issues. When an individual server reaches capacity, we just off load users to a less used server. If any server goes offline, it only affects the fraction of users</p><p>3 0.71996355 <a title="163-lda-3" href="../high_scalability-2007/high_scalability-2007-09-06-Scaling_IMAP_and_POP3.html">81 high scalability-2007-09-06-Scaling IMAP and POP3</a></p>
<p>Introduction: Another scalability strategy brought to you by Erik Osterman:     Just thought I'd drop a brief suggestion to anyone building a large mail system. Our solution for scaling mail pickup was to develop a sharded architecture whereby accounts are spread across a cluster of servers, each with imap/pop3 capability. Then we use a cluster of reverse proxies (Perdition) speaking to the backend imap/pop3 servers .      The benefit of this approach is you can use simply use round-robin or HA load balancing on the perdition servers that end users connect to (e.g. admins can easily move accounts around on the backend storage servers without affecting end users). Perdition manages routing users to the appropriate backend servers and has MySQL support.      What we also liked about this approach was that it had no dependency on a distributed or networked file system, so less chance of corruption or data consistency issues. When an individual server reaches capacity, we just off load users to a less u</p><p>4 0.68130994 <a title="163-lda-4" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>Introduction: Terrastore  is a new-born document store which provides advanced scalability and elasticity features without sacrificing consistency.   Here are a few highlights:
  
 Ubiquitous: based on the universally supported HTTP protocol.  
 Distributed: nodes can run and live everywhere on your network.  
 Elastic: you can add and remove nodes dynamically to/from your running cluster with no downtime and no changes at all to your configuration.  
 Scalable at the data layer: documents are partitioned and distributed among your nodes, with automatic and transparent re-balancing when nodes join and leave.  
 Scalable at the computational layer: query and update operations are distributed to the nodes which actually holds the queried/updated data, minimizing network traffic and spreading computational load.  
 Consistent: providing per-document consistency, you're guaranteed to always get the latest value of a single document, with read committed isolation for concurrent modifications. 
 Schemales</p><p>5 0.66898698 <a title="163-lda-5" href="../high_scalability-2012/high_scalability-2012-09-20-How_Vimeo_Saves_50%25_on_EC2_by_Playing_a_Smarter_Game.html">1326 high scalability-2012-09-20-How Vimeo Saves 50% on EC2 by Playing a Smarter Game</a></p>
<p>Introduction: Nothing shows how much software architectures have changed than the intelligent scheduling of computation over differently priced compute resources. This isn't just a false economy either. Vimeo saves up to 50% on their video transcoding bill by intelligently playing the spot, reserved, and on-demand markets. If you are ready for some advanced reindeer games then take a look at   Vimeo EC2 transcoding  where they explain their thinking. Even if you don't like their rules, it's the strategy that matters. This presentation was from 2011, so it would be interesting to see if the new reserved instance market has made a difference in their strategy. 
 
Here's Vimeo's approach for minimizing costs using spot, reserved, and on-demand instances:
  
 Never bid more than threshold. It is currently set to 80% of on-demand price. 
 Not more than 10 open spot requests at any time. 
 Bid 10% more than the average price over last hour 
 Buy reserve instance capacity to meet non-peak hour loads. 
 Use</p><p>6 0.64859486 <a title="163-lda-6" href="../high_scalability-2010/high_scalability-2010-01-13-10_Hot_Scalability_Links_for_January_13%2C_2010.html">760 high scalability-2010-01-13-10 Hot Scalability Links for January 13, 2010</a></p>
<p>7 0.63994175 <a title="163-lda-7" href="../high_scalability-2010/high_scalability-2010-07-07-Strategy%3A_Recompute_Instead_of_Remember_Big_Data.html">852 high scalability-2010-07-07-Strategy: Recompute Instead of Remember Big Data</a></p>
<p>8 0.62665629 <a title="163-lda-8" href="../high_scalability-2007/high_scalability-2007-09-17-Blog%3A_Adding_Simplicity_by_Dan_Pritchett.html">94 high scalability-2007-09-17-Blog: Adding Simplicity by Dan Pritchett</a></p>
<p>9 0.62156737 <a title="163-lda-9" href="../high_scalability-2011/high_scalability-2011-12-30-Stuff_The_Internet_Says_On_Scalability_For_December_30%2C_2011.html">1166 high scalability-2011-12-30-Stuff The Internet Says On Scalability For December 30, 2011</a></p>
<p>10 0.59877169 <a title="163-lda-10" href="../high_scalability-2009/high_scalability-2009-03-30-Ebay_history_and_architecture.html">550 high scalability-2009-03-30-Ebay history and architecture</a></p>
<p>11 0.59106058 <a title="163-lda-11" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>12 0.58998531 <a title="163-lda-12" href="../high_scalability-2007/high_scalability-2007-11-07-What_CDN_would_you_recommend%3F.html">144 high scalability-2007-11-07-What CDN would you recommend?</a></p>
<p>13 0.56438798 <a title="163-lda-13" href="../high_scalability-2009/high_scalability-2009-09-17-Infinispan_narrows_the_gap_between_open_source_and_commercial_data_caches_.html">708 high scalability-2009-09-17-Infinispan narrows the gap between open source and commercial data caches </a></p>
<p>14 0.52997279 <a title="163-lda-14" href="../high_scalability-2013/high_scalability-2013-10-11-Stuff_The_Internet_Says_On_Scalability_For_October_11th%2C_2013.html">1530 high scalability-2013-10-11-Stuff The Internet Says On Scalability For October 11th, 2013</a></p>
<p>15 0.52795076 <a title="163-lda-15" href="../high_scalability-2011/high_scalability-2011-06-15-101_Questions_to_Ask_When_Considering_a_NoSQL_Database.html">1062 high scalability-2011-06-15-101 Questions to Ask When Considering a NoSQL Database</a></p>
<p>16 0.52549094 <a title="163-lda-16" href="../high_scalability-2011/high_scalability-2011-06-06-NoSQL_Pain%3F_Learn_How_to_Read-write_Scale_Without_a_Complete_Re-write.html">1054 high scalability-2011-06-06-NoSQL Pain? Learn How to Read-write Scale Without a Complete Re-write</a></p>
<p>17 0.49968693 <a title="163-lda-17" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<p>18 0.4906514 <a title="163-lda-18" href="../high_scalability-2008/high_scalability-2008-12-05-Sprinkle_-_Provisioning_Tool_to_Build_Remote_Servers.html">461 high scalability-2008-12-05-Sprinkle - Provisioning Tool to Build Remote Servers</a></p>
<p>19 0.48267382 <a title="163-lda-19" href="../high_scalability-2008/high_scalability-2008-03-15-New_Website_Design_Considerations.html">276 high scalability-2008-03-15-New Website Design Considerations</a></p>
<p>20 0.47730604 <a title="163-lda-20" href="../high_scalability-2007/high_scalability-2007-09-06-Product%3A_Perdition_Mail_Retrieval_Proxy.html">80 high scalability-2007-09-06-Product: Perdition Mail Retrieval Proxy</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
