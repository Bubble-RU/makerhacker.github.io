<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-23" href="#">high_scalability-2007-23</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-23-html" href="http://highscalability.com//blog/2007/7/24/major-websites-down-or-why-you-want-to-run-in-two-or-more-da.html">html</a></p><p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-
back power outages power outages. More details atlaughingsquid.breakSites like
SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties,
TypePad, LiveJournal and Vox are all down. The cause was an underground
explosion in a transformer vault under a manhole at 560 Mission Street. Flames
shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without
power.What's perplexing is the UPS backup and diesel generators didn't kick in
to bring the datacenter back on line. I've never toured that datacenter, but
they usually have massive backup systems. It's probably one of those multiple
simultaneous failure situations that you hope never happen in real life, but
too often do. Or maybe the infrastructure wasn't rolled out completely.Update:
the cause was a cascade of failures in a tightly couples system that could
never happen :-) Details atFailure Happens: A summary of the power outage a</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('manhole', 0.399), ('flames', 0.181), ('perplexing', 0.181), ('underground', 0.181), ('technorati', 0.17), ('transformer', 0.17), ('diesel', 0.156), ('typepad', 0.156), ('yelp', 0.156), ('couples', 0.151), ('cascade', 0.14), ('website', 0.14), ('backup', 0.138), ('livejournal', 0.138), ('explosion', 0.138), ('feet', 0.138), ('never', 0.137), ('datacenter', 0.137), ('ups', 0.135), ('happen', 0.129)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9999997 <a title="23-tfidf-1" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-
back power outages power outages. More details atlaughingsquid.breakSites like
SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties,
TypePad, LiveJournal and Vox are all down. The cause was an underground
explosion in a transformer vault under a manhole at 560 Mission Street. Flames
shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without
power.What's perplexing is the UPS backup and diesel generators didn't kick in
to bring the datacenter back on line. I've never toured that datacenter, but
they usually have massive backup systems. It's probably one of those multiple
simultaneous failure situations that you hope never happen in real life, but
too often do. Or maybe the infrastructure wasn't rolled out completely.Update:
the cause was a cascade of failures in a tightly couples system that could
never happen :-) Details atFailure Happens: A summary of the power outage a</p><p>2 0.11316677 <a title="23-tfidf-2" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>Introduction: We can all learn from problems. The Google App Engine team has created a
teachable moment through a remarkably honest and forthcomingpost-mortem for
February 24th, 2010 outagepost, chronicling in elaborate detail a power outage
that took down Google App Engine for a few hours.The world is ending! The
cloud is unreliable! Jump ship! Not. This is not evidence that the cloud is a
beautiful, powerful and unsinkable ship that goes down on its maiden voyage.
Stuff happens, no matter how well you prepare. If you think private
datacenters don't go down, well, then I have some rearangeable deck chairs to
sell you. The goal is to keep improving and minimizing those failure windows.
From that perspective there is a lot to learn from the problems the Google App
Engine team encountered and how they plan to fix them.Please read the article
for all the juicy details, but here's what struck me as key:Power fails. Plan
for it. This seems to happen with unexpected frequency for such expensive and
well t</p><p>3 0.10881984 <a title="23-tfidf-3" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>Introduction: This is a guest post byYelp's JimBlomo. Jim manages a growing data mining team
that usesHadoop,mrjob, andoddjobto processTBsof data. Before Yelp, he built
infrastructure for startups and Amazon. Check out his upcoming talk
atOSCON2013 on Building a Cloud Culture at Yelp.In Q1 2013, Yelp had102
million unique visitors(source: Google Analytics) including approximately 10
million unique mobile devices using the Yelp app on a monthly average
basis.Yelpershavewritten more than39 million rich, local reviews, making Yelp
the leading local guide on everything from boutiques and mechanics to
restaurants and dentists. With respect to data, one of the most unique things
about Yelp is the variety of data: reviews, user profiles, business
descriptions, menus, check-ins, food photos... the list goes on.  We have many
ways to deal data, but today I'll focus on how we handle offline data
processing and analytics.In late 2009, Yelp investigated using Amazon's
ElasticMapReduce(EMR) as an alternativeto a</p><p>4 0.093469895 <a title="23-tfidf-4" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>Introduction: Many enterprises' high-availability architecture is based on the assumption
that you can prevent failure from happening by putting all your critical data
in a centralized database, back it up with expensive storage, and replicate it
somehow between the sites. As I argued in one of my previous posts (Why
Existing Databases (RAC) are So Breakable!) many of those assumptions are
broken at their core, as storage is doomed to failure just like any other
device, expensive hardware doesn't make things any better and database
replication is often not enough.One of the main lessons that we can take from
the likes of Amazon and Google is that the right way to ensure continuous high
availability is by designing our system to cope with failure. We need to
assume that what we tend to think of as unthinkable will probably happen, as
that's the nature of failure. So rather than trying to prevent failures, we
need to build a system that will tolerate them.As we can learn from a recent
outage event in</p><p>5 0.092772841 <a title="23-tfidf-5" href="../high_scalability-2007/high_scalability-2007-08-20-TypePad_Architecture.html">68 high scalability-2007-08-20-TypePad Architecture</a></p>
<p>Introduction: TypePad is considered the largest paid blogging service in the world. After
experience problems because of their meteoric growth, they eventually
transitioned to an architecture patterned after their sister company,
LiveJournal.Site: http://www.typepad.com/The
PlatformMySQLMemcachedPerlMogileFSApacheLinuxThe StatsAs of 2005 TypePad sends
250mbps of traffic using multiple network pipes for 3TB of traffic a day. They
were growing by 10-20% each month. I was unable to find more recent
statistics.The ArchitectureOriginal Architecture:- Single server running
Linux, Apache, Postgres, Perl, mod_perl- Storage was NFS on a filer.A
Devastating Crash Caused a New Direction- A RAID controller failed and spewed
data across all RAID disks.- The database was corrupted and the backups were
corrupted.- Their redundant filers suffered from "split brain" syndrome.They
move toLiveJournal Architecturetype architecture which isn't surprising since
TypePad and LiveJounral are both owned by Six Apart.- Replic</p><p>6 0.089632653 <a title="23-tfidf-6" href="../high_scalability-2012/high_scalability-2012-10-31-Gone_Fishin%27%3A_LiveJournal_Architecture.html">1352 high scalability-2012-10-31-Gone Fishin': LiveJournal Architecture</a></p>
<p>7 0.088131741 <a title="23-tfidf-7" href="../high_scalability-2007/high_scalability-2007-07-09-LiveJournal_Architecture.html">3 high scalability-2007-07-09-LiveJournal Architecture</a></p>
<p>8 0.087476268 <a title="23-tfidf-8" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>9 0.085595787 <a title="23-tfidf-9" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>10 0.083533794 <a title="23-tfidf-10" href="../high_scalability-2011/high_scalability-2011-11-04-Stuff_The_Internet_Says_On_Scalability_For_November_4%2C_2011.html">1137 high scalability-2011-11-04-Stuff The Internet Says On Scalability For November 4, 2011</a></p>
<p>11 0.083410524 <a title="23-tfidf-11" href="../high_scalability-2013/high_scalability-2013-01-23-Building_Redundant_Datacenter_Networks_is_Not_For_Sissies_-_Use_an_Outside_WAN_Backbone.html">1392 high scalability-2013-01-23-Building Redundant Datacenter Networks is Not For Sissies - Use an Outside WAN Backbone</a></p>
<p>12 0.079988636 <a title="23-tfidf-12" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>13 0.078809828 <a title="23-tfidf-13" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>14 0.078519046 <a title="23-tfidf-14" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>15 0.0779038 <a title="23-tfidf-15" href="../high_scalability-2007/high_scalability-2007-10-14-Newbie_in_scalability_design_issues.html">121 high scalability-2007-10-14-Newbie in scalability design issues</a></p>
<p>16 0.07738366 <a title="23-tfidf-16" href="../high_scalability-2011/high_scalability-2011-05-15-Building_a_Database_remote_availability_site.html">1041 high scalability-2011-05-15-Building a Database remote availability site</a></p>
<p>17 0.075760745 <a title="23-tfidf-17" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>18 0.074977323 <a title="23-tfidf-18" href="../high_scalability-2012/high_scalability-2012-12-05-5_Ways_to_Make_Cloud_Failure_Not_an_Option.html">1367 high scalability-2012-12-05-5 Ways to Make Cloud Failure Not an Option</a></p>
<p>19 0.074418791 <a title="23-tfidf-19" href="../high_scalability-2008/high_scalability-2008-03-19-RAD_Lab_is_Creating_a_Datacenter_Operating_System.html">284 high scalability-2008-03-19-RAD Lab is Creating a Datacenter Operating System</a></p>
<p>20 0.073579788 <a title="23-tfidf-20" href="../high_scalability-2013/high_scalability-2013-01-14-MongoDB_and_GridFS_for_Inter_and_Intra_Datacenter_Data_Replication_.html">1386 high scalability-2013-01-14-MongoDB and GridFS for Inter and Intra Datacenter Data Replication </a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.115), (1, 0.038), (2, -0.007), (3, 0.011), (4, -0.01), (5, -0.046), (6, 0.004), (7, -0.025), (8, 0.008), (9, -0.029), (10, -0.058), (11, -0.006), (12, -0.02), (13, -0.009), (14, 0.066), (15, 0.015), (16, 0.034), (17, -0.008), (18, -0.006), (19, 0.05), (20, 0.042), (21, 0.032), (22, -0.013), (23, 0.004), (24, -0.064), (25, 0.032), (26, 0.04), (27, 0.038), (28, -0.022), (29, 0.0), (30, -0.001), (31, -0.032), (32, 0.07), (33, -0.024), (34, -0.007), (35, 0.022), (36, -0.006), (37, 0.04), (38, 0.012), (39, 0.017), (40, 0.032), (41, -0.047), (42, -0.037), (43, 0.029), (44, 0.046), (45, -0.028), (46, -0.014), (47, 0.004), (48, -0.032), (49, -0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95278335 <a title="23-lsi-1" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-
back power outages power outages. More details atlaughingsquid.breakSites like
SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties,
TypePad, LiveJournal and Vox are all down. The cause was an underground
explosion in a transformer vault under a manhole at 560 Mission Street. Flames
shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without
power.What's perplexing is the UPS backup and diesel generators didn't kick in
to bring the datacenter back on line. I've never toured that datacenter, but
they usually have massive backup systems. It's probably one of those multiple
simultaneous failure situations that you hope never happen in real life, but
too often do. Or maybe the infrastructure wasn't rolled out completely.Update:
the cause was a cascade of failures in a tightly couples system that could
never happen :-) Details atFailure Happens: A summary of the power outage a</p><p>2 0.72494519 <a title="23-lsi-2" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>Introduction: We can all learn from problems. The Google App Engine team has created a
teachable moment through a remarkably honest and forthcomingpost-mortem for
February 24th, 2010 outagepost, chronicling in elaborate detail a power outage
that took down Google App Engine for a few hours.The world is ending! The
cloud is unreliable! Jump ship! Not. This is not evidence that the cloud is a
beautiful, powerful and unsinkable ship that goes down on its maiden voyage.
Stuff happens, no matter how well you prepare. If you think private
datacenters don't go down, well, then I have some rearangeable deck chairs to
sell you. The goal is to keep improving and minimizing those failure windows.
From that perspective there is a lot to learn from the problems the Google App
Engine team encountered and how they plan to fix them.Please read the article
for all the juicy details, but here's what struck me as key:Power fails. Plan
for it. This seems to happen with unexpected frequency for such expensive and
well t</p><p>3 0.71733028 <a title="23-lsi-3" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>Introduction: This is a question everyone must struggle with when building out their
datacenter. Storage choices are always the ones I have the least confidence
in. David Marks in his blogYou Can Change It Later!asks the questionShould I
get a SAN to scale my site architecture?and answers no. A better solution is
to use commodity hardware, directly attach storage on servers, and partition
across servers to scale and for greater availability.David's reasoning is
interesting:A SAN creates a SPOF (single point of failure) that is dependent
on a vendor to fly and fix when there's a problem. This can lead to long down
times during this outage you have no access to your data at all.Using easily
available commodity hardware minimizes risks to your company, it's not just
about saving money. Zooming over to Fry's to buy emergency equipment provides
the kind of agility startups need in order to respond quickly to ever changing
situations.It's hard to beat the power and flexibility (backups, easy to add
storag</p><p>4 0.71124625 <a title="23-lsi-4" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI
seized multiple racks of computers from DigitalOne, theseracks host websites
from many clients that just happened to be in the same racks as whomever they
are investigating. Downed sites include Instapaper, Curbed Network,
andPinboard. With thedensity of serversthese days many 1000s of sites could
easily have been effected.Sites like Pinboard were victims by association,
they did not inhale. This is an association sites have no control over. On a
shared hosting service, you have no control over your fellow VM mates. In a
cloud or a managed service, you have no control over which racks your servers
are in. So like second hand smoke, you get the disease by random association.
There's something inherently unfair about that.Acomment by illumin8 shows just
how Darth insidious this process can be:A popular method used by hackers is to
sign up for a virtual server with a stolen credit card. If they are careful
and only a</p><p>5 0.69353098 <a title="23-lsi-5" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:Summary of
the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West
Region. Power failed, backup generators failed to kick in, there weren't
enough resources for EBS volumes to recover, API servers where overwhelmed, a
DNS failure caused failovers to alternate availability zones to fail, a double
fault occurred as the power event interrupted the repair of a different bug.
All kind of typical stuff that just seems to happen.Considering the previous
outage, the big question for programmers is: what does this mean? What does it
mean for how systems should be structured? Have we learned something that
can't be unlearned?The Amazon post has lots of good insights into how EBS and
RDS work, plus lessons learned. The short of the problem is large + complex =
high probability of failure. The immediate fixes are adding more resources,
more redundancy, more isolation between components, more automation, reduce
recove</p><p>6 0.68434548 <a title="23-lsi-6" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>7 0.68190366 <a title="23-lsi-7" href="../high_scalability-2008/high_scalability-2008-03-25-Paper%3A_On_Designing_and_Deploying_Internet-Scale_Services.html">288 high scalability-2008-03-25-Paper: On Designing and Deploying Internet-Scale Services</a></p>
<p>8 0.66877991 <a title="23-lsi-8" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>9 0.65897006 <a title="23-lsi-9" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>10 0.655855 <a title="23-lsi-10" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>11 0.64679015 <a title="23-lsi-11" href="../high_scalability-2008/high_scalability-2008-03-19-RAD_Lab_is_Creating_a_Datacenter_Operating_System.html">284 high scalability-2008-03-19-RAD Lab is Creating a Datacenter Operating System</a></p>
<p>12 0.64586246 <a title="23-lsi-12" href="../high_scalability-2012/high_scalability-2012-12-05-5_Ways_to_Make_Cloud_Failure_Not_an_Option.html">1367 high scalability-2012-12-05-5 Ways to Make Cloud Failure Not an Option</a></p>
<p>13 0.63919038 <a title="23-lsi-13" href="../high_scalability-2011/high_scalability-2011-04-27-Heroku_Emergency_Strategy%3A_Incident_Command_System_and_8_Hour_Ops_Rotations_for_Fresh_Minds.html">1030 high scalability-2011-04-27-Heroku Emergency Strategy: Incident Command System and 8 Hour Ops Rotations for Fresh Minds</a></p>
<p>14 0.63785315 <a title="23-lsi-14" href="../high_scalability-2007/high_scalability-2007-10-30-Paper%3A_Dynamo%3A_Amazon%E2%80%99s_Highly_Available_Key-value_Store.html">139 high scalability-2007-10-30-Paper: Dynamo: Amazon’s Highly Available Key-value Store</a></p>
<p>15 0.62570846 <a title="23-lsi-15" href="../high_scalability-2013/high_scalability-2013-01-23-Building_Redundant_Datacenter_Networks_is_Not_For_Sissies_-_Use_an_Outside_WAN_Backbone.html">1392 high scalability-2013-01-23-Building Redundant Datacenter Networks is Not For Sissies - Use an Outside WAN Backbone</a></p>
<p>16 0.62160796 <a title="23-lsi-16" href="../high_scalability-2011/high_scalability-2011-03-24-Strategy%3A_Disk_Backup_for_Speed%2C_Tape_Backup_to_Save_Your_Bacon%2C_Just_Ask_Google.html">1010 high scalability-2011-03-24-Strategy: Disk Backup for Speed, Tape Backup to Save Your Bacon, Just Ask Google</a></p>
<p>17 0.60377198 <a title="23-lsi-17" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<p>18 0.6016438 <a title="23-lsi-18" href="../high_scalability-2011/high_scalability-2011-04-20-Packet_Pushers%3A_How_to_Build_a_Low_Cost_Data_Center.html">1027 high scalability-2011-04-20-Packet Pushers: How to Build a Low Cost Data Center</a></p>
<p>19 0.60152501 <a title="23-lsi-19" href="../high_scalability-2010/high_scalability-2010-05-25-Strategy%3A_Rule_of_3_Admins_to_Save_Your_Sanity.html">830 high scalability-2010-05-25-Strategy: Rule of 3 Admins to Save Your Sanity</a></p>
<p>20 0.59212649 <a title="23-lsi-20" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.098), (2, 0.181), (10, 0.019), (20, 0.368), (61, 0.072), (79, 0.16)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92037976 <a title="23-lda-1" href="../high_scalability-2008/high_scalability-2008-08-18-Forum_sort_order.html">370 high scalability-2008-08-18-Forum sort order</a></p>
<p>Introduction: G'day,I noticed the default sort order for the forum is to show the posts with
the most replies first. That seems a bit odd for a forum. Would it not make
sense to show the posts with the most recently replies first?It is possible to
re-sort the forum threads that way by clicking on the "Last post" header
(twice). It would seem like a more sensible default.I've checked and I see the
same behaviour as both a registered (logged in) and anonymous user.Cheers
-Callum.</p><p>2 0.87786335 <a title="23-lda-2" href="../high_scalability-2009/high_scalability-2009-11-25-Brian_Aker%27s_Hilarious_NoSQL_Stand_Up_Routine.html">745 high scalability-2009-11-25-Brian Aker's Hilarious NoSQL Stand Up Routine</a></p>
<p>Introduction: Brian Aker gave this 10 minutelightning talkon NoSQL at the Nov 2009
OpenSQLCamp in Portland, Oregon. It's incredibly funny, probably because
there's a lot of truth to what he's saying.Here are theslides and here are
thenotes. Found though #nosql.</p><p>same-blog 3 0.83091849 <a title="23-lda-3" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-
back power outages power outages. More details atlaughingsquid.breakSites like
SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties,
TypePad, LiveJournal and Vox are all down. The cause was an underground
explosion in a transformer vault under a manhole at 560 Mission Street. Flames
shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without
power.What's perplexing is the UPS backup and diesel generators didn't kick in
to bring the datacenter back on line. I've never toured that datacenter, but
they usually have massive backup systems. It's probably one of those multiple
simultaneous failure situations that you hope never happen in real life, but
too often do. Or maybe the infrastructure wasn't rolled out completely.Update:
the cause was a cascade of failures in a tightly couples system that could
never happen :-) Details atFailure Happens: A summary of the power outage a</p><p>4 0.74636722 <a title="23-lda-4" href="../high_scalability-2011/high_scalability-2011-02-24-Strategy%3A_Eliminate_Unnecessary_SQL.html">995 high scalability-2011-02-24-Strategy: Eliminate Unnecessary SQL</a></p>
<p>Introduction: MySQL Expert Ronald Bradford explains how one key way to improve the
scalability of a MySQL server, and undoubtedly nearly every other server, is
toeliminate unnecessary SQL, saying the most efficient way to improve an SQL
statement is to eliminate it:The MySQL kernel can only physically process a
certain number of SQL statements for a given time period (e.g. per second).
Regardless of the type of machine you have, there is a physical limit. If you
eliminate SQL statements that are unwarranted and unnecessary, you
automatically enable more important SQL statements to run. There are numerous
other downstream affects, however this is the simple math. To run more SQL,
reduce the number of SQL you need to run.Ronald shows how to use mk-query-
digest to look at query execution times and determine which ones can be
profitably whacked. Related ArticlesQuora:What are the best methods for
optimizing PHP/MySQL code for speed without caching?</p><p>5 0.68841106 <a title="23-lda-5" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:Summary of
the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West
Region. Power failed, backup generators failed to kick in, there weren't
enough resources for EBS volumes to recover, API servers where overwhelmed, a
DNS failure caused failovers to alternate availability zones to fail, a double
fault occurred as the power event interrupted the repair of a different bug.
All kind of typical stuff that just seems to happen.Considering the previous
outage, the big question for programmers is: what does this mean? What does it
mean for how systems should be structured? Have we learned something that
can't be unlearned?The Amazon post has lots of good insights into how EBS and
RDS work, plus lessons learned. The short of the problem is large + complex =
high probability of failure. The immediate fixes are adding more resources,
more redundancy, more isolation between components, more automation, reduce
recove</p><p>6 0.67773205 <a title="23-lda-6" href="../high_scalability-2009/high_scalability-2009-02-19-Heavy_upload_server_scalability.html">516 high scalability-2009-02-19-Heavy upload server scalability</a></p>
<p>7 0.67185199 <a title="23-lda-7" href="../high_scalability-2013/high_scalability-2013-12-18-How_to_get_started_with_sizing_and_capacity_planning%2C_assuming_you_don%27t_know_the_software_behavior%3F.html">1566 high scalability-2013-12-18-How to get started with sizing and capacity planning, assuming you don't know the software behavior?</a></p>
<p>8 0.65320528 <a title="23-lda-8" href="../high_scalability-2014/high_scalability-2014-03-19-Strategy%3A_Three_Techniques_to_Survive_Traffic_Surges_by_Quickly_Scaling_Your_Site.html">1615 high scalability-2014-03-19-Strategy: Three Techniques to Survive Traffic Surges by Quickly Scaling Your Site</a></p>
<p>9 0.62212729 <a title="23-lda-9" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>10 0.6217683 <a title="23-lda-10" href="../high_scalability-2014/high_scalability-2014-02-14-Stuff_The_Internet_Says_On_Scalability_For_February_14th%2C_2014.html">1596 high scalability-2014-02-14-Stuff The Internet Says On Scalability For February 14th, 2014</a></p>
<p>11 0.60595185 <a title="23-lda-11" href="../high_scalability-2009/high_scalability-2009-10-02-HighScalability_has_Moved_to_Squarespace.com%21_.html">714 high scalability-2009-10-02-HighScalability has Moved to Squarespace.com! </a></p>
<p>12 0.59610724 <a title="23-lda-12" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<p>13 0.59449899 <a title="23-lda-13" href="../high_scalability-2008/high_scalability-2008-09-05-Product%3A_Tungsten_Replicator.html">380 high scalability-2008-09-05-Product: Tungsten Replicator</a></p>
<p>14 0.58723485 <a title="23-lda-14" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>15 0.5858804 <a title="23-lda-15" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>16 0.58494329 <a title="23-lda-16" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>17 0.58391482 <a title="23-lda-17" href="../high_scalability-2013/high_scalability-2013-07-01-PRISM%3A_The_Amazingly_Low_Cost_of_%C2%ADUsing_BigData_to_Know_More_About_You_in_Under_a_Minute.html">1485 high scalability-2013-07-01-PRISM: The Amazingly Low Cost of ­Using BigData to Know More About You in Under a Minute</a></p>
<p>18 0.58202952 <a title="23-lda-18" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>19 0.58121306 <a title="23-lda-19" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>20 0.58103073 <a title="23-lda-20" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
