<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2007" href="../home/high_scalability-2007_home.html">high_scalability-2007</a> <a title="high_scalability-2007-50" href="#">high_scalability-2007-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2007-50-html" href="http://highscalability.com//blog/2007/7/31/berkeleydb-other-distributed-high-performance-keyvalue-datab.html">html</a></p><p>Introduction: I currently use BerkeleyDB as an embedded
databasehttp://www.oracle.com/database/berkeley-db/a decision which was
initially brought on by learning that Google used BerkeleyDB for their
universal sign-on feature.Lustre looks impressive, but their white paper shows
speeds of 800 files created per second, as a good number. However, BerkeleyDB
on my mac mini does 200,000 row creations per second, and can be used as a
distributed file system.I'm having I/O scalability issues with BerkeleyDB on
one machine, and about to implement their distributed replication feature (and
go multi-machine), which in effect makes it work like a distributed file
system, but with local access speeds. That's why I was looking at Lustre.The
key feature difference between BerkeleyDB and Lustre is that BerkeleyDB has a
complete copy of all the data on each computer, making it not a viable
solution for massive sized database applications. However, if you have < 1TB
(ie, one disk) of total possible data, it seems to</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('berkeleydb', 0.828), ('creations', 0.138), ('local', 0.132), ('btree', 0.115), ('mini', 0.115), ('lustre', 0.11), ('beats', 0.103), ('ie', 0.1), ('however', 0.095), ('mac', 0.092), ('files', 0.09), ('feature', 0.083), ('curious', 0.08), ('universal', 0.079), ('viable', 0.077), ('initially', 0.075), ('speeds', 0.075), ('impressive', 0.074), ('found', 0.073), ('white', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="50-tfidf-1" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>Introduction: I currently use BerkeleyDB as an embedded
databasehttp://www.oracle.com/database/berkeley-db/a decision which was
initially brought on by learning that Google used BerkeleyDB for their
universal sign-on feature.Lustre looks impressive, but their white paper shows
speeds of 800 files created per second, as a good number. However, BerkeleyDB
on my mac mini does 200,000 row creations per second, and can be used as a
distributed file system.I'm having I/O scalability issues with BerkeleyDB on
one machine, and about to implement their distributed replication feature (and
go multi-machine), which in effect makes it work like a distributed file
system, but with local access speeds. That's why I was looking at Lustre.The
key feature difference between BerkeleyDB and Lustre is that BerkeleyDB has a
complete copy of all the data on each computer, making it not a viable
solution for massive sized database applications. However, if you have < 1TB
(ie, one disk) of total possible data, it seems to</p><p>2 0.19000301 <a title="50-tfidf-2" href="../high_scalability-2007/high_scalability-2007-12-05-Product%3A_Tugela_Cache.html">174 high scalability-2007-12-05-Product: Tugela Cache</a></p>
<p>Introduction: Tugela Cacheis a cache system like memecached, but instead of storing data
just in RAM, it stores data in the file system using a b-tree. You trade
latency in order to have a very large cache. It's useful for sites that have
caching requirements that exceed their available memory. It uses the same wire
protocol asmemcachedso it can be dropped in without a hassle. From the
website:As large MediaWiki deployments may gain performance using Memcached,
at some level cost of RAM to store all objects becomes too high. In order to
balance resource usage and make more use of our Apache server disks, Tugela,
the distributed cached on-disk hash database, has arrived.Tugela Cache is
derived from Memcached. Much of the code remains the same, but notably, these
changes:Internal slab allocator replaced by BerkeleyDB B-Tree database.Expiry
policy management moved to external program tugela-expireMuch statistics code
made obsolete.An interesting point brought up in the comments is using
memcached with</p><p>3 0.097386748 <a title="50-tfidf-3" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>4 0.084816858 <a title="50-tfidf-4" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>Introduction: Lustre速is a scalable, secure, robust, highly-available cluster file system. It
is designed, developed and maintained by Cluster File Systems, Inc.The central
goal is the development of a next-generation cluster file system which can
serve clusters with 10,000's of nodes, provide petabytes of storage, and move
100's of GB/sec with state-of-the-art security and management
infrastructure.Lustre runs on many of the largest Linux clusters in the world,
and is included by CFS's partners as a core component of their cluster
offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1
supercomputers). Today's users have also demonstrated that Lustre scales down
as well as it scales up, and runs in production on clusters as small as 4 and
as large as 25,000 nodes.The latest version of Lustre is always available from
Cluster File Systems, Inc. Public Open Source releases of Lustre are available
under the GNU General Public License. These releases are found here, and are
used in produ</p><p>5 0.07004182 <a title="50-tfidf-5" href="../high_scalability-2008/high_scalability-2008-10-14-Implementing_the_Lustre_File_System_with_Sun_Storage%3A_High_Performance_Storage_for_High_Performance_Computing.html">411 high scalability-2008-10-14-Implementing the Lustre File System with Sun Storage: High Performance Storage for High Performance Computing</a></p>
<p>Introduction: Much of the focus of high performance computing (HPC) has centered on CPU
performance. However, as computing requirements grow, HPC clusters are
demanding higher rates of aggregate data throughput. Today's clusters feature
larger numbers of nodes with increased compute speeds. The higher clock rates
and operations per clock cycle create increased demand for local data on each
node. In addition, InfiniBand and other high-speed, low-latency interconnects
increase the data throughput available to each node.Traditional shared file
systems such as NFS have not been able to scale to meet this growing demand
for data throughput on HPC clusters. Scalable cluster file systems that can
provide parallel data access to hundreds of nodes and petabytes of storage are
needed to provide the high data throughput required by large HPC applications,
including manufacturing, electronic design, and research.This paper describes
an implementation of the Sun Lustre file system as a scalable storage cluster
u</p><p>6 0.069416143 <a title="50-tfidf-6" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>7 0.06571582 <a title="50-tfidf-7" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>8 0.055204947 <a title="50-tfidf-8" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>9 0.055073693 <a title="50-tfidf-9" href="../high_scalability-2013/high_scalability-2013-08-28-Sean_Hull%27s_20_Biggest_Bottlenecks_that_Reduce_and_Slow_Down_Scalability.html">1508 high scalability-2013-08-28-Sean Hull's 20 Biggest Bottlenecks that Reduce and Slow Down Scalability</a></p>
<p>10 0.054509714 <a title="50-tfidf-10" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>11 0.05449665 <a title="50-tfidf-11" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>12 0.053451233 <a title="50-tfidf-12" href="../high_scalability-2011/high_scalability-2011-08-05-Stuff_The_Internet_Says_On_Scalability_For_August_5%2C_2011.html">1093 high scalability-2011-08-05-Stuff The Internet Says On Scalability For August 5, 2011</a></p>
<p>13 0.052800518 <a title="50-tfidf-13" href="../high_scalability-2014/high_scalability-2014-03-14-Stuff_The_Internet_Says_On_Scalability_For_March_14th%2C_2014.html">1612 high scalability-2014-03-14-Stuff The Internet Says On Scalability For March 14th, 2014</a></p>
<p>14 0.052409746 <a title="50-tfidf-14" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>15 0.0519531 <a title="50-tfidf-15" href="../high_scalability-2011/high_scalability-2011-09-13-Must_see%3A_5_Steps_to_Scaling_MongoDB_%28Or_Any_DB%29_in_8_Minutes.html">1114 high scalability-2011-09-13-Must see: 5 Steps to Scaling MongoDB (Or Any DB) in 8 Minutes</a></p>
<p>16 0.051422607 <a title="50-tfidf-16" href="../high_scalability-2014/high_scalability-2014-04-18-Stuff_The_Internet_Says_On_Scalability_For_April_18th%2C_2014.html">1634 high scalability-2014-04-18-Stuff The Internet Says On Scalability For April 18th, 2014</a></p>
<p>17 0.051158138 <a title="50-tfidf-17" href="../high_scalability-2012/high_scalability-2012-06-22-Stuff_The_Internet_Says_On_Scalability_For_June_22%2C_2012.html">1270 high scalability-2012-06-22-Stuff The Internet Says On Scalability For June 22, 2012</a></p>
<p>18 0.051048633 <a title="50-tfidf-18" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>19 0.050843898 <a title="50-tfidf-19" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>20 0.0500887 <a title="50-tfidf-20" href="../high_scalability-2012/high_scalability-2012-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_5%2C_2012.html">1334 high scalability-2012-10-04-Stuff The Internet Says On Scalability For October 5, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.054), (2, -0.015), (3, 0.001), (4, -0.004), (5, 0.034), (6, 0.014), (7, -0.006), (8, -0.005), (9, 0.032), (10, 0.005), (11, -0.042), (12, 0.013), (13, -0.013), (14, 0.01), (15, 0.013), (16, -0.019), (17, -0.008), (18, -0.006), (19, -0.007), (20, 0.022), (21, -0.005), (22, -0.01), (23, 0.045), (24, -0.011), (25, -0.034), (26, 0.032), (27, -0.027), (28, -0.039), (29, -0.02), (30, -0.026), (31, -0.003), (32, -0.008), (33, 0.006), (34, -0.013), (35, 0.01), (36, -0.003), (37, -0.038), (38, 0.014), (39, -0.011), (40, -0.031), (41, -0.023), (42, -0.015), (43, -0.006), (44, 0.005), (45, 0.007), (46, -0.021), (47, 0.027), (48, -0.005), (49, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92859441 <a title="50-lsi-1" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>Introduction: I currently use BerkeleyDB as an embedded
databasehttp://www.oracle.com/database/berkeley-db/a decision which was
initially brought on by learning that Google used BerkeleyDB for their
universal sign-on feature.Lustre looks impressive, but their white paper shows
speeds of 800 files created per second, as a good number. However, BerkeleyDB
on my mac mini does 200,000 row creations per second, and can be used as a
distributed file system.I'm having I/O scalability issues with BerkeleyDB on
one machine, and about to implement their distributed replication feature (and
go multi-machine), which in effect makes it work like a distributed file
system, but with local access speeds. That's why I was looking at Lustre.The
key feature difference between BerkeleyDB and Lustre is that BerkeleyDB has a
complete copy of all the data on each computer, making it not a viable
solution for massive sized database applications. However, if you have < 1TB
(ie, one disk) of total possible data, it seems to</p><p>2 0.82224494 <a title="50-lsi-2" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>3 0.77833039 <a title="50-lsi-3" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>Introduction: Tachyon  (github) is interesting new filesystem brought to by the folks at
theUC Berkeley AMP Lab:Tachyon is a fault tolerant distributed ďŹ le system
enabling reliable file sharing at memory-speed across cluster frameworks, such
as Spark and MapReduce.It offers up to 300 times higher throughput than HDFS,
by leveraging lineage information and using memory aggressively. Tachyon
caches working set files in memory, and enables different jobs/queries and
frameworks to access cached files at memory speed. Thus, Tachyon avoids going
to disk to load datasets that is frequently read.It has a Java-like File API,
native support for raw tables, a pluggable file system, and it works with
Hadoop with no modifications. It might work well for streaming media too as
you wouldn't have to wait for the complete file to hit the disk before
rendering.Discuss on Hacker News</p><p>4 0.77144724 <a title="50-lsi-4" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>Introduction: How would you implement a key-value storage system if you were starting from
scratch? The approach Basho settled on withBitcask, their new backend for
Riak, is an interesting combination of using RAM to store a hash map of file
pointers to values and a log-structured file system for efficient writes.  In
this excellent Changelog interview, some folks from Basho describe Bitcask in
more detail.The essential Bitcask:Keys are stored in memory for fast lookups.
All keys must fit in RAM.Writes are append-only, which means writes are
strictly sequential and do not require seeking. Writes are write-through.
Every time a value is updated the data file on disk is appended and the in-
memory key index is updated with the file pointer.Read queries are satisfied
with O(1) random disk seeks. Latency is very predictable if all keys fit in
memory because there's no random seeking around through a file.For reads, the
file system cache in the kernel is used instead of writing a complicated
caching sche</p><p>5 0.76288933 <a title="50-lsi-5" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:GlusterFSis a clustered file-system capable of
scaling to several peta-bytes. It aggregates various storage bricks over
Infiniband RDMA or TCP/IP interconnect into one large parallel network file
system. Storage bricks can be made of any commodity hardware such as x86-64
server with SATA-II RAID and Infiniband HBA).Cluster file systems are still
not mature for enterprise market. They are too complex to deploy and maintain
though they are extremely scalable and cheap. Can be entirely built out of
commodity OS and hardware. GlusterFS hopes to solves this problem.GlusterFS
achieved35 GBps read throughput. The GlusterFS Aggregated I/O Benchmark was
performed on 64 bricks clustered storage system over 10 Gbps Infiniband
interconnect. A cluster of 220 clients pounded the storage system with
multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB
block size. GlusterFS was configured with unify translator and round-robin
scheduler.The advantage</p><p>6 0.75735253 <a title="50-lsi-6" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>7 0.72265726 <a title="50-lsi-7" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>8 0.72066164 <a title="50-lsi-8" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<p>9 0.71388388 <a title="50-lsi-9" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>10 0.71107346 <a title="50-lsi-10" href="../high_scalability-2011/high_scalability-2011-12-23-Stuff_The_Internet_Says_On_Scalability_For_December_23%2C_2011.html">1163 high scalability-2011-12-23-Stuff The Internet Says On Scalability For December 23, 2011</a></p>
<p>11 0.69916987 <a title="50-lsi-11" href="../high_scalability-2014/high_scalability-2014-05-19-A_Short_On_How_the_Wayback_Machine_Stores_More_Pages_than_Stars_in_the_Milky_Way.html">1650 high scalability-2014-05-19-A Short On How the Wayback Machine Stores More Pages than Stars in the Milky Way</a></p>
<p>12 0.69765556 <a title="50-lsi-12" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>13 0.68497813 <a title="50-lsi-13" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>14 0.68452775 <a title="50-lsi-14" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>15 0.67458868 <a title="50-lsi-15" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>16 0.67428482 <a title="50-lsi-16" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>17 0.6695894 <a title="50-lsi-17" href="../high_scalability-2010/high_scalability-2010-04-30-Hot_Scalability_Links_for_April_30%2C_2010.html">819 high scalability-2010-04-30-Hot Scalability Links for April 30, 2010</a></p>
<p>18 0.66632062 <a title="50-lsi-18" href="../high_scalability-2012/high_scalability-2012-09-07-Stuff_The_Internet_Says_On_Scalability_For_September_7%2C_2012.html">1318 high scalability-2012-09-07-Stuff The Internet Says On Scalability For September 7, 2012</a></p>
<p>19 0.65801191 <a title="50-lsi-19" href="../high_scalability-2013/high_scalability-2013-06-21-Stuff_The_Internet_Says_On_Scalability_For_June_21%2C_2013.html">1479 high scalability-2013-06-21-Stuff The Internet Says On Scalability For June 21, 2013</a></p>
<p>20 0.65696406 <a title="50-lsi-20" href="../high_scalability-2012/high_scalability-2012-08-14-MemSQL_Architecture_-_The_Fast_%28MVCC%2C_InMem%2C_LockFree%2C_CodeGen%29_and_Familiar_%28SQL%29.html">1304 high scalability-2012-08-14-MemSQL Architecture - The Fast (MVCC, InMem, LockFree, CodeGen) and Familiar (SQL)</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.062), (2, 0.624), (10, 0.021), (61, 0.019), (79, 0.097), (85, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9961496 <a title="50-lda-1" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>Introduction: I currently use BerkeleyDB as an embedded
databasehttp://www.oracle.com/database/berkeley-db/a decision which was
initially brought on by learning that Google used BerkeleyDB for their
universal sign-on feature.Lustre looks impressive, but their white paper shows
speeds of 800 files created per second, as a good number. However, BerkeleyDB
on my mac mini does 200,000 row creations per second, and can be used as a
distributed file system.I'm having I/O scalability issues with BerkeleyDB on
one machine, and about to implement their distributed replication feature (and
go multi-machine), which in effect makes it work like a distributed file
system, but with local access speeds. That's why I was looking at Lustre.The
key feature difference between BerkeleyDB and Lustre is that BerkeleyDB has a
complete copy of all the data on each computer, making it not a viable
solution for massive sized database applications. However, if you have < 1TB
(ie, one disk) of total possible data, it seems to</p><p>2 0.99463665 <a title="50-lda-2" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>Introduction: At a Cloud Computing Meetup, Siddharth "Sid" Anand of Netflix, backed by a
merry band of Netflixians, gave an interesting talk:Keeping Movies Running
Amid Thunderstorms. While the talk gave a good overview of their move to the
cloud, issues with capacity planning,thundering herds, latency problems,
andsimian armageddon, I found myself most taken with how they handlesoftware
deployment in the cloud.I've worked on half a dozen or more build and
deployment systems, some small, some quite large, but never for a large
organization like Netflix in the cloud. The cloud has this amazing capability
that has never existed before that enables a novel approach to fault-tolerant
software deployments:the ability to spin up huge numbers of instances to
completely run a new release while running the old release at the same
time.The process goes something like: Acanary machineis launched first with
the new software load running real traffic to sanity test the load in a
production environment. If the ca</p><p>3 0.99295509 <a title="50-lda-3" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>Introduction: Wille Faler hascreated an excellent list of best practicesfor building
scalable and high performance systems. Here's a short summary of his
points:Offload the database- Avoid hitting the database, and avoid opening
transactions or connections unless you absolutely need to use them.What a
difference a cache makes- For read heavy applications caching is the easiest
way offload the database.Cache as coarse-grained objects as possible- Coarse-
grained objects save CPU and time by requiring fewer reads to assemble
objects.Don't store transient state permanently- Is it really necessary to
store your transient data in the database?Location, Location- put things close
to where they are supposed to be delivered.Constrain concurrent access to
limited resource- it's quicker to let a single thread do work and finish
rather than flooding finite resources with 200 client threads.Staged,
asynchronous processing- separate a process using asynchronicity into separate
steps mediated by queues and execut</p><p>4 0.99153543 <a title="50-lda-4" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false beliefI thought I came here to stayWe're all just visitingAll
just breaking like wavesThe oceans made me, but who came up with me?Push me,
pull me, push me, or pull me out .So true Perl Jam(Push me Pull me lyrics), so
true. I too have wondered how web clients should be notified of model changes.
Should servers push events to clients or should clients pull events from
servers? A topic worthy of its own song if ever there was one.breakTo pull
events the client simply starts a timer and makes a request to the server.
This is polling. You can either pull a complete set of fresh data or get a
list of changes. The server "knows" if anything you are interested in has
changed and makes those changes available to you. Knowing what has changed can
be relatively simple with a publish-subscribe type backend or you can get very
complex with fine grained bit maps of attributes and keeping per client state
on what I client still needs to see.Polling is heavy man. Imagine all your
client</p><p>5 0.99047327 <a title="50-lda-5" href="../high_scalability-2010/high_scalability-2010-09-30-More_Troubles_with_Caching.html">911 high scalability-2010-09-30-More Troubles with Caching</a></p>
<p>Introduction: As a tasty pairing withFacebook And Site Failures Caused By Complex, Weakly
Interacting, Layered Systems, is another excellent tale of caching gone wrong
by Peter Zaitsev, in an exciting twin billing:Cache Miss Stormand More on
dangers of the caches. This is fascinating case where the cause turned out to
be software upgrade that ran long because it had to be rolled back. During the
long recovery time many of the cache entries timed out. When the database came
back, slam, all the clients queried the database to repopulate the cache and
bad things happened to the database. The solution was equally interesting: So
the immediate solution to bring the system up was surprisingly simple. We just
had to get traffic on the system in stages allowing Memcache to be warmed up.
There were no code which would allow to do it on application side so we did it
on MySQL side instead. "SET GLOBAL max_connections=20" to limit number of
connections to MySQL and so let application to err when it tries to put</p><p>6 0.98885435 <a title="50-lda-6" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>7 0.98878288 <a title="50-lda-7" href="../high_scalability-2010/high_scalability-2010-08-12-Strategy%3A_Terminate_SSL_Connections_in_Hardware_and_Reduce_Server_Count_by_40%25.html">878 high scalability-2010-08-12-Strategy: Terminate SSL Connections in Hardware and Reduce Server Count by 40%</a></p>
<p>8 0.98779869 <a title="50-lda-8" href="../high_scalability-2011/high_scalability-2011-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3%2C_2010.html">967 high scalability-2011-01-03-Stuff The Internet Says On Scalability For January 3, 2010</a></p>
<p>9 0.98698235 <a title="50-lda-9" href="../high_scalability-2008/high_scalability-2008-10-15-Outside.in_Scales_Up_with_Engine_Yard_and_moving_from_PHP_to_Ruby_on_Rails.html">417 high scalability-2008-10-15-Outside.in Scales Up with Engine Yard and moving from PHP to Ruby on Rails</a></p>
<p>10 0.98602438 <a title="50-lda-10" href="../high_scalability-2010/high_scalability-2010-06-04-Strategy%3A_Cache_Larger_Chunks_-_Cache_Hit_Rate_is_a_Bad_Indicator.html">836 high scalability-2010-06-04-Strategy: Cache Larger Chunks - Cache Hit Rate is a Bad Indicator</a></p>
<p>11 0.98471248 <a title="50-lda-11" href="../high_scalability-2008/high_scalability-2008-01-25-Google%3A_Introduction_to_Distributed_System_Design.html">223 high scalability-2008-01-25-Google: Introduction to Distributed System Design</a></p>
<p>12 0.98400068 <a title="50-lda-12" href="../high_scalability-2008/high_scalability-2008-12-01-MySQL_Database_Scale-out_and_Replication_for_High_Growth_Businesses.html">455 high scalability-2008-12-01-MySQL Database Scale-out and Replication for High Growth Businesses</a></p>
<p>13 0.98369938 <a title="50-lda-13" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>14 0.98350954 <a title="50-lda-14" href="../high_scalability-2008/high_scalability-2008-11-02-Strategy%3A_How_to_Manage_Sessions_Using_Memcached.html">436 high scalability-2008-11-02-Strategy: How to Manage Sessions Using Memcached</a></p>
<p>15 0.98260361 <a title="50-lda-15" href="../high_scalability-2012/high_scalability-2012-02-10-Stuff_The_Internet_Says_On_Scalability_For_February_10%2C_2012.html">1190 high scalability-2012-02-10-Stuff The Internet Says On Scalability For February 10, 2012</a></p>
<p>16 0.9816733 <a title="50-lda-16" href="../high_scalability-2011/high_scalability-2011-03-17-Are_long_VM_instance_spin-up_times_in_the_cloud_costing_you_money%3F.html">1006 high scalability-2011-03-17-Are long VM instance spin-up times in the cloud costing you money?</a></p>
<p>17 0.98116457 <a title="50-lda-17" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<p>18 0.98116457 <a title="50-lda-18" href="../high_scalability-2009/high_scalability-2009-04-13-Benchmark_for_keeping_data_in_browser_in_AJAX_projects.html">565 high scalability-2009-04-13-Benchmark for keeping data in browser in AJAX projects</a></p>
<p>19 0.97948468 <a title="50-lda-19" href="../high_scalability-2012/high_scalability-2012-07-13-Stuff_The_Internet_Says_On_Scalability_For_July_13%2C_2012.html">1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</a></p>
<p>20 0.97900438 <a title="50-lda-20" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
